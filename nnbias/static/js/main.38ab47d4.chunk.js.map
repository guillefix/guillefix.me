{"version":3,"sources":["components.js","App.js","serviceWorker.js","index.js"],"names":["B1","_Component","_this","Object","classCallCheck","this","possibleConstructorReturn","getPrototypeOf","call","state","exp_visible","inherits","createClass","key","value","setState","prevProps","prevState","props","console","log","react_default","a","createElement","style","className","isB2","onClick","handleClick","bind","children","color","display","exp","Component","B2","user","components_B1","assign","App","script","document","src","async","body","appendChild","e","target","parentNode","forceUpdate","dangerouslySetInnerHTML","__html","gridArea","textAlign","href","alt","width","position","top","left","components_B2","id","fontSize","padding","transform","Boolean","window","location","hostname","match","ReactDOM","render","src_App_0","getElementById","navigator","serviceWorker","ready","then","registration","unregister"],"mappings":"yNAEaA,EAAb,SAAAC,GACE,SAAAD,IAAc,IAAAE,EAAA,OAAAC,OAAAC,EAAA,EAAAD,CAAAE,KAAAL,IACZE,EAAAC,OAAAG,EAAA,EAAAH,CAAAE,KAAAF,OAAAI,EAAA,EAAAJ,CAAAH,GAAAQ,KAAAH,QACKI,MAAQ,CAACC,aAAa,GAFfR,EADhB,OAAAC,OAAAQ,EAAA,EAAAR,CAAAH,EAAAC,GAAAE,OAAAS,EAAA,EAAAT,CAAAH,EAAA,EAAAa,IAAA,cAAAC,MAAA,WAMIT,KAAKU,SAAS,CAACL,aAAaL,KAAKI,MAAMC,gBAN3C,CAAAG,IAAA,qBAAAC,MAAA,SAQqBE,EAAWC,GACxBA,IAAcZ,KAAKI,OAAOJ,KAAKU,SAAS,CAACL,aAAY,MAT7D,CAAAG,IAAA,SAAAC,MAAA,WAWW,IACAI,EAAgBb,KAAhBa,MAAOT,EAASJ,KAATI,MAEd,OADAU,QAAQC,IAAI,KAAMX,EAAMC,aAExBW,EAAAC,EAAAC,cAAA,OAAKC,MAAON,EAAMM,MAAOC,WAAYP,EAAMQ,KAAO,GAAK,mBAAmBR,EAAMO,WAC9EJ,EAAAC,EAAAC,cAAA,OAAKI,QAAStB,KAAKuB,YAAYC,KAAKxB,OAAQa,EAAMY,UAClDT,EAAAC,EAAAC,cAAA,OAAKE,UAAU,aAAaD,MAAO,CAACO,MAAO,UAAWC,QAASvB,EAAMC,YAAc,QAAU,SAAUQ,EAAMe,UAjBnHjC,EAAA,CAAwBkC,aAuBXC,EAAK,SAACjB,GACjB,OACEG,EAAAC,EAAAC,cAAA,WAAKF,EAAAC,EAAAC,cAAA,QAAME,UAAU,QAAQP,EAAMkB,KAA9B,MAA4Cf,EAAAC,EAAAC,cAACc,EAADlC,OAAAmC,OAAA,GAAQpB,EAAR,CAAeO,UAAW,oBAAoBP,EAAMY,YCwM1FS,qMA5NP,IAAIC,EAASC,SAASlB,cAAc,UAEpCiB,EAAOE,IAAM,0FACbF,EAAOG,OAAQ,EAEfF,SAASG,KAAKC,YAAYL,IAE1BA,EAASC,SAASlB,cAAc,WAEzBmB,IAAM,2EAEbD,SAASG,KAAKC,YAAYL,uCAEpBM,GAKkD,eAAxDA,EAAEC,OAAOC,WAAWA,WAAWA,WAAWvB,WAC5CpB,KAAK4C,+CAGP,OACJ5B,EAAAC,EAAAC,cAAA,OAAKE,UAAU,OACfJ,EAAAC,EAAAC,cAAA,OAAK2B,wBAAyB,CAACC,OAAO,sPAEpC9B,EAAAC,EAAAC,cAAA,OAAKE,UAAU,SAASD,MAAO,CAAC4B,SAAW,SAAUC,UAAY,WACjEhC,EAAAC,EAAAC,cAAA,KAAG+B,KAAK,uBAAuBP,OAAO,UAAS1B,EAAAC,EAAAC,cAAA,OAAKmB,IAAI,aAAaa,IAAI,GAAG/B,MAAO,CAACgC,MAAQ,OAAOC,SAAW,WAAWC,IAAM,QAAQC,KAAO,UAC9ItC,EAAAC,EAAAC,cAAA,KAAG+B,KAAK,iCAAiCP,OAAO,UAAS1B,EAAAC,EAAAC,cAAA,OAAKmB,IAAI,eAAea,IAAI,GAAG/B,MAAO,CAACgC,MAAQ,OAAOC,SAAW,WAAWC,IAAM,QAAQC,KAAO,WACxJtC,EAAAC,EAAAC,cAAA,MAAIE,UAAU,gBAAd,mGACAJ,EAAAC,EAAAC,cAAA,MAAIE,UAAU,kBAAd,4DACAJ,EAAAC,EAAAC,cAAA,MAAIE,UAAU,uBAAd,oDACAJ,EAAAC,EAAAC,cAAA,MAAIE,UAAU,IAAGJ,EAAAC,EAAAC,cAAA,wHAGrBF,EAAAC,EAAAC,cAAA,OAAKI,QAAStB,KAAKuB,YAAYC,KAAKxB,MAAOoB,UAAU,aACnDJ,EAAAC,EAAAC,cAAA,OAAKE,UAAU,aAAaD,MAAO,CAAC4B,SAAY,SAG9C/B,EAAAC,EAAAC,cAACc,EAAD,CAAIJ,IAAKZ,EAAAC,EAAAC,cAAA,6LAAT,sCACAF,EAAAC,EAAAC,cAACqC,EAAD,CAAIxB,KAAK,6BAA6BH,IAAI,yUAA1C,oCACAZ,EAAAC,EAAAC,cAACc,EAAD,oDACAhB,EAAAC,EAAAC,cAACqC,EAAD,CAAIxB,KAAK,YAAYH,IAAKZ,EAAAC,EAAAC,cAAA,uGAA+F,eAA/F,aAAwHF,EAAAC,EAAAC,cAAA,4CAAxH,kFAAgPF,EAAAC,EAAAC,cAAA,kBAAhP,iBAA2Q,eAA3Q,oNAA2eF,EAAAC,EAAAC,cAAA,uEAArgB,+BACAF,EAAAC,EAAAC,cAAA,WACAF,EAAAC,EAAAC,cAACqC,EAAD,CAAIxB,KAAK,uBAAuBH,IAAKZ,EAAAC,EAAAC,cAAA,8QAAqQF,EAAAC,EAAAC,cAACc,EAAD,CAAIJ,IAAKZ,EAAAC,EAAAC,cAAA,6SAAoSF,EAAAC,EAAAC,cAACc,EAAD,CAAIJ,IAAKZ,EAAAC,EAAAC,cAAA,WAAKF,EAAAC,EAAAC,cAAA,OAAKC,MAAO,CAACgC,MAAM,SAAUd,IAAI,mDAAmDa,IAAI,eAAtG,kBAA7S,gBAA1S,4DACAlC,EAAAC,EAAAC,cAAA,WACAF,EAAAC,EAAAC,cAACqC,EAAD,CAAI3B,IAAKZ,EAAAC,EAAAC,cAAA,WAAKF,EAAAC,EAAAC,cAAA,uLAAiLF,EAAAC,EAAAC,cAAA,oGAA8FF,EAAAC,EAAAC,cAAA,gIAAiIa,KAAK,8DAAna,yDACAf,EAAAC,EAAAC,cAACc,EAAD,CAAIJ,IAAI,iLAAR,mEACAZ,EAAAC,EAAAC,cAACqC,EAAD,CAAIxB,KAAK,YAAYH,IAAKZ,EAAAC,EAAAC,cAAA,uEAA8DF,EAAAC,EAAAC,cAAA,uEAA9D,oGAA1B,SACAF,EAAAC,EAAAC,cAACc,EAAD,0FACAhB,EAAAC,EAAAC,cAACqC,EAAD,CAAIxB,KAAK,IAAT,gBACAf,EAAAC,EAAAC,cAACc,EAAD,CAAIJ,IAAKZ,EAAAC,EAAAC,cAAA,WAAKF,EAAAC,EAAAC,cAAA,4LAAsLF,EAAAC,EAAAC,cAAA,6DAAoDF,EAAAC,EAAAC,cAAA,yCAApD,6DAClMF,EAAAC,EAAAC,cAAA,SAAI,yLACJF,EAAAC,EAAAC,cAAA,SAAGF,EAAAC,EAAAC,cAAA,WAAKF,EAAAC,EAAAC,cAAA,wCAAR,KAAkD,kCAClDF,EAAAC,EAAAC,cAAA,SAAI,uHACJF,EAAAC,EAAAC,cAAA,SAAI,8HAINF,EAAAC,EAAAC,cAACc,EAAD,CAAIZ,UAAU,SAASQ,IAAI,0bACzBZ,EAAAC,EAAAC,cAAA,OAAKE,UAAU,iBACbJ,EAAAC,EAAAC,cAAA,MAAIE,UAAU,gBAAd,0DAEFJ,EAAAC,EAAAC,cAAA,+DACAF,EAAAC,EAAAC,cAAA,wIAIAF,EAAAC,EAAAC,cAACc,EAAD,CAAIJ,IAAKZ,EAAAC,EAAAC,cAAA,WAAKF,EAAAC,EAAAC,cAAA,0GAAoGF,EAAAC,EAAAC,cAAA,SAAI,6HAAgIF,EAAAC,EAAAC,cAACc,EAAD,CAAIJ,IAAI,qMAAR,mCAEpPZ,EAAAC,EAAAC,cAAA,OAAKmB,IAAI,yBAAyBa,IAAI,gBAE1ClC,EAAAC,EAAAC,cAACqC,EAAD,2CACAvC,EAAAC,EAAAC,cAACc,EAAD,KACEhB,EAAAC,EAAAC,cAAA,iSAEFF,EAAAC,EAAAC,cAACc,EAAD,KACEhB,EAAAC,EAAAC,cAAA,KAAGU,IAAI,6WAAP,iOAEFZ,EAAAC,EAAAC,cAACqC,EAAD,sHAGAvC,EAAAC,EAAAC,cAACc,EAAD,CAAIZ,UAAU,SAASQ,IAAKZ,EAAAC,EAAAC,cAAA,wPAA+OF,EAAAC,EAAAC,cAACc,EAAD,CAAIJ,IAAI,wcAAR,0CACzQZ,EAAAC,EAAAC,cAAA,OAAKE,UAAU,iBACbJ,EAAAC,EAAAC,cAAA,MAAIE,UAAU,gBAAd,mDAEFJ,EAAAC,EAAAC,cAAA,4JAEFF,EAAAC,EAAAC,cAACc,EAAD,CAAIJ,IAAKZ,EAAAC,EAAAC,cAAA,wJAAgJ,EAAhJ,4FAA2OF,EAAAC,EAAAC,cAACc,EAAD,CAAIJ,IAAI,uOAAR,4BACpPZ,EAAAC,EAAAC,cAAA,OAAKmB,IAAI,qDAAqDmB,GAAG,cAAcN,IAAI,eAEnFlC,EAAAC,EAAAC,cAACc,EAAD,CAAIJ,IAAKZ,EAAAC,EAAAC,cAAA,gNAAuMF,EAAAC,EAAAC,cAACc,EAAD,CAAIJ,IAAI,2KAAR,kBAChNZ,EAAAC,EAAAC,cAAA,OAAKmB,IAAI,oCAAoCmB,GAAG,eAAeN,IAAI,iBAInElC,EAAAC,EAAAC,cAACqC,EAAD,KACEvC,EAAAC,EAAAC,cAAA,OAAKE,UAAU,iBACbJ,EAAAC,EAAAC,cAAA,MAAIE,UAAU,eAAd,kCAGJJ,EAAAC,EAAAC,cAACc,EAAD,KACEhB,EAAAC,EAAAC,cAAA,uIAA8HF,EAAAC,EAAAC,cAAA,2CAA9H,6EAEFF,EAAAC,EAAAC,cAACqC,EAAD,CAAIxB,KAAK,iBACPf,EAAAC,EAAAC,cAAA,6DAAoDF,EAAAC,EAAAC,cAAA,mBAApD,2HACAF,EAAAC,EAAAC,cAAA,SAAI,4BACJF,EAAAC,EAAAC,cAAA,SAAI,iIACJF,EAAAC,EAAAC,cAAA,SAAGF,EAAAC,EAAAC,cAAA,+HAELF,EAAAC,EAAAC,cAACc,EAAD,8HACAhB,EAAAC,EAAAC,cAACqC,EAAD,oEAGAvC,EAAAC,EAAAC,cAACc,EAAD,CAAIZ,UAAU,SAASQ,IAAI,ocACzBZ,EAAAC,EAAAC,cAAA,OAAKE,UAAU,iBACbJ,EAAAC,EAAAC,cAAA,MAAIE,UAAU,gBAAd,iGAGJJ,EAAAC,EAAAC,cAACc,EAAD,CAAIJ,IAAI,omBACRZ,EAAAC,EAAAC,cAAA,OAAKmB,IAAI,kGAAkGa,IAAI,iCAE/GlC,EAAAC,EAAAC,cAACqC,EAAD,qDACAvC,EAAAC,EAAAC,cAACc,EAAD,KACEhB,EAAAC,EAAAC,cAAA,sCACAF,EAAAC,EAAAC,cAAA,SACAF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,wBAAeF,EAAAC,EAAAC,cAAA,mCAAf,qEACAF,EAAAC,EAAAC,cAAA,+PAAsPF,EAAAC,EAAAC,cAAA,SAAGF,EAAAC,EAAAC,cAAA,0BAAzP,SAIJF,EAAAC,EAAAC,cAACqC,EAAD,gFAGAvC,EAAAC,EAAAC,cAACc,EAAD,CAAIb,MAAO,CAACsC,SAAW,UACrBzC,EAAAC,EAAAC,cAAA,SAAGF,EAAAC,EAAAC,cAAA,wBAAH,kCAAoDF,EAAAC,EAAAC,cAAA,iCAApD,uBAAoG,qDAApG,IAA0JF,EAAAC,EAAAC,cAAA,uBAA1J,IAA6K,qLAC5K,iIACA,kQAEHF,EAAAC,EAAAC,cAACqC,EAAD,+OACAvC,EAAAC,EAAAC,cAAA,WACAF,EAAAC,EAAAC,cAACqC,EAAD,CAAIxB,KAAK,gBAAT,+IACAf,EAAAC,EAAAC,cAACqC,EAAD,CAAIxB,KAAK,2CAAT,+EACAf,EAAAC,EAAAC,cAACqC,EAAD,CAAIxB,KAAK,sBAAT,yFAAoHf,EAAAC,EAAAC,cAAA,SAAOE,UAAU,iBAAiBD,MAAO,CAACuC,QAAU,gBAApD,wDACpH1C,EAAAC,EAAAC,cAACc,EAAD,6BACuBhB,EAAAC,EAAAC,cAAA,2CADvB,uJACgNF,EAAAC,EAAAC,cAAA,sEADhN,4KAKAF,EAAAC,EAAAC,cAACqC,EAAD,KACEvC,EAAAC,EAAAC,cAAA,OAAKE,UAAU,iBACbJ,EAAAC,EAAAC,cAAA,MAAIE,UAAU,gBAAd,qDAEFJ,EAAAC,EAAAC,cAAA,4GAIFF,EAAAC,EAAAC,cAACc,EAAD,KACEhB,EAAAC,EAAAC,cAAA,wFACAF,EAAAC,EAAAC,cAAA,2FAAkFF,EAAAC,EAAAC,cAAA,mBAAlF,iEAA6JF,EAAAC,EAAAC,cAAA,mBAA7J,kGAEFF,EAAAC,EAAAC,cAACc,EAAD,KACEhB,EAAAC,EAAAC,cAAA,8LAAqLF,EAAAC,EAAAC,cAAA,SAAGF,EAAAC,EAAAC,cAAA,0BAAxL,6BAEFF,EAAAC,EAAAC,cAACc,EAAD,CAAIJ,IAAI,ijBACRZ,EAAAC,EAAAC,cAAA,OAAKsC,GAAG,mBAAmBnB,IAAI,8GAA8Ga,IAAI,gCAE/IlC,EAAAC,EAAAC,cAAA,QAAMC,MAAO,CAACiC,SAAW,WAAYC,IAAM,OAAOC,KAAO,SAAzD,gBACAtC,EAAAC,EAAAC,cAAA,QAAMC,MAAO,CAACQ,QAAU,eAAeyB,SAAW,WAAYC,IAAM,MAAMC,KAAO,SAASK,UAAY,mBAAtG,YACF3C,EAAAC,EAAAC,cAACc,EAAD,KACEhB,EAAAC,EAAAC,cAAA,iFAAwEF,EAAAC,EAAAC,cAAA,kFAAxE,yDAEFF,EAAAC,EAAAC,cAAA,WACAF,EAAAC,EAAAC,cAACqC,EAAD,qBACAvC,EAAAC,EAAAC,cAACc,EAAD,2JACAhB,EAAAC,EAAAC,cAACc,EAAD,wKAAoKhB,EAAAC,EAAAC,cAAA,wCAA+BF,EAAAC,EAAAC,cAAA,uBAA/B,YAGpKF,EAAAC,EAAAC,cAACqC,EAAD,CAAIpC,MAAO,CAACsC,SAAW,UACrBzC,EAAAC,EAAAC,cAAA,SACEF,EAAAC,EAAAC,cAAA,uBAIFF,EAAAC,EAAAC,cAAA,yHAGAF,EAAAC,EAAAC,cAAA,oJAGAF,EAAAC,EAAAC,cAAA,4GAGAF,EAAAC,EAAAC,cAAA,wHAGAF,EAAAC,EAAAC,cAAA,mIAGAF,EAAAC,EAAAC,cAAA,+GAGAF,EAAAC,EAAAC,cAAA,oIAGAF,EAAAC,EAAAC,cAAA,2HACAF,EAAAC,EAAAC,cAAA,mGACAF,EAAAC,EAAAC,cAAA,kLACAF,EAAAC,EAAAC,cAAA,+EAnNYW,cCOE+B,QACW,cAA7BC,OAAOC,SAASC,UAEe,UAA7BF,OAAOC,SAASC,UAEhBF,OAAOC,SAASC,SAASC,MACvB,2DCZNC,IAASC,OAAOlD,EAAAC,EAAAC,cAACiD,EAAD,MAAS/B,SAASgC,eAAe,SD2H3C,kBAAmBC,WACrBA,UAAUC,cAAcC,MAAMC,KAAK,SAAAC,GACjCA,EAAaC","file":"static/js/main.38ab47d4.chunk.js","sourcesContent":["import React, { Component } from 'react';\n\nexport class B1 extends Component {\n  constructor() {\n    super();\n    this.state = {exp_visible: false}\n  }\n  handleClick() {\n    this.setState({exp_visible:!this.state.exp_visible})\n  }\n  componentDidUpdate(prevProps, prevState) {\n    if (prevState === this.state) this.setState({exp_visible:false})\n  }\n  render() {\n    const {props, state} = this;\n    console.log(\"hi\", state.exp_visible);\n    return (\n    <div style={props.style} className={(props.isB2 ? \"\" : \"speech-bubble1 \")+props.className}>\n      <div onClick={this.handleClick.bind(this)}>{props.children}</div>\n      <div className=\"ackshually\" style={{color: \"#606060\", display: state.exp_visible ? \"block\" : \"none\"}}>{props.exp}</div>\n    </div>\n    )\n  }\n}\n\nexport const B2 = (props) => {\n  return (\n    <div><span className=\"user\">{props.user}  </span><B1 {...props} className={\"speech-bubble2 \"}>{props.children}</B1></div>\n  )\n}\n","import React, { Component } from 'react';\n\nimport { B1,B2 } from './components'\nimport './App.css';\n\nclass App extends Component {\n      componentDidMount () {\n        let script = document.createElement(\"script\");\n\n        script.src = \"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML\";\n        script.async = true;\n\n        document.body.appendChild(script);\n\n        script = document.createElement(\"script\");\n\n        script.src = \"MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}});\";\n\n        document.body.appendChild(script);\n      }\n  handleClick(e) {\n    // console.log(e.target.parentNode.parentNode.parentNode.className);\n    //ackshually nodes\n    // let ackshuallies = Array.from(e.target.parentNode.querySelectorAll(\".ackshually\"));\n    //if any ackshually is hidden in the clicked boi, then update all dem chat bubbless\n    if (e.target.parentNode.parentNode.parentNode.className === \"panel chat\")\n      this.forceUpdate()\n  }\n  render() {\n    return (\n<div className=\"App\">\n<div dangerouslySetInnerHTML={{__html:\"<script type='text/javascript' async src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js'></script><script type='text/x-mathjax-config'>MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\\\\\\\(','\\\\\\\\)']]}});</script>\"}}></div>\n\n  <div className=\"header\" style={{\"gridArea\":\"header\", \"textAlign\":\"center\"}}>\n  <a href=\"http://www.ox.ac.uk/\" target=\"_blank\"><img src=\"oulogo.png\" alt=\"\" style={{\"width\":\"12vw\",\"position\":\"absolute\",\"top\":\"2.5em\",\"left\":\"0vw\"}}/></a>\n  <a href=\"http://www.sysbiodtc.ox.ac.uk/\" target=\"_blank\"><img src=\"dtclogo.jpeg\" alt=\"\" style={{\"width\":\"12vw\",\"position\":\"absolute\",\"top\":\"2.5em\",\"left\":\"88vw\"}}/></a>\n    <h1 className=\"poster-title\">Deep learning generalizes because the parameter-function map is biased towards simple functions</h1>\n    <h3 className=\"poster-authors\">Guillermo Valle-Pérez, Chico Q. Camargo, Ard A. Louis</h3>\n    <h3 className=\"poster-affiliations\">Departments of Physics, University of Oxford, UK</h3>\n    <h4 className=\"\"><mark>Tap on the bubbles to epxpand explanation. WIP, at me @guillefix on twitter with questions/suggestions</mark></h4>\n  </div>\n\n<div onClick={this.handleClick.bind(this)} className=\"container\">\n  <div className=\"panel chat\" style={{\"gridArea\": \"chat\"}}>\n\n  {/*intro*/}\n    <B1 exp={<div>Generalization is the ability of a supervised learning algorithm to predict correctly on data outside the training set (typically assumed to come from the same distribution)</div>}>Why does deep learning generalize?</B1>\n    <B2 user=\"Supervised learning theory\" exp=\"All the theorems that guarantee generalization (PAC, VC/growth function, Rademacher, PAC-Bayes, compression, stability) try to quantify an inductive bias. This is obvious really, think of a completely unbiased algorithm (no restrictions on hypotheses, or bias within them), it just guesses randomly outside the training data.\">Because it has an inductive bias</B2>\n    <B1>Ok, but why does it have an inductive bias?</B1>\n    <B2 user=\"VC theory\" exp={<div>In binary classification, Vapnik-Chervonenkis theory determines when a hypothesis class ${\"\\\\mathcal{H}\"}$ has the <emph>uniform convergence property</emph>, which means that with high probability over the sampling of the training set <u>every</u> function in ${\"\\\\mathcal{H}\"}$ has a training error which is close to its generalization error, for sufficiently large training sets. It also quantifies how large the training sets need to be, giving generalization error bounds which are <u>worst-case over all empirical-risk-minimizing algorithms</u></div>}>Limited expressivity maybe?</B2>\n    <br/>\n    <B2 user=\"Zhang et al. (2017a)\" exp={<div>If a binary classification model can fit any labelling on a given set of instances this means, by definition, that the VC dimension is greater than the size of the training set, which in turn implies that VC-dimension worst-case bounds are vacuous ($>1$). <B1 exp={<div>In reality, the experiments by Zhang et al. at most show that it can fit almost any labelling, but this is just a detail, the growth function would still be large, or equivalently, the VC dimension can only be guaranteed to be at least [a bit smaller than the size of the training data] <B1 exp={<div><img style={{width:\"180px\"}} src=\"https://i.ytimg.com/vi/YYLWwRUMXR0/hqdefault.jpg\" alt=\"dankmeme\"/></div>}>Aaackshually</B1></div>}>Ackshually</B1></div>}>No, neural networks (NNs) can fit randomly labelled data</B2>\n    <br/>\n    <B2 exp={<div><p>Soudry et al. prove particular properties of gradient descent, when using cross-entropy loss, showing that it converges to a maximum-margin distribution on separable data</p><p>Zhang et al. (2017b) prove properties of SGD including that it converges to flat minima</p><p>Zhang et al. (2018) also show that SGD can be biased to wide minima, even at the expense of a higher value of loss</p></div>} user=\"D Soudry et al., Zhang et al. (2017b), Zhang et al. (2018)\">Maybe SGD is what's biasing towards certain solutions</B2>\n    <B1 exp=\"For example: GD, Adam, RMSProp, and all the variants of SGD; even evolutionary algorithms and random search are found to work well in the domains where they have been tried!\">But many very different optimization algorithms generalize well</B1>\n    <B2 user=\"Wu et al.\" exp={<div>This is just one example of a paper where they find this <small>(the paper is also interesting for many other reasons)</small>. They find that the full-batch gradient descent only generalizes a few percept worse than SGD.</div>}>Yeah!</B2>\n    <B1>Hmm, maybe it's an intrinsic property of the NN, like its parameter-function map?</B1>\n    <B2 user=\"\">What's that?</B2>\n    <B1 exp={<div><p>So intuitively, a function is the input-output map that the network implements. This is actually the mathematical definition of function, as a collection of input-output pairs</p><p>Because of overparametrization, there is lots of <u>redundancy in the parameters</u>, and so many paramaters give rise to the same function</p></div>}>\n      <p>{\"Let the space of functions that the model can express be $\\\\mathcal{F}$. If the model has $p$ real valued parameters, taking values within a set $\\\\Theta \\\\subseteq \\\\mathbb{R}^p$, \"}</p>\n      <p><big><b>the parameter-function map</b></big>, {\"$\\\\mathcal{M}$, is defined as:\"}</p>\n      <p>{\"$$\\\\begin{align} \\\\mathcal{M} : \\\\Theta &\\\\to \\\\mathcal{F}\\\\\\\\ \\\\theta &\\\\mapsto f_\\\\mathbf{\\\\theta} \\\\end{align}$$\"}</p>\n      <p>{\"where $f_\\\\mathbf{\\\\theta}$ is the function implemented by the model with choice of parameter vector $\\\\mathbf{\\\\theta}$.\"}</p>\n    </B1>\n\n  {/*bias*/}\n    <B1 className=\"result\" exp=\"Another more precise statement: Upon sampling parameters i.i.d. using a Gaussian or uniform distribution, the probability of obtaining different functions varies by maany orders of magnitude.     Note that we can talk about the probability of a single function, because we consider only binary classification, so the output is 0/1, and we have a finite number of inputs (say $N$), there are then in total a finite number ($2^N$) functions\">\n      <div className=\"panel-heading\">\n        <h3 className=\"bubble-title\">Result 1: The parameter-function map is hugely biased</h3>\n      </div>\n      <p>For all the neural network architectures we tried:</p>\n      <blockquote>\n      The volumes of regions of parameter space producing particular funcions span a huge range of orders of magnitude.\n      </blockquote>\n    </B1>\n      <B1 exp={<div><p>Fig 1. Probability of obtaining a particular function versus its rank (ranked by probability)</p><p>{\"Obtained in a sample of $10^{10}$ (blue) or $10^7$ (others) parameters for a fully connected network of shape (7,40,40,1)\"}</p><B1 exp=\"The different lines (differen colors) represent different distributions over the parameters. The red line, however, represents Zipf'z law, where $P(\\text{rank})\\propto\\frac{1}{\\text{rank}}$\">What are the different lines?</B1></div>}>\n        {/*<span style={{\"fontSize\":\"0.6em\",\"position\":\"relative\", \"top\":\"178px\",\"left\":\"1140px\",\"padding\":\"0.5em\",\"background-color\":\"white\"}}>{\"$P(r)=\\\\frac{1}{\\\\ln{(N_0)}r}$\"}</span>*/}\n        <img src=\"rank_plot_extended.png\" alt=\"logP vs LZ\"/>\n      </B1>\n    <B2>Oh, and how did you find that out?</B2>\n    <B1>\n      <p>For a family of fully connected feedforward neural networks with $7$ Boolean inputs and one Boolean output of varying depths and widths, we sampled parameters with several distributions. In Figure 1, we show the empirical frequencies by which different functions are obtained</p>\n    </B1>\n    <B1>\n      <p exp=\"We explain the Gaussian process approximation in more detail later.     Also note that in this cases, we need to look at the functions constrained on a limited set of inputs, because we aren't gonna enumerate all possible images. We can say we are talking about labellings rather than functions proper, but the math is the same, and the inutition basically too.\">For some larger neural networks with higher-dimensional input spaces, we used a Gaussian process approximation to calculate the probability of different functions. This can be seen in Figures 2a and the inset of Figure 3</p>\n    </B1>\n    <B2>Ok, but do we have any way to characterize the bias? What kinds of functions are the networks biased towards?</B2>\n\n    {/*simpbias*/}\n    <B1 className=\"result\" exp={<div>We correlate the probability of obtaining a function with its complexity. We also have shown that if we plot Figure 2b as a histogram, most of the probability weight is near the bound, so that the correlation is actually even better. <B1 exp=\"We try several. They are all intended to approximate Kolmogorov complexity (which is uncomputable), and try to capture how compressible something is.      Lempel-Ziv complexity for example, basically looks for statistical regularities in a sequence, and compresses it. Our complexity measure is basically the compressed size. We apply it to the truth table of the Boolean functions.  Other measures of complexity we tried are described in the Appendix.\">What are the measures of complexity></B1></div>}>\n      <div className=\"panel-heading\">\n        <h3 className=\"bubble-title\">Result 2: The bias is towards simple functions</h3>\n      </div>\n      <p>We found that in all cases, the probability of a function inversely correlated with its complexity (using a variety of measures of complexity)</p>\n    </B1>\n    <B1 exp={<div>Probability versus Lempel-Ziv complexity. Probabilities are estimated from a sample of $10^8$ parameters. Points with a frequency of $10^{-8}$ are removed for clarity because these suffer from finite-size effects (see Appendix G) <B1 exp=\"Lempel-Ziv complexity for example, basically looks for statistical regularities in a sequence, and compresses it. Our complexity measure is basically the compressed size. We apply it to the truth table of the Boolean functions.\">What is LZ complexity?</B1></div>}>\n    <img src=\"cnt_100000000_7_40_40_1_relu_freq_LZ_with_line.png\" id=\"freq_lz_img\" alt=\"rank plot\"/>\n    </B1>\n    <B1 exp={<div>Probability (using GP approximation) versus critical sample ratio (CSR) of labelings of 1000 random CIFAR10 inputs, produced by $250$ random samples of parameters. The network is a 4 layer CNN. <B1 exp=\"In short, CSR is a measure of how sensitive the function is to small perturbation of the inputs, it measures what fraction of inputs are close to the decision boundary\">What is CSR?</B1></div>}>\n    <img src=\"CSR_logP_test_cnn_4_none_0000.png\" id=\"CSR_logP_img\" alt=\"logP vs CSR\"/>\n    </B1>\n\n    {/*whybias*/}\n    <B2>\n      <div className=\"panel-heading\">\n        <h3 className=\"panel-title\">Why are the networks biased?</h3>\n      </div>\n    </B2>\n    <B1>\n      <p>No deeper explanation yet about why the parameter-function map is biased. However, we do have some deeper reason, based on <b>algorithmic information theory</b> for why it is biased towards simple functions, given that it is biased.</p>\n    </B1>\n    <B2 user=\"Dingle et al.\">\n      <p>The probability $P(x)$ to obtain output $x$ of a <i>simple</i> map $f$, upon sampling its inputs uniformly at random, depends only on the Kolmogorov complexity of the output $K(x)$:</p>\n      <p>{\"$$P(x) \\\\leq 2^{-K(x)}$$\"}</p>\n      <p>{\"The main condition on the map is that its Kolmogorov complexity is negligible relative to that of the output $K(f) \\\\ll K(x)$\"}</p>\n      <p><small>Kolmogorov complexity is uncomputable, so we use computable approximations to it, like Lempel-Ziv complexity</small></p>\n    </B2>\n    <B1>The parameter function map satisfies $K(f) \\ll K(x)$, and indeed we found that the bound works (red line in Figure) </B1>\n    <B2>Is this bias enough to explain the observed generalization?</B2>\n\n    {/*gener*/}\n    <B1 className=\"result\" exp=\"What do I mean by the bulk? I mean that the bound is tight, so that we think that most of the generalization is explained by our theory. However, differences of a few percent typically obtained by using different algorithms, or other tricks, are not captured by our theory, but these are typically only small relative differences in generalization, which we argue are lower-order effects. We aim to offer a first-order explanation of generalization\">\n      <div className=\"panel-heading\">\n        <h3 className=\"bubble-title\">Result 3: The bias is enough to explain \"the bulk\" of the generalization in our experiments</h3>\n      </div>\n    </B1>\n    <B1 exp=\"Mean generalization error and corresponding PAC-Bayes bound versus percentage of label corruption, for three datasets and a training set of size 10000. Training set error is 0 in all experiments. Note that the bounds follow the same trends as the true generalization errors. The empirical errors are averaged over 8 initializations. The Gaussian process parameters were $\\sigma_w = 1.0$, $\\sigma_b = 1.0$ and the network was a 4-layer CNN with no pooling. Insets show the marginal likelihood of the data as computed by the Gaussian process approximation (in natural log scale), versus the label corruption.\">\n    <img src=\"new_bound_insets_cnn_sigmaw_1_sigmab_1_MNIST_fashionMNIST_CIFAR_generror_vs_labelcorruption.png\" alt=\"generalization error bounds\"/>\n    </B1>\n    <B2>Interesting, and how did you determine that?</B2>\n    <B1>\n      <p>To explore this question:</p>\n      <p>\n      <ul>\n        <li>We use the <b>PAC-Bayesian framework</b> to translate probabilistic biases into generalization guarantees</li>\n        <li>We make the assumption that the algorithm optimizing the parameters is unbiased, to isolate the effect of the parameter-function map. More precisely, we assume that the optimization algorithm samples the zero-error region close to uniformly (<i><b>Assumption 1</b></i>).</li>\n      </ul>\n      </p>\n    </B1>\n    <B2>Can you provide more details on your method to obtain PAC-Bayes bounds?</B2>\n\n  {/*methods*/}\n    <B1 style={{\"fontSize\":\"0.9em\"}}>\n      <i><b>Corollary 1</b> (of Langford's version of the <b>PAC-Bayesian theorem</b> (Langford et al.)) {\"For any distribution $P$ on any function space and\"} <u>realizable</u> {\"distribution $\\\\mathcal{D}$ on a space of instances we have, for $0< \\\\delta \\\\leq 1 $, that with probability at least $1-\\\\delta$ over the choice of sample $S$ of $m$ instances\"}</i>\n      {\"$$-\\\\ln{\\\\left(1-\\\\epsilon(Q^*)\\\\right)} \\\\leq \\\\frac{\\\\ln{\\\\frac{1}{P(U)}} + \\\\ln{\\\\left(\\\\frac{2m}{\\\\delta}\\\\right)}}{m-1}$$\"}\n      {\"where $\\\\epsilon(Q^*)$ is the expected generalization error under distribution over functions $Q^*(c)=\\\\frac{P(c)}{\\\\sum_{c\\\\in U} P(c)}$, $U$ is the set of functions in $\\\\mathcal{H}$ consistent with the sample $S$, and where $P(U)=\\\\sum_{c\\\\in U} P(c)$\"}\n    </B1>\n    <B2>Ah I see. So the bound depends on the data via $P(U)$, which is nothing but the marginal likelihood of the labels on the data, given by the prior $P(f)$. But how do you calculate $P(U)$ for neural networks, isn't that intractable?</B2>\n    <br/>\n    <B2 user=\"J Lee et al.\">Yes. However, $P(f)$ for deep fully connected neural networks approaches a Gaussian process as the width of the layers approaches infinity.</B2>\n    <B2 user=\"A Garriga-Alonso et al., R Novak et al.\">also for convolutional networks, as the number of filters goes to infinity!</B2>\n    <B2 user=\"AGG Mathews et al.\">and it seems the networks don't need to be that wide for the approximation to be good <small className=\"speech-bubble1\" style={{\"padding\":\"0.2em 0.5em\"}}>&nbsp;&nbsp;&nbsp;&nbsp;(we independently checked this too)</small></B2>\n    <B1>\n      Thanks everyone! The <b>Gaussian process approximation</b> is what allows us to compute $P(U)$ for realistically-sized NNs. However, the marginal likelihood for a Gaussian process with Bernoulli likelihood <small>(for binary classification, the setting of PAC-Bayes)</small> is still intractable, and so we explored some approximation techniques: Variational, Laplace, expectation-propagation (EP), and found EP to work best for our purposes.\n    </B1>\n\n    {/*algo*/}\n    <B2>\n      <div className=\"panel-heading\">\n        <h3 className=\"bubble-title\">What's the effect of the optimization algorithm?</h3>\n      </div>\n      <p>\n        After all, different optimization algorithms do show differences in generalization in practice\n      </p>\n    </B2>\n    <B1>\n      <p>Yes, but differences in generalization are typically of only a few percent.</p>\n      <p>However, you raise an important point. Although we have shown that the bias is <i>enough</i> to explain the bulk of the generalization, whether it is the <i>actual</i> origin of the generalization in DNNs depends on the behaviour of the optimization algorithm.</p>\n    </B1>\n    <B1>\n      <p> A sufficient (though not necessary) condition for the parameter-function map to be main origin of the generalization is that the optimization algorithm isn't too biased, namely <i><b>Assumption 1</b></i> is approximately valid.</p>\n    </B1>\n    <B1 exp=\"Average probability of finding a function for a variant of SGD (advSGD in this case; see Appendix A), versus average probability of finding a function when using the Gaussian process approximation to a 2 layer fully connected network. This is done for a randomly chosen, but fixed, target Boolean function of Lempel-Ziv complexity 84.0. See Appendix D for details. The Gaussian process parameters are $\\sigma_w = 10.0$, and $\\sigma_b = 10.0$. For advSGD, we have removed functions which only appeared once in the whole sample, to avoid finite-size effects\">\n    <img id=\"advSGD_Bayes_img\" src=\"SGD_prob_EPapprox_no_single_sample_vs_advsgd_many_84.0_7_40_2_150000_10000_118_no_replace_1.0_10.0_10.0.png\" alt=\"advSGD vs GP probabilities\"/>\n    </B1>\n      <span style={{\"position\":\"relative\", \"top\":\"11em\",\"left\":\"32em\"}}>advSGD probs</span>\n      <span style={{\"display\":\"inline-block\",\"position\":\"relative\", \"top\":\"5em\",\"left\":\"18.7em\",\"transform\":\"rotate(-90deg)\"}}>GP probs</span>\n    <B1>\n      <p>We conjecture that it is for many common DNN optimization algorithms <small>(note that for exact Bayesian sampling it is true, by definition)</small>, and show some empirical evidence supporting this .</p>\n    </B1>\n    <br/>\n    <B2>Future work?</B2>\n    <B1>There are problems regarding the validity of Assumption 1, the EP or other approximations to $P(U)$, as well as the tightness of PAC-Bayes itself.</B1>\n    <B1>Furthermore, one can dig deeper to try to better understand the origin of the bias, and characterize it. In particular, there is the very important question of <b>why is the bias helpful for <i>real-world</i> tasks?</b></B1>\n\n    {/*refs*/}\n    <B2 style={{\"fontSize\":\"0.8em\"}}>\n      <p>\n        <b>\n        Starring:\n        </b>\n      </p>\n      <p>\n      Zhang et al (2017a). Understanding deep learning requires rethinking generalization.  Published in ICLR 2017\n      </p>\n      <p>\n      Wu et al. Towards understanding generalization of deep learning: Perspective of loss landscapes. arXiv preprint arXiv:1706.10239, 2017.\n      </p>\n      <p>\n      J Lee et al. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165, 2017.\n      </p>\n      <p>\n      A Garriga-Alonso et al.  Deep convolutional networks as shallow Gaussian processes.  Published in ICLR 2019\n      </p>\n      <p>\n      R Novak et al. Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes.  Published in ICLR 2019\n      </p>\n      <p>\n      AGG Mathews et al. Gaussian Process Behaviour in Wide Deep Neural Networks. Published in ICLR 2018\n      </p>\n      <p>\n      Dingle et al. Input–output maps are strongly biased towards simple outputs. Nature communications, 9(1):761, 2018.\n      </p>\n      <p>D Soudry et al. The implicit bias of gradient descent on separable data. arXiv preprint arXiv:1710.10345, 2017</p>\n      <p>Zhang et al. (2017b) Musings on deep learning: Properties of sgd. CBMM Memos 04/2017. </p>\n      <p>Zhang et al. (2018) Energy–entropy competition and the effectiveness of stochastic gradient descent in machine learning. Molecular Physics, pp. 1–10, 2018.</p>\n      <p>Langford et al. Bounds for averaging classifiers. 2001.</p>\n    </B2>\n  </div>\n\n  </div>\n\n</div>\n    );\n  }\n}\n\nexport default App;\n","// This optional code is used to register a service worker.\n// register() is not called by default.\n\n// This lets the app load faster on subsequent visits in production, and gives\n// it offline capabilities. However, it also means that developers (and users)\n// will only see deployed updates on subsequent visits to a page, after all the\n// existing tabs open on the page have been closed, since previously cached\n// resources are updated in the background.\n\n// To learn more about the benefits of this model and instructions on how to\n// opt-in, read https://bit.ly/CRA-PWA\n\nconst isLocalhost = Boolean(\n  window.location.hostname === 'localhost' ||\n    // [::1] is the IPv6 localhost address.\n    window.location.hostname === '[::1]' ||\n    // 127.0.0.1/8 is considered localhost for IPv4.\n    window.location.hostname.match(\n      /^127(?:\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/\n    )\n);\n\nexport function register(config) {\n  if (process.env.NODE_ENV === 'production' && 'serviceWorker' in navigator) {\n    // The URL constructor is available in all browsers that support SW.\n    const publicUrl = new URL(process.env.PUBLIC_URL, window.location.href);\n    if (publicUrl.origin !== window.location.origin) {\n      // Our service worker won't work if PUBLIC_URL is on a different origin\n      // from what our page is served on. This might happen if a CDN is used to\n      // serve assets; see https://github.com/facebook/create-react-app/issues/2374\n      return;\n    }\n\n    window.addEventListener('load', () => {\n      const swUrl = `${process.env.PUBLIC_URL}/service-worker.js`;\n\n      if (isLocalhost) {\n        // This is running on localhost. Let's check if a service worker still exists or not.\n        checkValidServiceWorker(swUrl, config);\n\n        // Add some additional logging to localhost, pointing developers to the\n        // service worker/PWA documentation.\n        navigator.serviceWorker.ready.then(() => {\n          console.log(\n            'This web app is being served cache-first by a service ' +\n              'worker. To learn more, visit https://bit.ly/CRA-PWA'\n          );\n        });\n      } else {\n        // Is not localhost. Just register service worker\n        registerValidSW(swUrl, config);\n      }\n    });\n  }\n}\n\nfunction registerValidSW(swUrl, config) {\n  navigator.serviceWorker\n    .register(swUrl)\n    .then(registration => {\n      registration.onupdatefound = () => {\n        const installingWorker = registration.installing;\n        if (installingWorker == null) {\n          return;\n        }\n        installingWorker.onstatechange = () => {\n          if (installingWorker.state === 'installed') {\n            if (navigator.serviceWorker.controller) {\n              // At this point, the updated precached content has been fetched,\n              // but the previous service worker will still serve the older\n              // content until all client tabs are closed.\n              console.log(\n                'New content is available and will be used when all ' +\n                  'tabs for this page are closed. See https://bit.ly/CRA-PWA.'\n              );\n\n              // Execute callback\n              if (config && config.onUpdate) {\n                config.onUpdate(registration);\n              }\n            } else {\n              // At this point, everything has been precached.\n              // It's the perfect time to display a\n              // \"Content is cached for offline use.\" message.\n              console.log('Content is cached for offline use.');\n\n              // Execute callback\n              if (config && config.onSuccess) {\n                config.onSuccess(registration);\n              }\n            }\n          }\n        };\n      };\n    })\n    .catch(error => {\n      console.error('Error during service worker registration:', error);\n    });\n}\n\nfunction checkValidServiceWorker(swUrl, config) {\n  // Check if the service worker can be found. If it can't reload the page.\n  fetch(swUrl)\n    .then(response => {\n      // Ensure service worker exists, and that we really are getting a JS file.\n      const contentType = response.headers.get('content-type');\n      if (\n        response.status === 404 ||\n        (contentType != null && contentType.indexOf('javascript') === -1)\n      ) {\n        // No service worker found. Probably a different app. Reload the page.\n        navigator.serviceWorker.ready.then(registration => {\n          registration.unregister().then(() => {\n            window.location.reload();\n          });\n        });\n      } else {\n        // Service worker found. Proceed as normal.\n        registerValidSW(swUrl, config);\n      }\n    })\n    .catch(() => {\n      console.log(\n        'No internet connection found. App is running in offline mode.'\n      );\n    });\n}\n\nexport function unregister() {\n  if ('serviceWorker' in navigator) {\n    navigator.serviceWorker.ready.then(registration => {\n      registration.unregister();\n    });\n  }\n}\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport * as serviceWorker from './serviceWorker';\n\nReactDOM.render(<App />, document.getElementById('root'));\n\n// If you want your app to work offline and load faster, you can change\n// unregister() to register() below. Note this comes with some pitfalls.\n// Learn more about service workers: https://bit.ly/CRA-PWA\nserviceWorker.unregister();\n"],"sourceRoot":""}