<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.17" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Manifold learning: Cosmos — Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Dimensionality%20reduction " data-tags="[[Dimensionality reduction]]" data-tiddler-title="Manifold learning"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="tiddlymap" class="tc-btn-invisible tc-btn-%24%3A%2Fplugins%2Ffelixhayashi%2Ftiddlymap%2Fmisc%2FquickConnectButton " title="Toggle TiddlyMap actions">


</button></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Manifold learning
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 29th November 2017 at 6:32am
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Dimensionality reduction
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p><em>aka nonlinear dimensionality reduction</em></p><p>nonlinear dimensionality reduction consist of two steps: first, they start with con - structing a representation of local affinity of the data points (typically, a sparsely connected graph). Second, the data points are embedded into a low-dimensional space, trying to preserve some criterion of the original affinity. For example, spectral embeddings tend to map points with many connec- tions between them to nearby locations, and multidimension- al scaling (MDS)-type methods try to preserve global information, such as graph geodesic distances. Examples of manifold learning include different flavors of MDS [26] , locally linear embedding [27] , sto- chastic neighbor embedding [28 ], spectral embeddings, such as Laplacian eigenmaps [29] and diffusion maps [30] , and deep models [31] . Instead of embedding the vertices, the graph structure can be pro - cessed by decomposing it into small subgraphs called motifs [36] or graphlets [37]. Finally, most recent approaches [32]– [34] tried to apply the successful word-embedding model [35] to graphs.</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=SFxypsvhhMQ#=44m" rel="noopener noreferrer" target="_blank">Input distribution lies in a manifold when there is structure/correlations in high dimensional space</a> – like <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Sloppy%2520model.html">Sloppy model</a>s</p><p><a class="tc-tiddlylink-external" href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Laplacian_eigenmaps" rel="noopener noreferrer" target="_blank">https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Laplacian_eigenmaps</a></p><p>Several efficient manifold learning techniques have been proposed.</p><ul><li><strong>Isometric feature mapping</strong> (ISOMAP) (Balasubramanian et al., 2002) estimates the geodesic distances on the manifold and uses them for projection. <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Multidimensional%2520scaling.html">Multidimensional scaling</a> with <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Geodesic.html">Geodesic</a> distance, basically. <a class="tc-tiddlylink-external" href="http://science.sciencemag.org/content/290/5500/2319?ijkey=4459d99d55dbcf7cf47149bee86b1e483a2b4437&amp;keytype2=tf_ipsecsha" rel="noopener noreferrer" target="_blank">A Global Geometric Framework for Nonlinear Dimensionality Reduction</a> – <a class="tc-tiddlylink-external" href="http://science.sciencemag.org/content/295/5552/7" rel="noopener noreferrer" target="_blank">The Isomap Algorithm and Topological Stability</a></li><li>Locality Preserving Projections (LPP)</li><li><strong>Locally linear embedding</strong> (LLE) (Roweis and Saul, 2000) projects data points to a low-dimensional space that preserves local geometric properties. <a class="tc-tiddlylink-external" href="http://www.robots.ox.ac.uk/~az/lectures/ml/lle.pdf" rel="noopener noreferrer" target="_blank">paper</a></li><li>Laplacian eigenmaps (LE) (Belkin and Niyogi, 2003) uses the weighted distance between two points as the loss function to get the dimension reduction results.</li><li>Local tangent space alignment (LTSA) (Zhang and Zha, 2004) constructs a local tangent space for each point and obtains the global low-dimensional embedding results through affine transformation of the local tangent spaces.</li></ul><p>Yan et al. (2007) present a general formulation known as graph embedding to unify different dimensionality reduction algorithms within a common framework.</p></div>


</div>

</p>

</section>
</body>
</html>
