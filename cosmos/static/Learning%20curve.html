<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.15" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Learning curve: Cosmos â€” Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Generalization%20error"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Learning curve
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 29th December 2018 at 12:38am
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Generalization error
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p>The theory of learning curves seems to be not well-known enough, but it's very useful. For example, it provides a rigorous derivation of the optimal regularization coefficient for L2 regularized least squares.
One can show the following. Assume data is generated as y=w_0*x + eta, where x are generated from Gaussian with unit correlation, and where eta is Gaussian noise with sigma^2 variance. Assume your task is: given training data {(x_1,y_1),...,(x_m,y_m)}, generated that way, find w that minimizes the regularized mean squared error</p><p>(1/m)sum_{i=1}^m (w*x_i - y_i)^2 + lambda *|w|^2</p><p>and you are interested in the generalization error</p><p>Expectation[(w*x - y)^2] upon sampling a new pair (x,y) according to the data generating process. In particular, we consider the average of Expectation[(w*x - y)], over training data samples (which determine w).</p><p>Then,</p><p>for lambda between 0 and sigma^2/|w_0|^2, the generalization error decreases with increasing lambda.</p><p>for lambda greater than sigma^2/|w_0|^2, the error increases with increasing lambda. However, it stays lower than for lambda = 0, until a certain point lambda_c. This point is determined by how aligned, in expectation, w_0 is with the principal components of the correlation matrix of the training data. If w_0 is aligned with principal components then lambda_c is close to sigma^2/|w_0|^2, and viceversa. So making lambda higher than the optimal (overregularization) is safer when the true weight vector is pointing along the non-principal components, where there is little data variance. The intuition is that if this is the case, then there is smaller signal-to-noise ratio, so regularization is more benefitial (as it makes the algo less sensitive to noise, damping down potential large variance in w).</p><p>I find it pretty neat that one can have a well principled and quantitative justification of regularization! And now you know how big you need to make the reg coefficient (under some assumptions) :)</p><p>See nice discussion in comments with Cristof here: <a class="tc-tiddlylink-external" href="https://www.facebook.com/guillermovalleperez/posts/10156593654876223" rel="noopener noreferrer" target="_blank">https://www.facebook.com/guillermovalleperez/posts/10156593654876223</a></p></div>


</div>

</p>

</section>
</body>
</html>
