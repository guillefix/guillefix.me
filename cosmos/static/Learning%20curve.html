<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.21" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Learning curve: Cosmos â€” All that is, or was, or ever will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">
<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists  tc-tagged-Generalization%20error" data-tags="[[Generalization error]]" data-tiddler-title="Learning curve"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Learning curve
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 10th December 2019 at 6:47pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">
<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Generalization error
</span>
<span class="tc-drop-down tc-reveal" hidden="true"></span></span></div>
</div>

<div class="tc-tiddler-body tc-reveal"><p><a class="tc-tiddlylink-external" href="https://papers.nips.cc/paper/1501-learning-curves-for-gaussian-processes.pdf" rel="noopener noreferrer" target="_blank">Learning curves for Gaussian processes </a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Gaussian%2520process%2520learning%2520curve.html">Gaussian process learning curve</a></p><p>Learning curves for deep learning seem to follow <a class="tc-tiddlylink tc-tiddlylink-missing" href="Power%2520law.html">Power law</a>s: <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1712.00409" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/1712.00409</a></p><hr><p>The theory of learning curves seems to be not well-known enough, but it's very useful. For example, it provides a rigorous derivation of the optimal regularization coefficient for L2 regularized least squares.
One can show the following. Assume data is generated as y=w_0*x + eta, where x are generated from Gaussian with unit correlation, and where eta is Gaussian noise with sigma^2 variance. Assume your task is: given training data {(x_1,y_1),...,(x_m,y_m)}, generated that way, find w that minimizes the regularized mean squared error</p><p>(1/m)sum_{i=1}^m (w*x_i - y_i)^2 + lambda *|w|^2</p><p>and you are interested in the generalization error</p><p>Expectation[(w*x - y)^2] upon sampling a new pair (x,y) according to the data generating process. In particular, we consider the average of Expectation[(w*x - y)], over training data samples (which determine w).</p><p>Then,</p><p>for lambda between 0 and sigma^2/|w_0|^2, the generalization error decreases with increasing lambda.</p><p>for lambda greater than sigma^2/|w_0|^2, the error increases with increasing lambda. However, it stays lower than for lambda = 0, until a certain point lambda_c. This point is determined by how aligned, in expectation, w_0 is with the principal components of the correlation matrix of the training data. If w_0 is aligned with principal components then lambda_c is close to sigma^2/|w_0|^2, and viceversa. So making lambda higher than the optimal (overregularization) is safer when the true weight vector is pointing along the non-principal components, where there is little data variance. The intuition is that if this is the case, then there is smaller signal-to-noise ratio, so regularization is more benefitial (as it makes the algo less sensitive to noise, damping down potential large variance in w).</p><p>I find it pretty neat that one can have a well principled and quantitative justification of regularization! And now you know how big you need to make the reg coefficient (under some assumptions) :)</p><p>See nice discussion in comments with Cristof here: <a class="tc-tiddlylink-external" href="https://www.facebook.com/guillermovalleperez/posts/10156593654876223" rel="noopener noreferrer" target="_blank">https://www.facebook.com/guillermovalleperez/posts/10156593654876223</a></p><hr><p><a class="tc-tiddlylink-external" href="http://www.ki.tu-berlin.de/fileadmin/fg135/publikationen/opper/Op01.pdf" rel="noopener noreferrer" target="_blank">Learning to generalize</a></p><p><a class="tc-tiddlylink-external" href="https://www.cis.upenn.edu/~mkearns/papers/statmech.pdf" rel="noopener noreferrer" target="_blank">Rigorous Learning Curve Bounds from Statistical Mechanics</a></p><p>entropy bias learning curve <a class="tc-tiddlylink-external" href="https://www.desmos.com/calculator/mb7g8wpxou" rel="noopener noreferrer" target="_blank">https://www.desmos.com/calculator/mb7g8wpxou</a></p><hr><p><u>TAB DUUUMPPP</u></p><p>URL list from Wednesday, May. 1 2019 23:08 PM
To copy this list, type [Ctrl] A, then type [Ctrl] C. </p><p>Exact learning curves for Gaussian process regression on large random graphs
<a class="tc-tiddlylink-external" href="http://papers.nips.cc/paper/3981-exact-learning-curves-for-gaussian-process-regression-on-large-random-graphs" rel="noopener noreferrer" target="_blank">http://papers.nips.cc/paper/3981-exact-learning-curves-for-gaussian-process-regression-on-large-random-graphs</a></p><p>Kernels and learning curves for Gaussian process regression on random graphs
<a class="tc-tiddlylink-external" href="http://papers.nips.cc/paper/3840-kernels-and-learning-curves-for-gaussian-process-regression-on-random-graphs" rel="noopener noreferrer" target="_blank">http://papers.nips.cc/paper/3840-kernels-and-learning-curves-for-gaussian-process-regression-on-random-graphs</a></p><p>Learning Curves for Gaussian Process Regression: Approximations and Bounds | Neural Computation | MIT Press Journals
<a class="tc-tiddlylink-external" href="https://www.mitpressjournals.org/doi/abs/10.1162/089976602753712990" rel="noopener noreferrer" target="_blank">https://www.mitpressjournals.org/doi/abs/10.1162/089976602753712990</a></p><p>Learning Curves for Gaussian Processes
<a class="tc-tiddlylink-external" href="https://papers.nips.cc/paper/1501-learning-curves-for-gaussian-processes.pdf" rel="noopener noreferrer" target="_blank">https://papers.nips.cc/paper/1501-learning-curves-for-gaussian-processes.pdf</a></p><p>Learning Curves for Gaussian Processes Models: Fluctuations and Universality | SpringerLink
<a class="tc-tiddlylink-external" href="https://link.springer.com/chapter/10.1007/3-540-44668-0_39" rel="noopener noreferrer" target="_blank">https://link.springer.com/chapter/10.1007/3-540-44668-0_39</a></p><p>Learning Curves for Gaussian Processes via Numerical Cubature Integration | SpringerLink
<a class="tc-tiddlylink-external" href="https://link.springer.com/chapter/10.1007/978-3-642-21735-7_25" rel="noopener noreferrer" target="_blank">https://link.springer.com/chapter/10.1007/978-3-642-21735-7_25</a></p><p>Learning-curves-for-Gaussian-process-regression-on-random-graphs-effects-of-graph-structure.pdf
<a class="tc-tiddlylink-external" href="https://www.researchgate.net/profile/Peter_Sollich/publication/255645199_Learning_curves_for_Gaussian_process_regression_on_random_graphs_effects_of_graph_structure/links/00b7d536d37136de6f000000/Learning-curves-for-Gaussian-process-regression-on-random-graphs-effects-of-graph-structure.pdf" rel="noopener noreferrer" target="_blank">https://www.researchgate.net/profile/Peter_Sollich/publication/255645199_Learning_curves_for_Gaussian_process_regression_on_random_graphs_effects_of_graph_structure/links/00b7d536d37136de6f000000/Learning-curves-for-Gaussian-process-regression-on-random-graphs-effects-of-graph-structure.pdf</a></p><p>Urry: Random walk kernels and learning curves for... - Google Scholar
<a class="tc-tiddlylink-external" href="https://scholar.google.co.uk/scholar?cites=12048397498288311790&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en" rel="noopener noreferrer" target="_blank">https://scholar.google.co.uk/scholar?cites=12048397498288311790&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en</a></p><p>Upper and Lower Bounds on the Learning Curve for Gaussian Processes | SpringerLink
<a class="tc-tiddlylink-external" href="https://link.springer.com/article/10.1023/A:1007601601278" rel="noopener noreferrer" target="_blank">https://link.springer.com/article/10.1023/A:1007601601278</a></p><p>Replica theory for learning curves for Gaussian processes on random graphs - IOPscience
<a class="tc-tiddlylink-external" href="https://iopscience.iop.org/article/10.1088/1751-8113/45/42/425005/meta" rel="noopener noreferrer" target="_blank">https://iopscience.iop.org/article/10.1088/1751-8113/45/42/425005/meta</a></p><p>Feature selection and learning curves of a multilayer perceptron chromosome classifier - IEEE Conference Publication
<a class="tc-tiddlylink-external" href="https://ieeexplore.ieee.org/document/576994" rel="noopener noreferrer" target="_blank">https://ieeexplore.ieee.org/document/576994</a></p><p>[1412.4869] Expectation propagation as a way of life: A framework for Bayesian inference on partitioned data
<a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1412.4869" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/1412.4869</a></p><p>[1812.11118] Reconciling modern machine learning and the bias-variance trade-off
<a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1812.11118" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/1812.11118</a></p><p>1409.6179.pdf
<a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1409.6179.pdf" rel="noopener noreferrer" target="_blank">https://arxiv.org/pdf/1409.6179.pdf</a></p><p>1805.08522.pdf
<a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1805.08522.pdf" rel="noopener noreferrer" target="_blank">https://arxiv.org/pdf/1805.08522.pdf</a></p><p>Continuous-Space Gaussian Process Regression and Generalized Wiener Filtering with Application to Learning Curves | SpringerLink
<a class="tc-tiddlylink-external" href="https://link.springer.com/chapter/10.1007/978-3-642-38886-6_17" rel="noopener noreferrer" target="_blank">https://link.springer.com/chapter/10.1007/978-3-642-38886-6_17</a></p><p>DLMF: 6.12 Asymptotic Expansions
<a class="tc-tiddlylink-external" href="https://dlmf.nist.gov/6.12" rel="noopener noreferrer" target="_blank">https://dlmf.nist.gov/6.12</a></p><p>Haussler: Rigorous learning curve bounds from statistical... - Google Scholar
<a class="tc-tiddlylink-external" href="https://scholar.google.com/scholar?safe=off&amp;biw=1745&amp;bih=956&amp;um=1&amp;ie=UTF-8&amp;lr&amp;cites=2913399941842643655&amp;authuser=0" rel="noopener noreferrer" target="_blank">https://scholar.google.com/scholar?safe=off&amp;biw=1745&amp;bih=956&amp;um=1&amp;ie=UTF-8&amp;lr&amp;cites=2913399941842643655&amp;authuser=0</a></p><p>JST_pacbayes-JohnShawe.pdf
<a class="tc-tiddlylink-external" href="http://web.cse.ohio-state.edu/mlss09/mlss09_talks/1.june-MON/JST_pacbayes-JohnShawe.pdf" rel="noopener noreferrer" target="_blank">http://web.cse.ohio-state.edu/mlss09/mlss09_talks/1.june-MON/JST_pacbayes-JohnShawe.pdf</a></p><p>1206.1901.pdf
<a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1206.1901.pdf" rel="noopener noreferrer" target="_blank">https://arxiv.org/pdf/1206.1901.pdf</a></p><p>[1904.11955] On Exact Computation with an Infinitely Wide Neural Net
<a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1904.11955" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/1904.11955</a></p><p>1904.11694.pdf
<a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1904.11694.pdf" rel="noopener noreferrer" target="_blank">https://arxiv.org/pdf/1904.11694.pdf</a></p><p>thesis.dvi
<a class="tc-tiddlylink-external" href="https://www.bcs.org/upload/pdf/mseeger.pdf" rel="noopener noreferrer" target="_blank">https://www.bcs.org/upload/pdf/mseeger.pdf</a></p><p>shell script - How would I use GNU Parallel for this while loop? - Unix &amp; Linux Stack Exchange
<a class="tc-tiddlylink-external" href="https://unix.stackexchange.com/questions/229034/how-would-i-use-gnu-parallel-for-this-while-loop" rel="noopener noreferrer" target="_blank">https://unix.stackexchange.com/questions/229034/how-would-i-use-gnu-parallel-for-this-while-loop</a></p><p>seeger02a.dvi
<a class="tc-tiddlylink-external" href="http://www.jmlr.org/papers/volume3/seeger02a/seeger02a.pdf" rel="noopener noreferrer" target="_blank">http://www.jmlr.org/papers/volume3/seeger02a/seeger02a.pdf</a></p><p>Malzahn: Learning curves for Gaussian processes models:... - Google Scholar
<a class="tc-tiddlylink-external" href="https://scholar.google.co.uk/scholar?cites=152224715068156717&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en" rel="noopener noreferrer" target="_blank">https://scholar.google.co.uk/scholar?cites=152224715068156717&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en</a>
</p></div>



</div>

</p>
</section>
</body>
</html>
