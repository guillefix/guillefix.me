<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.15" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Neuromorphic computing: Cosmos — All that is, or was, or ever will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Neuromorphic%20engineering"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Neuromorphic computing
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 10th December 2017 at 4:24pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Neuromorphic engineering
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Computer.html">Computing system</a>s that imitate the working of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Neuronal%2520network.html">Neuronal network</a>s, at hardware and/or software level. A basic model is the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Spiking%2520neural%2520network.html">Spiking neural network</a>. One advantage is that they tend to be more energy and resource-efficient.</p><p><a class="tc-tiddlylink-external" href="https://www.wikiwand.com/en/Neuromorphic_engineering" rel="noopener noreferrer" target="_blank">https://www.wikiwand.com/en/Neuromorphic_engineering</a></p><p>Numenta</p><p>IBM TrueNorth.  – </p><h3 class=""><u><a class="tc-tiddlylink-external" href="http://arxiv.org/abs/1603.08270" rel="noopener noreferrer" target="_blank">Convolutional Networks for Fast, Energy-Efficient Neuromorphic Computing</a></u></h3><p>This is direct evidence that an “integrate-and-spike” mechanism has the similar computational capability as the more proven ANNs.   The IBM paper however highlighted one major weakness of SNN.  That is, training of the TrueNorth system required simulation of back-propagation using another conventional GPU:</p><p>Training was performed offline on conventional GPUs, using a library of custom training layers built upon functions from the MatConvNet toolbox. Network specication and training complexity using these layers is on par with standard deep learning.</p><p>See more interesting stuff here: <a class="tc-tiddlylink-external" href="http://blog.alluviate.com/?p=123" rel="noopener noreferrer" target="_blank">Microglia: A Biologically Plausible Basis for Back-Propagation</a></p><p>There however has been no biological evidence of a structural mechanism of “back-propagation” in biological brains.  Yoshua Bengio published a paper in 2015 (see:  <a class="tc-tiddlylink-external" href="http://arxiv.org/abs/1502.04156" rel="noopener noreferrer" target="_blank">http://arxiv.org/abs/1502.04156</a> ) “Towards Biologically Plausible Deep Learning”.  The investigation attempts to explain a mechanism for back-propagation exists in Spike-Timing-Dependent Plasticity (STDP) of biological neurons. </p><p>It is however questionable whether neurons are  able to learn by themselves without the need of an external feedback pathway that spans multiple layers.</p><p>There is however an alternative mechanism that recently has been discovered that may be a more convincing argument that is based on a structure that is independent of the brain’s neurons.   There is a large class of cells in the Brain called Microglia ( see: <a class="tc-tiddlylink-external" href="https://www.technologyreview.com/s/601137/the-rogue-immune-cells-that-wreck-the-brain" rel="noopener noreferrer" target="_blank">https://www.technologyreview.com/s/601137/the-rogue-immune-cells-that-wreck-the-brain</a> ) that are responsible for regulating the neurons and their connectivity.</p><p>In summary, biological brains have a regulatory mechanism in the form of microglia that are highly dynamic in regulating synapse connectivity and pruning neural growth.   The activity is most pronounced during sleep. SNNs have been shown to have inference capabilities equivalent to Convolution Networks.  SNNs however have not shown to effectively learn on their own without a ‘back-propagation’ mechanism.   This mechanism is most plausibly provided by the microglia.</p><p><a class="tc-tiddlylink-external" href="http://www.pnas.org/content/113/41/11387.short" rel="noopener noreferrer" target="_blank">Energy-efficient neural network chips approach human recognition capabilities</a></p></div>


</div>

</p>

</section>
</body>
</html>
