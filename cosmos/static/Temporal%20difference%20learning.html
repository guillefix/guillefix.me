<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.17" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Temporal difference learning: Cosmos — Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Reinforcement%20learning " data-tags="[[Reinforcement learning]]" data-tiddler-title="Temporal difference learning"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Temporal difference learning
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 9th April 2018 at 6:27pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Reinforcement learning
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p>An approach to <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Reinforcement%2520learning.html">Reinforcement learning</a> (particularly <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Model-free%2520reinforcement%2520learning.html">Model-free reinforcement learning</a>) – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=34m10s" rel="noopener noreferrer" target="_blank">video</a></p><p>TD learning is a combination of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Monte%2520Carlo.html">Monte Carlo</a> ideas and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Dynamic%2520programming.html">Dynamic programming</a> (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment's dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap).</p><p>Here we discuss the basic <a class="tc-tiddlylink tc-tiddlylink-resolves" href="On-policy%2520learning.html">On-policy learning</a>s. See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Off-policy%2520learning.html">Off-policy learning</a> for their extensions to off-policy learning.</p><p><a class="tc-tiddlylink-external" href="http://videolectures.net/deeplearning2017_sutton_td_learning/" rel="noopener noreferrer" target="_blank">Sutton -- TD learning</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=EeMCEQa85tw" rel="noopener noreferrer" target="_blank">DeepMind's Richard Sutton - The Long-term of AI &amp; Temporal-Difference Learning</a> <small><a class="tc-tiddlylink-external" href="https://youtu.be/Qgd3OK5DZWI?t=23m30s" rel="noopener noreferrer" target="_blank">Demis Hassabis talks about it here</a></small></p><h1 class=""><u>TD <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Policy%2520evaluation.html">Policy evaluation</a></u></h1><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h13m" rel="noopener noreferrer" target="_blank">intuition for why we update the current value function assuming the value function at the state after one step, instead of updating it the other way</a></p><h2 class=""><u>TD0</u></h2><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=36m10s" rel="noopener noreferrer" target="_blank">intro vid</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=dV80NAlEins&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;index=16#t=19m30s" rel="noopener noreferrer" target="_blank">vid</a></p><p>A kind of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Gradient%2520descent.html">Gradient descent</a> to converge to solution to V(s) that satisfies <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Bellman%2520equation.html">Bellman equation</a></p><p><a class="tc-tiddlylink-external" href="https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node60.html" rel="noopener noreferrer" target="_blank">https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node60.html</a></p><p>Proven to work (converge to true value function, in the case of table-lookup representation. But in the case of representing value function in some other ways (parametric function approximation), <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=53m10s" rel="noopener noreferrer" target="_blank">there are subtleties.</a></p><h3 class=""><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=59m" rel="noopener noreferrer" target="_blank">Simple example comparing monte carlo vs TD0</a>.</h3><p>If you let TD0 converge on a limited sample (a limited set of episodes from an MDP), it will converge to the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Maximum%2520likelihood.html">Maximum likelihood</a> estimate <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Markov%2520reward%2520process.html">MRP</a> for that data. <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h3m30s" rel="noopener noreferrer" target="_blank">TD makes use of the Markov property</a></p><h3 class="">Optimality of TD(0)</h3><p>See section 6.3 of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Sutton-Barto.html">Sutton-Barto</a></p><p><u>n-step look-ahead</u></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h12m5s" rel="noopener noreferrer" target="_blank">intro vid</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h16m" rel="noopener noreferrer" target="_blank">video</a></p><p>We can take <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span></span></span></span></span> steps of the (unknown) MDP, instead of 1. Monte Carlo <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Model-free%2520reinforcement%2520learning.html">Model-free reinforcement learning</a> is when <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mo>→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">n \rightarrow \infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span><span class="mrel">→</span><span class="mord mathrm">∞</span></span></span></span></span></p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="TD(lambda).html">TD(lambda)</a></u></h2><h1 class=""><u>TD control</u></h1><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=38m45s" rel="noopener noreferrer" target="_blank">introduction to TD learning for control</a></p><p>TD prediction + policy improvement (<a class="tc-tiddlylink tc-tiddlylink-missing" href="Generalized%2520policy%2520iteration.html">GPI</a>)</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Sarsa.html">Sarsa</a></p><hr><p>Related with <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Actor-critic%2520method.html">Actor-critic method</a>s</p></div>


</div>

</p>

</section>
</body>
</html>
