<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.15" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Spectral methods in reinforcement learning: Cosmos â€” Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Reinforcement%20learning"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Spectral methods in reinforcement learning
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 9th November 2018 at 10:06pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Reinforcement learning
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p><em>aka spectral reinforcement learning</em></p><p><a class="tc-tiddlylink-external" href="http://www.jmlr.org/papers/volume8/mahadevan07a/mahadevan07a.pdf" rel="noopener noreferrer" target="_blank">Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=w7QCLFR8hJU" rel="noopener noreferrer" target="_blank">Reinforcement Learning of POMDP's using Spectral Methods</a></p><p>The theory of spectral reinforcement learning has some beautiful results that I wasn't aware of, which we discussed in the last deep rl reading group, as there was a guy there who wrote his whole thesis about it.</p><p>Given a policy, a Markov decision process (the set up of any Reinforcement Learning problem) is a Markov chain. A Markov chain is a weighted graph. For weighted graphs (and their infinite generalizations) one can define a matrix (operator, generally) whose eigenvectors (egienfunctions) give a basis in which one can represent/embed the states (each eigenvector gives a number to each state. For each state therefore one has a vector of numbers, one per eigenvector (one can multiply the numbers by the corresponding eigenvalues for extra benefit)).</p><p>If one chooses the uniform policy, this state representation has some impressive theoretical properties. If one embeds states in this space, then the Euclidean distance to a goal state can be used as a proto-value function. More generally one can consider the weighted sum of distances if each state has some reward. If one considers the set of all possible reward functions, then this proto-value function is optimal in the sense of being closest in average square distance to all possible value functions.</p><p>Basically, what this means is this: if you don't know what your task is, then there is an optimal way to represent/embed states, so that any future task can be solved by just walking in a straight line in this embedding space (which you can think as the way states are represented in your brain).</p><p>If you know something more about the task, then there is apparently extensions of this result based on &quot;successor representations&quot; which use the optimal policy, but I don't know much about those yet.</p><p>The way these representations look is very natural. Eigenfunctions of the Laplacian are the same as the modes in which the graph/space would vibrate if you hit it, and basically cluster things, so that one eigenfunction may represent &quot;this room&quot;, another &quot;that wing of the building&quot;, another &quot;the top of the desk&quot;, etc. which are nice and meaningful.</p><p>The issue is that for some reason, not well understood yet, the representation seems quite sensitive to the policy chosen in defining the Markov chain not being quite right, so that it unfortunately turned out to be quite unstable in realistic situations. The poor guy told me: &quot;it's all theoretical beautiful, but not useful, don't recommend it, just go with VAEs&quot; (one could feel the PhD pain in his words...) Still, I find the idea captivating, and in fact it turns out that I was working on a simple version of this in my summer project, and found out that for solving fully-known mazes, the system worked super well (much better than I could get a neural network to learn)</p><p>Recently, there was this paper which is proposing a way to scale up the method, and they say it's promising. The story is not over yet... Also grid cells in the brain seem to behave in similar ways, and a neuroscientist I met believes they take part in key elements of human cognition, currently laking in AI systems.</p><p>We'll see</p><p>Some refs: <a class="tc-tiddlylink-external" href="http://www.jmlr.org/papers/v14/boehmer13a.html" rel="noopener noreferrer" target="_blank">http://www.jmlr.org/papers/v14/boehmer13a.html</a>
<a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1810.04586" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/1810.04586</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1606.05312.pdf" rel="noopener noreferrer" target="_blank">https://arxiv.org/pdf/1606.05312.pdf</a> spectral RL
<a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1706.04859.pdf" rel="noopener noreferrer" target="_blank">https://arxiv.org/pdf/1706.04859.pdf</a>
spectral RL <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1806.02215" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/1806.02215</a>
cognitive map <a class="tc-tiddlylink-external" href="https://www.biorxiv.org/content/early/2018/07/10/365593" rel="noopener noreferrer" target="_blank">https://www.biorxiv.org/content/early/2018/07/10/365593</a></p><p>..........</p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1806.02813" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/1806.02813</a></p></div>


</div>

</p>

</section>
</body>
</html>
