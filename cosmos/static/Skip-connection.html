<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.17" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Skip-connection: Cosmos — Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Artificial%20neural%20network " data-tags="[[Artificial neural network]]" data-tiddler-title="Skip-connection"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Skip-connection
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 20th August 2017 at 7:11pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Artificial neural network
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p>Found in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Residual%2520neural%2520network.html">Residual neural network</a>s, and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Highway%2520network.html">Highway network</a>s.</p><p>See connections to <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Spiking%2520neural%2520network.html">Spiking neural network</a>s <a class="tc-tiddlylink-external" href="https://gingkoapp.com/app#7887cb3f9b23d73d7600001f" rel="noopener noreferrer" target="_blank">here</a>. See also <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Time-delay%2520neural%2520network.html">Time-delay neural network</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1604.03640" rel="noopener noreferrer" target="_blank">Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=x1kf4Zojtb0#t=16m52.5s" rel="noopener noreferrer" target="_blank">Skip connections</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1701.09175" rel="noopener noreferrer" target="_blank">Skip Connections as Effective Symmetry-Breaking</a>. We argue that skip connections help break symmetries inherent in the loss landscapes of deep networks, leading to drastically simplified landscapes. We find, however, that skip connections confer additional benefits over and above symmetry-breaking, such as the ability to deal effectively with the vanishing gradients problem.</p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1612.07771.pdf" rel="noopener noreferrer" target="_blank">HIGHWAY AND RESIDUAL NETWORKS LEARN UNROLLED ITERATIVE ESTIMATION</a></p><p><a class="tc-tiddlylink-external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bell_Inside-Outside_Net_Detecting_CVPR_2016_paper.pdf" rel="noopener noreferrer" target="_blank">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</a> . Contextual information outside
the region of interest is integrated using spatial recurrent
neural networks. Inside, we use skip pooling to extract
information at multiple scales and levels of abstraction – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=-QNDu_8Q7_A&amp;feature=youtu.be" rel="noopener noreferrer" target="_blank">video</a> –<a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=-QNDu_8Q7_A&amp;feature=youtu.be#t=2m" rel="noopener noreferrer" target="_blank">SKIP CONNECTIONS -- what and where</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1611.05552" rel="noopener noreferrer" target="_blank">DelugeNets: Deep Networks with Massive and Flexible Cross-layer Information Inflows</a>
 – <a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1608.06993.pdf" rel="noopener noreferrer" target="_blank">Densely Connected Convolutional Networks</a>
 – <a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1411.5752.pdf" rel="noopener noreferrer" target="_blank">Hypercolumns for Object Segmentation and Fine-grained Localization</a> – </p><p>See applications in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Image%2520segmentation.html">Image segmentation</a>, and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Object%2520detection.html">Object detection</a></p><p>I think skip-connections can simulate <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Polychronization.html">Polychronization</a></p><p><a class="tc-tiddlylink-external" href="https://pdfs.semanticscholar.org/1a6b/8d832037f1565f6b81e3743fdacd227c6009.pdf" rel="noopener noreferrer" target="_blank">Recurrent Residual Learning for Sequence Classification</a> – We show that
for sequence classification tasks, incorporating
residual connections into recurrent structures
yields similar accuracy to Long Short
Term Memory (LSTM) RNN with much fewer
model parameters. – <a class="tc-tiddlylink-external" href="https://publish.illinois.edu/yirenwang/home/emnlp16source/" rel="noopener noreferrer" target="_blank">Code</a></p><p><a class="tc-tiddlylink-external" href="http://papers.nips.cc/paper/6303-architectural-complexity-measures-of-recurrent-neural-networks.pdf" rel="noopener noreferrer" target="_blank">Architectural complexity of RNNs</a></p><p>Deep transition RNN - <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1312.6026" rel="noopener noreferrer" target="_blank">How to Construct Deep Recurrent Neural Networks</a></p></div>


</div>

</p>

</section>
</body>
</html>
