<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.21" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Overfitting and underfitting: Cosmos â€” All that is, or was, or ever will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">
<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists  tc-tagged-Learning%20theory" data-tags="[[Learning theory]]" data-tiddler-title="Overfitting and underfitting"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Overfitting and underfitting
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 6th March 2017 at 7:42pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">
<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Learning theory
</span>
<span class="tc-drop-down tc-reveal" hidden="true"></span></span></div>
</div>

<div class="tc-tiddler-body tc-reveal"><p>Overfitting and underfitting refer to ways of misstraining a model, i.e., making it have poor generalization error, compared to the optimal model.</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=tojaGtMPo5U&amp;list=PLA89DCFA6ADACE599&amp;index=9#t=4m1.5s" rel="noopener noreferrer" target="_blank">Bias-variance tradeoff</a>, see also <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=tojaGtMPo5U&amp;list=PLA89DCFA6ADACE599&amp;index=9#t=1h03m40s" rel="noopener noreferrer" target="_blank">Relation to bias/variance tradeoff</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=tojaGtMPo5U&amp;list=PLA89DCFA6ADACE599&amp;index=9#t=1h08m20s" rel="noopener noreferrer" target="_blank">training error/generalization error</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Underfitting.html">Underfitting</a>. A learning algorithm with a lot of <strong>bias</strong>, meaning that they impose a lot of a priori structure/assumptions to the fitted functions. These, however, tend to have low <strong>variance</strong>, meaning that the fitted function doesn't tend to vary much when different training data sampled from the same process are used, they are <em>stable</em></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Overfitting.html">Overfitting</a>. A learning algorithm with low bias (it is more <em>flexible</em>), which however has a lot of variance, as it fits the idiosyncrasies of the data; it fits the noise.</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=JfkbyODyujw&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=14#t=6m45s" rel="noopener noreferrer" target="_blank">See explanation here</a> and <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Fs-raHUnF2M&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=16#t=9m05s" rel="noopener noreferrer" target="_blank">here</a></p><p><img src="https://lh3.googleusercontent.com/1vNipHvUofw7v2XurbRcoKWdrha4XwW2Wg6iv_CjnPJ7yaJeLuOGPHEXP5r0bHiXDa5jXmi3gXWzRs-rnOmoWT2qdrpDlhqoPaINOW1e8wCnkcMmsfjL5I7MAnuysZNkA0ZS-AduSU6My_vj8QjrLwgU7PtqeOxEmOHYOzJMm1COtI55peywxXwYc4Ot0XMg3WSk4ctE620Fg-kQuA8Zw86ejVU0wPx4C6f-yYJYDol4KmH_zV43EJREoK0ZJaU0v4j54Luq0_enrS9FA4oPcWX5v4h6hTCXJq3aubFRI-HBAP0Az3Js3cA9ZxPQV0U-1MZBCEdfI-0b87bEVSEAvZ7vsWTfyadsG43bfwc8ZGr4XRhXWYVlGj48WxrpQyTPFhPQMXNoiRURzx5bm4ZHukhomdEE98JJ4c5XqhybUHdIk6qJbUS7BXjcYaBlm3z8bGiBlPtDSdt61a59mbotPi7DS3N-LdHrHUd3PXtG59t_5fHfKi3WpqNS_dJOefgRukPJ0OAK4fE579XHNw_8l0Fi2mAqsP7Y8WNm1lg8yXQI2c6hrlGzWt2jO_4it_Zef_2r=w1269-h675-no"></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=qz9bKfOqd0Y&amp;index=5&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw#t=25m43s" rel="noopener noreferrer" target="_blank">Deep Learning Lecture 5: Regularization, model complexity and data complexity (part 2)</a></p><p>So the simplest model that works seems to work best most of the time. Seems like an example of Occam's razor, and thus related to Solomonoff's ideas on inference (see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Algorithmic%2520information%2520theory.html">Algorithmic information theory</a>). Epicurus principle also related to Bayesian inference, because we give a distribution over models, but we keep all of them.</p><p>Hmm, also your error can't be smaller than the fundamental noise in the data. Well it can, but your model will at best be wasteful then.</p></div>



</div>

</p>
</section>
</body>
</html>
