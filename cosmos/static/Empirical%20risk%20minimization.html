<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.15" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Empirical risk minimization: Cosmos — Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Learning%20theory"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Empirical risk minimization
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 29th November 2017 at 1:20pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Learning theory
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p>Risk minimization (see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Learning%2520theory.html">Learning theory</a>), requires knowing the joint probability distribution <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(x,y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></span>, so one often uses the sample mean of the risk as an estimator for the expected value of the risk. Minimizing this empirical quantity is called empirical risk minimization (ERM).</p><p>Depending on the form of the risk function, this optimization problem may be <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Convex%2520optimization.html">convex</a> or non-convex. If one uses a 0-1 loss function, the problem is non-convex, and finding its solution is <a class="tc-tiddlylink tc-tiddlylink-resolves" href="NP-hard.html">NP-hard</a>. A smoothed loss function may convert it into convex problem, solvable by <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Gradient%2520descent.html">Gradient descent</a>.</p><h3 class=""><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=5adNQvSlF50&amp;index=7&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH" rel="noopener noreferrer" target="_blank">Neural networks [2.1] : Training neural networks - empirical risk minimization</a></h3><p>Empirical risk minimization is thus defined as the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Optimization.html">Optimization</a> problem of minimizing</p><p><span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mn>1</mn></mrow><mrow><mi>m</mi></mrow></mfrac><msub><mo>∑</mo><mi>i</mi></msub><mi>l</mi><mo>(</mo><mi>f</mi><mo>(</mo><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo separator="true">;</mo><mrow><mi>θ</mi></mrow><mo>)</mo><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo><mo>+</mo><mi>λ</mi><mi mathvariant="normal">Ω</mi><mo>(</mo><mrow><mi>θ</mi></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">\frac{1}{m} \sum\limits_i l(f(x^{(i)}; \mathbf{\theta}), y ^{(i)}) + \lambda  \Omega (\mathbf{\theta})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8879999999999999em;"></span><span class="strut bottom" style="height:1.865669em;vertical-align:-0.977669em;"></span><span class="base textstyle uncramped"><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.345em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">m</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.394em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mop op-limits"><span class="vlist"><span style="top:0.877669em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span style="top:-0.000004999999999977245em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span><span class="op-symbol small-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mopen">(</span><span class="mord mathit">i</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">;</span><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mopen">(</span><span class="mord mathit">i</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mbin">+</span><span class="mord mathit">λ</span><span class="mord mathrm">Ω</span><span class="mopen">(</span><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span><span class="mclose">)</span></span></span></span></span></p><p>where <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span></span> is our <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Model.html">Model</a> (hypothesis, that depend on the model parameters <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>θ</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{\theta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span></span></span>; <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.01968em;">l</span></span></span></span></span> is the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Loss%2520function.html">Loss function</a>, <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Ω</mi><mo>(</mo><mrow><mi>θ</mi></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex"> \Omega (\mathbf{\theta})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">Ω</span><span class="mopen">(</span><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span><span class="mclose">)</span></span></span></span></span> is the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Regularization.html">regularizer</a>. <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">λ</span></span></span></span></span> is a hyperparameter that balances the two terms.</p><p>When we add the regularizer, ERM is called <em>structural risk minimization</em>.</p><h3 class=""><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=aILazXK059Y&amp;t=12m30s" rel="noopener noreferrer" target="_blank">another video explanation</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=9-oxo_k69qs#t=23m40s" rel="noopener noreferrer" target="_blank">Another good lecture</a></h3><hr><p><sub><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=tojaGtMPo5U&amp;list=PLA89DCFA6ADACE599&amp;index=9#t=7m35s" rel="noopener noreferrer" target="_blank">Setting up formally a linear classifier, to explore the problems of learning theory, and using empirical risk minimization as learning principle</a></sub></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=tojaGtMPo5U&amp;list=PLA89DCFA6ADACE599&amp;index=9#t=10m30s" rel="noopener noreferrer" target="_blank">Intro to ERM</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=tojaGtMPo5U&amp;list=PLA89DCFA6ADACE599&amp;index=9#t=12m20s" rel="noopener noreferrer" target="_blank">def</a></p></div>


</div>

</p>

</section>
</body>
</html>
