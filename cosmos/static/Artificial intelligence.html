<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.17" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Artificial intelligence: Cosmos — Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Intelligence tc-tagged-Computer%20Science%20and%20IT " data-tags="Intelligence [[Computer Science and IT]]" data-tiddler-title="Artificial intelligence"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Artificial intelligence
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 14th July 2018 at 2:58pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Computer Science and IT
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
<span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Intelligence
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p>See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Machine%2520learning.html">Machine learning</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Intelligence.html">Intelligence</a></p><p><a class="tc-tiddlylink-external" href="http://www.scholarpedia.org/article/Encyclopedia:Computational_intelligence" rel="noopener noreferrer" target="_blank">Computational intelligence - Scholarpedia</a></p><p><a class="tc-tiddlylink-external" href="http://bit.do/oxtorch" rel="noopener noreferrer" target="_blank">http://bit.do/oxtorch</a></p><p><a class="tc-tiddlylink-external" href="https://www.goodai.com/roadmap" rel="noopener noreferrer" target="_blank">GoodAI roadmap</a></p><p><a class="tc-tiddlylink-external" href="https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/" rel="noopener noreferrer" target="_blank">Oxford course (with video)</a> <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Deep%2520learning.html">Deep learning</a>.</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yDLKJtOVx5c&amp;list=PLD0F06AA0D2E8FFBA" rel="noopener noreferrer" target="_blank">Youtube playlist by mathematicalmonk</a></p><p>Hugo latochelle YB videos</p><p>Read Neural Turing machines paper</p><p>See also: <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Evolutionary%2520computing.html">Evolutionary computing</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Bio-inspired%2520computing.html">Bio-inspired computing</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Sloppy%2520systems.html">Sloppy systems</a></p><p><a class="tc-tiddlylink-external" href="http://events.technologyreview.com/video/watch/dileep-george-vicarious-ai-work/" rel="noopener noreferrer" target="_blank">Artificial Intelligence At Work</a> (<a class="tc-tiddlylink-external" href="http://www.vicarious.com/" rel="noopener noreferrer" target="_blank">http://www.vicarious.com/</a>)</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=mVvxCZI2P4E" rel="noopener noreferrer" target="_blank">Drop Everything to Work on Artificial Intelligence - Foresight Institute</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=oPyCHwPS04E" rel="noopener noreferrer" target="_blank">Creating Human-Level AI: How and When - Ray Kurzweil</a></p><p><a class="tc-tiddlylink-external" href="http://aima.cs.berkeley.edu/" rel="noopener noreferrer" target="_blank">http://aima.cs.berkeley.edu/</a></p><p><a class="tc-tiddlylink-external" href="https://twitter.com/guillefix/lists/ai" rel="noopener noreferrer" target="_blank">Twitter AI list</a></p><hr><p><strong><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Artificial%2520intelligence.html">Artificial intelligence</a></strong> (<strong>AI</strong>) has the overall goal of understanding and engineering <em>intelligence</em>, behaviour that involves understanding, and higher <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Cognitive%2520science.html">cognitive</a> functions. It is a broad and very interdisciplinary field. It feeds to and from <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Machine%2520learning.html">Machine learning</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Logic.html">Logic</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Cognitive%2520science.html">Cognitive science</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Neuroscience.html">Neuroscience</a>, etc.</p><h2 class=""><u>Approaches</u></h2><ul><li>Connectionists. Deep learning, artificial neural network. Backpropagatiob</li><li>Evolutionaries</li><li>Bayesians. Bayesian networks.</li><li>Symbolists</li><li>Analogizers.</li></ul><h3 class=""><u>AI and complex systems</u></h3><p><a class="tc-tiddlylink-external" href="http://pure.abdn.ac.uk:8080/portal/en/researchoutput/synthetic-biology-routes-to-bioartificial-intelligence(cb7d62e5-5877-4659-8286-ba1ab187fbc4).html" rel="noopener noreferrer" target="_blank">Synthetic biology routes to bio-artificial intelligence</a></p><p><a class="tc-tiddlylink-external" href="https://www.researchgate.net/profile/Richard_Loosemore/publication/228680056_Complex_systems_artificial_intelligence_and_theoretical_psychology/links/02bfe50fea30911805000000.pdf?origin=publication_detail" rel="noopener noreferrer" target="_blank">Complex Systems, Artificial Intelligence and Theoretical Psychology</a></p><hr><p><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="AI%2520safety.html">AI safety</a></u></p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-missing" href="General%2520artificial%2520intelligence.html">General artificial intelligence</a></u></h3><p><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Universal%2520AI.html">Universal AI</a> theory</u></p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Explainable%2520artificial%2520intelligence.html">Explainable artificial intelligence</a></u></h3><hr><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Integrating%2520symbols%2520into%2520deep%2520learning.html">Integrating symbols into deep learning</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Program%2520induction.html">Program induction</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Neural%2520networks%2520with%2520memory.html">Neural networks with memory</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Intelligence.html">Intelligence</a></p><hr><p>thinking perception action. Loops b?w them</p><p>Oxford's society <a class="tc-tiddlylink tc-tiddlylink-resolves" href="OxAI.html">OxAI</a></p><hr><p><strong>Machine intelligence</strong>, is essentially a synonym of AI, but with the connotation of using machines and computers to create and understand intelligence. The biggest part of it, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Machine%2520learning.html">Machine learning</a>, deals with the problem of <u>learning a model</u> from data so as to solve (mostly) <u>predictive</u> tasks.</p><hr><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Applications%2520of%2520AI.html">Applications of AI</a></u></h3><hr><p><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="AI%2520companies%2520and%2520projects.html">Companies and projects</a></u></p><hr><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Miscellaneous%2520notes%2520from%2520first%2520Nando's%2520first%2520deep%2520learning%2520lecture.html">Miscellaneous notes from first Nando's first deep learning lecture</a></p><hr><p>Challenges: <a class="tc-tiddlylink tc-tiddlylink-resolves" href="One-shot%2520learning.html">One-shot learning</a>, multi-task &amp; transfer learning, scaling and energy efficiency, ability to generate data (e.g. vision as inverse graphics), architectures for AI.</p><hr><p>See more at <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Machine%2520learning.html">Machine learning</a></p><hr><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Mathematical%2520modelling%2520of%2520neural%2520networks.html">Mathematical modelling of neural networks</a></u></h3><p>Why Deep Learning models perform so well?</p><p>Seems to be a result of:</p><ul><li>Very large datasets</li><li>Increasing computing power</li><li>Flexibility of the models. Lots of parameters when lots of layers. Furthermore multiple layers avoide the curse of dimensionality</li></ul><hr><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=QPWs3BBKBdY&amp;feature=youtu.be" rel="noopener noreferrer" target="_blank">Eric Drexler - A Cambrian explosion in Deep learning</a></p><hr><p><a class="tc-tiddlylink-external" href="http://www.demo.cs.brandeis.edu/papers/wcci98.pdf" rel="noopener noreferrer" target="_blank">A Gradient Descent Method for a Neural Fractal Memory</a></p><p><a class="tc-tiddlylink-external" href="https://www.oreilly.com/ideas/the-current-state-of-machine-intelligence-2-0" rel="noopener noreferrer" target="_blank">https://www.oreilly.com/ideas/the-current-state-of-machine-intelligence-2-0</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Computer%2520vision.html">Computer vision</a></p><p><a class="tc-tiddlylink-external" href="https://www.amazon.co.uk/Artificial-Intelligence-Humans-Learning-Networks/dp/1505714346/ref=pd_cp_14_1?_encoding=UTF8&amp;psc=1&amp;refRID=QQNERJ110GTGKXQRXNZ8" rel="noopener noreferrer" target="_blank">Artificial Intelligence for Humans, Volume 3: Deep Learning and Neural Networks Paperback – 28 Oct 2015</a></p><p><a class="tc-tiddlylink-external" href="http://shop.oreilly.com/product/0636920039709.do" rel="noopener noreferrer" target="_blank">Fundamentals of Deep Learning</a></p><p><a class="tc-tiddlylink-external" href="https://medium.com/machine-learnings/a-humans-guide-to-machine-learning-e179f43b67a0#.gumb86nos" rel="noopener noreferrer" target="_blank">https://medium.com/machine-learnings/a-humans-guide-to-machine-learning-e179f43b67a0#.gumb86nos</a></p><p>Yes, the idea is not that humans will not do anything. The idea is that we set goals, make individual and collective decisions, just as today. But with higher prowess. As a result, most people will work less, in the sense that they spend almost all their time in leisure, and almost none in rote activities.
In car factories, for instance, you will need less and less people, as the work becomes closer and closer to just deciding the overall high level goals. The remaining jobs, will also become quite close to laisure anyway, with almost all rote aspects removed.
You know, people for fun will stay make cars and other things by hand, and with different degrees of automation. A big thing, I believe, will be people and machines working together as a team, each benefiting from each other's strengths. Later on, we will also become hybridized with machines in more direct ways, until, &quot;we become one&quot;. Or how I prefer to put it, we become many (in the sense that the diversity of forms of sentient beings/people will grow drastically, with a huge spectrum of personal choice/journey to explore and expand)</p><p><a class="tc-tiddlylink-external" href="http://www.artificialbrains.com/" rel="noopener noreferrer" target="_blank">http://www.artificialbrains.com/</a></p><p><a class="tc-tiddlylink-external" href="http://users.dsic.upv.es/~flip/papers/Thesis_fmartinez.pdf" rel="noopener noreferrer" target="_blank">Nice PhD thesis</a> – <a class="tc-tiddlylink-external" href="http://users.dsic.upv.es/~fmartinez/" rel="noopener noreferrer" target="_blank">author page</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=JJj4allguoU" rel="noopener noreferrer" target="_blank">Prof. Schmidhuber - The Problems of AI Consciousness and Unsupervised Learning Are Already Solved</a></p><hr><p>Yeah, I'm not saying people won't have things to do. But yeah mostly: either personal projects, or leisure, in the case that for some reason you don't want to leave it to machines.</p><p>But &quot;understanding&quot; may become not so &quot;important&quot;, just like calculating isn't. You can always ask your understanding AI, for advice, if you need some of that understanding xD.</p><p>That seems extremely lazy. Although it's not hard to imagine people doing that.</p><p>In a time before computers. Knowing your multiplication tables was actually very important.</p><p>So, to say what is considered &quot;important&quot; (in the sense of important for humans), we'll need to know our role in society. Right now, computers are doing the computing, and we do the understanding. Perhaps next, computers will also do the understanding, and we'll just to the objective-setting, to different degrees of laziness. Society may just become a hedonistic dream, like in wallee (but not as cartoonshily dystopian...)</p><p>Until we merge with the machines, and we do everything again, including multipying of course :) Multiplying and computing is actually extremelly important. It's just that humans aren't the best at it now, so we just say it's not important, so we can feel better about not doing it.</p><p>Once we merge, we'll rightfully recognize the importance of all the little things that matter :)</p><hr><p>Some quick notes to self, on designing cognitive systems (AIs):
System that sees and acts on the world.
It creates models of the world and of itself acting on it.
It meanwhile decides what to do, based on its internal models, and some sort of reward mechanisms. It can decide to act on the world, or run internal simulations (thinking, consciousness), or both, and can organically alternate between the two. Active and passive/reflective modes. Fast and slow thinking.
Creating models &lt;–&gt; Unsupervised learning. Promising approaches: GANs, DBNs. Need more advances in this area.
Models that include the acting agent itself. Juergen Schmidhuber seems to have made work on this, but not sure what is his approach.
Best reward mechanisms unclear. Also, amount of pre-existing structure for system to work appropriately also unclear &lt;&gt; the rebirth of the nurture vs nature problem, now from the &quot;Creator&quot;'s perspective.</p><p><a class="tc-tiddlylink-external" href="http://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html?mwrsm=Facebook&amp;_r=0" rel="noopener noreferrer" target="_blank">The AI awakening</a></p><p><a class="tc-tiddlylink-external" href="https://www.vicarious.com/2017/10/26/common-sense-cortex-and-captcha/" rel="noopener noreferrer" target="_blank">https://www.vicarious.com/2017/10/26/common-sense-cortex-and-captcha/</a></p><hr><p>more notes from GKeep....</p><p>–&gt; Try DeepGA for supervised learning problem! Check how robust they are against adversarial examples</p><p>– try actinf code combining the loss function! – use World models models/code perhaps, <a class="tc-tiddlylink-external" href="https://worldmodels.github.io/" rel="noopener noreferrer" target="_blank">https://worldmodels.github.io/</a>  – &quot;We can now use our trained V model to pre-process each frame at time tt into z_tz ​to train our M model.&quot; – They train the models one after the other, given dependences... Makes sense. &quot;In principle, we can train both models together in an end-to-end manner, although we found that training each separately is more practical, and also achieves satisfactory results. Training each model only required less than an hour of computation time using a single NVIDIA P100 GPU. We can also train individual VAE and MDN-RNN models without having to exhaustively tune hyperparameters.&quot;</p><p>&quot;Furthermore, we see that in making these fast reflexive driving decisions during a car race, the agent does not need to plan ahead and roll out hypothetical scenarios of the future. Since h_th  contain information about the probability distribution of the future, the agent can just query the RNN instinctively to guide its action decisions. Like a seasoned Formula One driver or the baseball player discussed earlier, the agent can instinctively predict when and where to navigate in the heat of the moment. &quot;</p><p>&quot;In this simulation, we don’t need the V model to encode any real pixel frames during the hallucination process, so our agent will therefore only train entirely in a latent space environment.&quot;</p><p>Avoiding catastrophic forgetting by learning loss functions with a neural net, for each of the tasks, and using them as part of the loss when doing new tasks.. Hmm probably won't work.. Could just save the networks trained on the previous tasks and using them as extra labels.. Nah, because inputs likely to be from different distribution..</p><p>Best way of evolving hypernets?</p><p>Evolve GANs!!</p><hr><p>Mutar los seeds (tipo 1 por mutacion, y mutar preferentemente los más recientes). </p><p>self-referential nets..</p><p>Capsule networks functional maps manifolds. capsule networks inverse graphics &lt;&gt; DrawRNN</p><p>codewords to learn text representations</p><p>predictive learning. LeCun agreed that reward needs to be intrinsic, and rich – rather than learning from occasional task-specific rewards, AI systems should learn by constantly predicting “everything from everything”, without requiring training labels or a task definition. Once you understand the world (unsupervised/predictive learning) it's much easier to learn how to act (policy/RL)</p><p>Measure complexity of policies.</p><p>deform graph and reuse spectral representations, to navigate in similar graphs?</p><p>param-fun map in rnns Turing complete? Hmm. No, But with hyperrecurrent networks, maybe yes! Giving input to the network, and giving it parameters, merge into a single thing! –&gt; hyperrecurrent network</p><p>Evolutionary algos for GANs.
They are very unstable, indicating gradient information perhaps not that useful..
Mutations that change only generator or discriminator. Unsupervised learning, is often about self-supervision, but because of lack of external supervision, probably gradients not as useful, and exploration+Occam's razor is better? Hmm. Wolfram agrees that something closer to random search (on program space!) may be better. Demis thinks AGI solutions may be very sparse, better to get as much headway from brain.</p><p>Evolutionary strategies with spiking or more general nets. neuroevolution</p><p>easily programmable nets. They need to learn concepts/abstract knowledge first #DeepMind</p><p>Try random GANs
RNNs, RL</p><p>RG..</p><p>supervised learning, can use gradients very effecitvely because of supervision.</p><p>try pluggin trained gan to trained cnn to optimize to generate images of particular categories –&gt; seems to work for MNIST. Try multi-objective. Otherwise, just randomly search GAN inputs, as there aren't that many outputs, so easy to find any by random search.. so not that interesting. But with multi-objective (cat+bicycle or whatever) the space grows combinatorially</p><hr><p>When you start making a list of successively harder tasks for AI, and you realize you are writting about the history of life</p><p>Tasks
* Navigate (change location). Need basic understanding of space. Follow gradients, for instance, chemotaxis..
• Forage. Find food/water. Need memory for this. Need more understanding of space and time. Grid and place cells, hippocampus.
• Flee. Need very good perception, outlier detection, super good navigation, basics of causality. Attention mechanicsms.
• Attack/hunt. Need even better navigation. Stealth, super basic theory of mind (I don't want to be heard).
• Build (change arrangement of objects). Basic world modeling
• Socialize. More advanced world modeling, and theory of mind
• Communicate. More advanced socialization
• Make tools. More advanced world modeling, planning-oriented
• Make more tools.
• Make art. Tools for thinking. Thoughts that cause actions that change the world in ways that evoke new thoughts. You start seeing your own thoughts. Metacognition. Self-awareness. Because you realize you can control thoughts, you starting becoming aware of them!
• Writing. Realize the above can be useful for communication.
• First technological singularity
• ...&quot;history&quot;...
• Try to overcome the limits of biology by reverse engineering yourself.</p><p>DeepMind's idea is that you can't just make AI by working on natural language processing (akin to jumping to the communicate, or writting parts), because that thing is built on top of the previous machinery.
My attempt above was to make a list of things that only rely on things before, so that it makes sense to work on them in that order.</p><p>– make mini brain RL nn, plug to the web</p><hr><p>why is training when input is +/-1 instead of 0/1 so much worse??</p></div>


</div>

</p>

</section>
</body>
</html>
