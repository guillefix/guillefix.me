<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.15" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Computational learning theory: Cosmos — Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Learning%20theory"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Computational learning theory
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 3rd August 2018 at 1:22am
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Learning theory
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p><em>actually a lot of this is statistical learning theory..</em></p><p>See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Machine%2520learning.html">Machine learning</a> for more general picture.</p><p>Mostly <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Supervised%2520learning.html">Supervised learning</a> (see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Learning%2520theory.html">Learning theory</a>).</p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Probably%2520approximately%2520correct.html">Probably approximately correct</a></u></h2><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=aILazXK059Y" rel="noopener noreferrer" target="_blank">video</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=b5NlRg8SjZg&amp;t=1h12m40s" rel="noopener noreferrer" target="_blank">Basic setup for supervised learning</a>. Assumptions:</p><ul><li><strong>Invariance assumption</strong>. Data is randomly generated, and from the same distribution as data will be tested on!.</li><li>Realizability assumption. We assume our answer lies within some class of hypotheses. Relaxed in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Agnostic%2520learning.html">Agnostic learning</a></li></ul><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Empirical%2520risk%2520minimization.html">Empirical risk minimization</a> is a good approach. If ERM is 0, we call it a consistent learner.</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Overfitting.html">Overfitting</a>. <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=aILazXK059Y&amp;t=28m" rel="noopener noreferrer" target="_blank">To guard against overfitting, limit size of  hypothesis class</a> (<u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Occam's%2520razor.html">Occam's razor</a> in learning theory</u>). It basically looks like an extension of <a class="tc-tiddlylink tc-tiddlylink-missing" href="Hypothesis%2520testing.html">Hypothesis testing</a>, to a class of hypotheses.. <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=aILazXK059Y&amp;t=43m30s" rel="noopener noreferrer" target="_blank">Proof of Occam PAC learning theorem</a></p><p>Two types of error:</p><ul><li>Missfortune error: bad samples. <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=aILazXK059Y&amp;t=44m" rel="noopener noreferrer" target="_blank">vid</a> (basic def of PAC learning here too)</li><li>Rarity error: error on rare circumstances which weren't learned.</li></ul><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=iknI2iga9ps#t=56m45s" rel="noopener noreferrer" target="_blank">Uniform convergence</a>, main tool to proving results about PAC learnability. The basic result is <a class="tc-tiddlylink tc-tiddlylink-missing" href="Occam%2520theorem.html">Occam theorem</a> (see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Probably%2520approximately%2520correct.html">Probably approximately correct</a>)</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="VC%2520dimension.html">VC dimension</a> characterizes PAC-learnability (see &quot;Understanding machine learning&quot; book – <a class="tc-tiddlylink tc-tiddlylink-missing" href="Fundamental%2520theorem%2520of%2520learning%2520theory.html">Fundamental theorem of learning theory</a>)</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Covering-number%2520generalization%2520error%2520bounds.html">Covering-number generalization error bounds</a></p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Exact%2520learning.html">Exact learning</a></u></h2><p>We say concept class <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">C</span></span></span></span></span> is efficiently learnable with membership and equivalence queries, if there exists a polynomial p(.) and an algorithmL L with access to membership and equivalence queries oracles, <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">∀</mi><mi>c</mi><mo>∈</mo><msub><mi>C</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">\forall c \in C_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">∀</span><span class="mord mathit">c</span><span class="mrel">∈</span><span class="mord"><span class="mord mathit" style="margin-right:0.07153em;">C</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07153em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">n</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span> outputs in time <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mi>n</mi><mo separator="true">,</mo><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo>(</mo><mi>c</mi><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">p(n, size(c))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit">n</span><span class="mpunct">,</span><span class="mord mathit">s</span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mord mathit">e</span><span class="mopen">(</span><span class="mord mathit">c</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span> a concept <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi><mo>∈</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">h \in C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.73354em;vertical-align:-0.0391em;"></span><span class="base textstyle uncramped"><span class="mord mathit">h</span><span class="mrel">∈</span><span class="mord mathit" style="margin-right:0.07153em;">C</span></span></span></span></span> that is equivalent to <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">C</span></span></span></span></span>.</p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="SQ%2520learning.html">SQ learning</a></u></h2><p>Also, learning in the presence of noise</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Membership%2520query.html">Membership query</a>, and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Equivalence%2520query.html">Equivalence query</a></p><p>Example oracle + membership query is sufficient for PAC learning</p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Learning%2520real-valued%2520functions.html">Learning real-valued functions</a></u></h2><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Convex%2520optimization.html">Convex optimization</a></h3><p><a class="tc-tiddlylink-external" href="http://www.cs.ox.ac.uk/people/varun.kanade/teaching/AML-HT2017/lectures/lecture08.pdf" rel="noopener noreferrer" target="_blank">notes</a></p><p><a class="tc-tiddlylink tc-tiddlylink-missing" href="Rademacher%2520Complexity.html">Rademacher Complexity</a> analogous to <a class="tc-tiddlylink tc-tiddlylink-resolves" href="VC%2520dimension.html">VC dimension</a>, but for real-valued concepts.</p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Agnostic%2520learning.html">Agnostic learning</a></u></h2><p>So far in all the learning frameworks we’ve studied, we’ve made an assumption that there is
some “ground truth” target function that we attempt to learn.  Our goal has been to identify
a  hypothesis  that  is  close  to  this  target,  with  respect  to  the  target  distribution.   Learning
algorithms are given access to the target function in the form of labelled observations, which
in some cases may be noisy.  In this lecture, we’ll drop the assumption of a ground-truth target
completely; it is for this reason that the framework is called
agnostic
learning.  As there is no
longer a well-defined notion of target, our goal will be to identify a hypothesis that is competitive
with respect to the best concept from a particular class</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=aILazXK059Y&amp;t=9m40s" rel="noopener noreferrer" target="_blank">video</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PflkE9JmNLc&amp;t=55m55s" rel="noopener noreferrer" target="_blank">weakness of the PAC definition which motivates agnostic learning</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PflkE9JmNLc&amp;t=1h8m50s" rel="noopener noreferrer" target="_blank">to approach this we redefine succesful learning to have only a relative error guarantee</a> –&gt; <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PflkE9JmNLc&amp;t=1h10m22s" rel="noopener noreferrer" target="_blank">Definition of Agnostic PAC learnability</a></p><p><a class="tc-tiddlylink-external" href="http://www.cs.ox.ac.uk/people/varun.kanade/teaching/AML-HT2017/lectures/lecture09.pdf" rel="noopener noreferrer" target="_blank">notes</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Lyz4ewLefpE" rel="noopener noreferrer" target="_blank">proof that finite classes are agnostic learnable</a></p><p><a class="tc-tiddlylink-external" href="https://www.cs.bgu.ac.il/~asml162/Class_Material" rel="noopener noreferrer" target="_blank">https://www.cs.bgu.ac.il/~asml162/Class_Material</a></p><hr><p>See &quot;Understanding machine learning&quot; by Shai and Shai for more</p><h2 class=""><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=iknI2iga9ps#t=36m" rel="noopener noreferrer" target="_blank">General losses and more general definition of the learning problem</a></h2><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=iknI2iga9ps#t=29m" rel="noopener noreferrer" target="_blank">examples of other learning tasks</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=iknI2iga9ps#t=49m30s" rel="noopener noreferrer" target="_blank">kind of clustering</a> <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>∼</mo></mrow><annotation encoding="application/x-tex">\sim</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.36687em;"></span><span class="strut bottom" style="height:0.36687em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mrel">∼</span></span></span></span></span> to representing data with <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span></span> code words.</p><p>Probabilistic prediction rules can't predict better. But they can give us our uncertainty in the prediction, for each particular case we encounter. That extra information can be useful! (this is what modern <big><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Bayesian%2520inference.html">Bayesian inference</a></big> recognizes)</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=iknI2iga9ps#t=56m25s" rel="noopener noreferrer" target="_blank">How to learn in such a general situation?</a></p><ul><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=iknI2iga9ps#t=56m45s" rel="noopener noreferrer" target="_blank">Uniform convergence</a>, main tool to proving results about PAC learnability. <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=iknI2iga9ps#t=58m25s" rel="noopener noreferrer" target="_blank">Define epsilon-representativity</a>, if the empirical loss on the sample is a good approximation to true loss (within <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">ϵ</span></span></span></span></span>). <ul><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=iknI2iga9ps#t=1h5m10s" rel="noopener noreferrer" target="_blank">Claim1</a> and <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=iknI2iga9ps#t=1h8m55s" rel="noopener noreferrer" target="_blank">proof</a> (<a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Lyz4ewLefpE#t=16m30s" rel="noopener noreferrer" target="_blank">proof 2</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Lyz4ewLefpE#t=21m45s" rel="noopener noreferrer" target="_blank">actually here</a>): if we have a representative sample, then <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Empirical%2520risk%2520minimization.html">Empirical risk minimization</a> (over finite class) is <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">2\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">2</span><span class="mord mathit">ϵ</span></span></span></span></span> accurate (relative to best in class)</li><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=iknI2iga9ps#t=1h17m" rel="noopener noreferrer" target="_blank">Claim2</a>: a large enough sample is representative w.h.p. See proof and more at <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Agnostic%2520learning.html">Agnostic learning</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Lyz4ewLefpE#t=24m47s" rel="noopener noreferrer" target="_blank">proof here</a> <ul><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Lyz4ewLefpE#t=26m15s" rel="noopener noreferrer" target="_blank">Defining sample complexity of uniform convergence</a></li></ul></li><li>Finally to proof useful bounds, we need to bound the sample complexity of the uniform convergence. See example at <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Agnostic%2520learning.html">Agnostic learning</a>, <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Lyz4ewLefpE&amp;t=1h11m40s" rel="noopener noreferrer" target="_blank">result for sample complexity of uniform convergence</a></li></ul></li></ul><p>There are relations to <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Game%2520theory.html">Game theory</a></p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Online%2520learning.html">Online learning</a></u></h2><hr><p><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1505.05800.pdf" rel="noopener noreferrer" target="_blank">Complexity Theoretic Limitations on Learning Halfspaces</a></p><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Grammar%2520learning.html">Grammar learning</a></h3><hr><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=b5NlRg8SjZg&amp;t=19m30s" rel="noopener noreferrer" target="_blank">Nice classical experiment about supervision</a> (1st lecture of lecture seires on ML)</p><p><a class="tc-tiddlylink-external" href="https://simons.berkeley.edu/workshops/schedule/9161" rel="noopener noreferrer" target="_blank">https://simons.berkeley.edu/workshops/schedule/9161</a></p></div>


</div>

</p>

</section>
</body>
</html>
