<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.17" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Deep learning: Cosmos — Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Machine%20learning " data-tags="[[Machine learning]]" data-tiddler-title="Deep learning"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Deep learning
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 26th February 2018 at 1:42pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Machine learning
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p>Deep learning <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Machine%2520learning.html">Machine learning</a> in a modular way using <strong>layers</strong>, like in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Torch%2520(Deep%2520learning%2520framework).html">Torch</a>. <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Artificial%2520neural%2520network.html">Artificial neural network</a>s, with many layers..</p><p><a class="tc-tiddlylink-external" href="http://www.abigailsee.com/2018/02/21/deep-learning-structure-and-innate-priors.html" rel="noopener noreferrer" target="_blank">http://www.abigailsee.com/2018/02/21/deep-learning-structure-and-innate-priors.html</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=He4t7Zekob0&amp;index=5&amp;list=PLujxSBD-JXglGL3ERdDOhthD3jTlfudC2" rel="noopener noreferrer" target="_blank">Two+ Minute Papers - How Does Deep Learning Work?</a>
– 
<a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=g-dKXOlsf98" rel="noopener noreferrer" target="_blank">The computer that mastered Go</a></p><p><a class="tc-tiddlylink-external" href="https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/" rel="noopener noreferrer" target="_blank">Oxford course (with video)</a> on lecture 12</p><p><a class="tc-tiddlylink-external" href="https://uk.mathworks.com/discovery/deep-learning.html" rel="noopener noreferrer" target="_blank">matlab</a></p><p>The idea is also that layers are <em>recursive</em>, i.e. layers can be made up of layers.</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=x1kf4Zojtb0#t=43m05" rel="noopener noreferrer" target="_blank">future</a></p><p>Concepts as programs; programs as networks</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Probabilistic%2520programming.html">Probabilistic programming</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Program%2520induction.html">Program induction</a></p><p>trainning models based on demonstration</p><p>Multi-agents, and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Communication.html">Communication</a></p><p>Generating programs is not that different from generating <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Explainable%2520artificial%2520intelligence.html">explanations</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Augmented%2520RNN.html">Augmented RNN</a>s</p><p><a class="tc-tiddlylink-external" href="http://www.thespermwhale.com/jaseweston/" rel="noopener noreferrer" target="_blank">http://www.thespermwhale.com/jaseweston/</a></p><p>NIPS2016</p><hr><h2 class="">Deep learning methods</h2><h3 class=""><u>Neural networks for spatially structured data</u></h3><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Convolutional%2520neural%2520network.html">Convolutional neural network</a></p><p><a class="tc-tiddlylink tc-tiddlylink-missing" href="Deep%2520multi-scale%2520video%2520prediction%2520beyond%2520mean%2520square%2520error%257Chttp%253A%252F%252Farxiv.org%252Fabs%252F1511.05440.html">Multi-scale networks</a> and <a class="tc-tiddlylink-external" href="http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf" rel="noopener noreferrer" target="_blank">an application</a>.</p><p><a class="tc-tiddlylink-external" href="http://arxiv.org/abs/1202.2160" rel="noopener noreferrer" target="_blank">Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers</a></p><p><a class="tc-tiddlylink-external" href="http://www.clement.farabet.net/research.html#parsing" rel="noopener noreferrer" target="_blank">http://www.clement.farabet.net/research.html#parsing</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Computer%2520vision.html">Computer vision</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Residual%2520neural%2520network.html">Residual neural network</a></p><h3 class=""><u>Neural networks for <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Sequence.html">sequential</a> data</u></h3><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Recurrent%2520neural%2520network.html">Recurrent neural network</a></p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Transfer%2520learning.html">Transfer learning</a></u></h3><p>good for generalizing models, <strong>transfer learning</strong>, <strong>multi-task learning</strong>. Good when don't have much supervision data.</p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Neural%2520networks%2520with%2520memory.html">Neural networks with memory</a></u></h3><p><strong>Memory</strong> is good for recognizing time sequence data. See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Long%2520short-term%2520memory.html">Long short-term memory</a>.</p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Attention%2520in%2520machine%2520learning.html">Attention in machine learning</a></u></h3><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Integrating%2520symbols%2520into%2520deep%2520learning.html">Integrating symbols into deep learning</a></u></h3><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Neural%2520Turing%2520machine.html">Neural Turing machine</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Neural%2520programmer-interpreter.html">Neural programmer-interpreter</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Deep%2520symbolic%2520reinforcement%2520learning.html">Deep symbolic reinforcement learning</a></li></ul><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Deep%2520reinforcement%2520learning.html">Deep reinforcement learning</a></u></h3><p><em>more...</em></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=x1kf4Zojtb0#t=20m" rel="noopener noreferrer" target="_blank">Structured learning</a> – Learning to learn and compositionality with deep recurrent neural networks</p><hr><h3 class="">Some techniques for deep learning</h3><p><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Layers%2520for%2520deep%2520learning.html">Layers for deep learning</a></u></p><h2 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="New%2520advances%2520in%2520deep%2520learning.html">New advances in deep learning</a></h2><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Dropout.html">Dropout</a>. <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=NUKp0c4xb8w&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;index=9#t=50m20s" rel="noopener noreferrer" target="_blank">usefulness of dropout</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Batch%2520normalization.html">Batch normalization</a></p><p><a class="tc-tiddlylink-external" href="file:///home/guillefix/Dropbox/Oxford/Systems%20Biology%20DPhil/Research/DeFreitas-NIPS2013_5025.pdf" rel="noopener noreferrer" target="_blank">Predicting Parameters in Deep Learning</a> The intuition motivating the techniques in this paper is the well known observation that the first layer features of a neural network trained on natural image patches tend to be globally smooth with local edge features, similar to local Gabor features [6, 13]. I.e. they are seizing the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Simplicity.html">Simplicity</a> often found in real-world structures. Given this structure, representing the value of each pixel in the feature separately is redundant, since it is highly likely that the value of a pixel will be equal to a weighted average of its neighbours.  </p><p>The core of the technique is based on representing the weight matrix as a low rank product of two smaller matrices. </p><hr><h2 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Deep%2520learning%2520theory.html">Deep learning theory</a></h2><h2 class="">Deep learning applications</h2><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Deep%2520art.html">Deep art</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Applications%2520of%2520AI.html">Applications of AI</a></p><hr><p><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Hardware%2520for%2520deep%2520learning.html">Hardware for deep learning</a></u></p><p><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Software%2520for%2520deep%2520learning.html">Software for deep learning</a></u></p><hr><p><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="People%2520in%2520deep%2520learning.html">People in deep learning</a></u></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="History%2520of%2520deep%2520learning.html">History of deep learning</a></p><hr><p><u>Books and reources</u></p><p><a class="tc-tiddlylink-external" href="http://www.sciencedirect.com/science/article/pii/S0893608014002135" rel="noopener noreferrer" target="_blank">Deep learning in neural networks: An overview</a></p><p><a class="tc-tiddlylink-external" href="https://deepmind.com/publications.html" rel="noopener noreferrer" target="_blank">https://deepmind.com/publications.html</a></p><p><a class="tc-tiddlylink-external" href="http://www.deeplearningbook.org/" rel="noopener noreferrer" target="_blank">http://www.deeplearningbook.org/</a></p><p><a class="tc-tiddlylink-external" href="http://carpedm20.github.io/" rel="noopener noreferrer" target="_blank">http://carpedm20.github.io/</a></p><hr><p>Work on giving prior knowledge to deep learning: <a class="tc-tiddlylink-external" href="https://yani.io/annou/thesis_online.pdf" rel="noopener noreferrer" target="_blank">https://yani.io/annou/thesis_online.pdf</a></p></div>


</div>

</p>

</section>
</body>
</html>
