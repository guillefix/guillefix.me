<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.15" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Generalization in deep learning: Cosmos — Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Deep%20learning%20theory tc-tagged-Generalization"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Generalization in deep learning
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 13th November 2018 at 1:48pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Deep learning theory
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
<span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Generalization
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p><style>.fancy-title {
background: #CBF2D3;  /* fallback for old browsers */

/*border: 1px solid black;*/

color: black;

}
.fancy-title a {
color: #2847D1;
font-weight:bold;
}

</style></p><p>– <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Deep%2520learning%2520theory.html">Deep learning theory</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1706.08947.pdf" rel="noopener noreferrer" target="_blank">Exploring generalization in deep learning</a> – <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1710.05468" rel="noopener noreferrer" target="_blank">Generalization in Deep Learning</a> – <small><a class="tc-tiddlylink-external" href="https://youtu.be/KDRN-FyyqK0?t=52m22s" rel="noopener noreferrer" target="_blank">Current state of generalization theory for deep learning (2018)</a>)</small> – <a class="tc-tiddlylink-external" href="http://www.mit.edu/~rakhlin/papers/myths.pdf" rel="noopener noreferrer" target="_blank">slides on myths about generalization in deep learning</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1808.01174" rel="noopener noreferrer" target="_blank">Generalization Error in Deep Learning</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1706.10239" rel="noopener noreferrer" target="_blank">Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes</a> – <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1706.05394" rel="noopener noreferrer" target="_blank">A Closer Look at Memorization in Deep Networks</a></p><p>See gingkoapp tree: <a class="tc-tiddlylink-external" href="https://gingkoapp.com/app#7abe722f5a31aa3e1000001b" rel="noopener noreferrer" target="_blank">Kolmogorov complexity and generalization in deen neural networks</a></p><p>–&gt; Even for <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Kernel%2520method.html">Kernel method</a>s, the classical learning theory doesn't predict what we observe (– <a class="tc-tiddlylink-external" href="https://youtu.be/T4T63TT-Hy4?list=PL04QVxpjcnjhtL3IIVyFRMOgdhWtPn7YJ&amp;t=21m44s" rel="noopener noreferrer" target="_blank">see here</a>). <a class="tc-tiddlylink-external" href="https://youtu.be/T4T63TT-Hy4?list=PL04QVxpjcnjhtL3IIVyFRMOgdhWtPn7YJ&amp;t=53m10s" rel="noopener noreferrer" target="_blank">Overparametrization in deep learning</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1611.03530" rel="noopener noreferrer" target="_blank">Understanding deep learning requires rethinking generalization</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1805.08522" rel="noopener noreferrer" target="_blank">Deep learning generalizes because the parameter-function map is biased towards simple functions</a> </p><h3 class=""><u>Other approaches</u></h3><p><a class="tc-tiddlylink-external" href="https://dspace.mit.edu/handle/1721.1/118307" rel="noopener noreferrer" target="_blank">Towards Understanding Generalization via Analytical Learning Theory</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1806.08734" rel="noopener noreferrer" target="_blank">On the Spectral Bias of Neural Networks</a> .... lower frequency functions (or components) are more robust! (like <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1706.10239" rel="noopener noreferrer" target="_blank">Wu et al.</a> find also!).</p><p><small><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1711.01530" rel="noopener noreferrer" target="_blank">Fisher-Rao Metric, Geometry, and Complexity of Neural Networks</a> – <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1810.09665?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A+arxiv%2FQSXk+%28ExcitingAds%21+cs+updates+on+arXiv.org%29" rel="noopener noreferrer" target="_blank">A jamming transition from under- to over-parametrization affects loss landscape and generalization</a> – <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1810.01075" rel="noopener noreferrer" target="_blank">Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning</a></small></p><p><a class="tc-tiddlylink-external" href="https://openreview.net/forum?id=ryfMLoCqtQ" rel="noopener noreferrer" target="_blank">An analytic theory of generalization dynamics and transfer learning in deep linear networks </a></p><h2 class="fancy-title"><u><a class="tc-tiddlylink tc-tiddlylink-missing" href="Flat%2520minima.html">Flat minima</a></u></h2><p><a class="tc-tiddlylink-external" href="http://www.bioinf.jku.at/publications/older/3304.pdf" rel="noopener noreferrer" target="_blank">FLAT MINIMA</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1609.04836" rel="noopener noreferrer" target="_blank"> On large-batch train-ing for deep learning: Generalization gap and sharp minima.</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1703.11008" rel="noopener noreferrer" target="_blank">Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data</a> – Sharpness + norm bounds (as an alternative to margin + norm bounds, but similar as margin is similar to sharpness; robustness of the training loss to changes in weights; see <a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1706.08947.pdf" rel="noopener noreferrer" target="_blank">Exploring generalization in deep learning</a>)</p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1703.04933" rel="noopener noreferrer" target="_blank">Sharp Minima Can Generalize For Deep Nets</a> – maybe it's not sharp, but <strong>frequent</strong>! (<a class="tc-tiddlylink tc-tiddlylink-resolves" href="Arrival%2520of%2520the%2520frequent.html">Arrival of the frequent</a>)</p><p>Entropy-SGD – <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1712.09376" rel="noopener noreferrer" target="_blank">Entropy-SGD optimizes the prior of a PAC-Bayes bound: Generalization properties of Entropy-SGD and data-dependent priors</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1711.04623" rel="noopener noreferrer" target="_blank">Three Factors Influencing Minima in SGD</a> – Characterizing the relation between learning rate, batch size and the properties of the final minima, such as width or generalization</p><h2 class="fancy-title"><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="PAC-Bayesian%2520learning.html">PAC-Bayesian learning</a> approach</u></h2><p><a class="tc-tiddlylink-external" href="https://openreview.net/forum?id=rye4g3AqFm" rel="noopener noreferrer" target="_blank">Deep learning generalizes because the parameter-function map is biased towards simple functions</a> :)</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=dHUH0hmKvs8" rel="noopener noreferrer" target="_blank">(video) Karolina Dziugaite on Nonvacuous Generalization Bounds for Deep Neural Networks via PAC-Bayes</a> – <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1802.09583" rel="noopener noreferrer" target="_blank">Data-dependent PAC-Bayes priors via differential privacy</a> – <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1712.09376" rel="noopener noreferrer" target="_blank">Entropy-SGD optimizes the prior of a PAC-Bayes bound: Generalization properties of Entropy-SGD and data-dependent priors</a></p><p><small><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1707.09564" rel="noopener noreferrer" target="_blank">  PAC-bayesian  approach  to spectrally-normalized margin bounds for neural networks.</a> – <a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1706.08947.pdf" rel="noopener noreferrer" target="_blank">Exploring Generalization in Deep Learning</a></small></p><h2 class="fancy-title"><u>Norm/<a class="tc-tiddlylink tc-tiddlylink-resolves" href="Margin-based%2520generalization%2520bound.html">Margin-based generalization bound</a></u></h2><p>See Understanding Machine Learning by Shai and Shai for SVM bounds; they arell all based on <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Structural%2520risk%2520minimization.html">Structural risk minimization</a> and bounding <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Rademacher%2520complexity.html">Rademacher complexity</a> by bounding norms of the parameters <small>(sometimes refered to as <em>scale-sensitive bound/analysis</em>)</small></p><p><a class="tc-tiddlylink-external" href="https://papers.nips.cc/paper/1204-for-valid-generalization-the-size-of-the-weights-is-more-important-than-the-size-of-the-network.pdf" rel="noopener noreferrer" target="_blank">For valid generalization the size of the weights is more important than the size of thenetwork</a> – <a class="tc-tiddlylink-external" href="https://pdfs.semanticscholar.org/9f1e/b4445219fbc994eb3e47e76cf1428d99815c.pdf" rel="noopener noreferrer" target="_blank">The sample complexity of pattern classification with neural networks: the size ofthe weights is more important than the size of the network.</a></p><p><a class="tc-tiddlylink-external" href="http://cbcl.mit.edu/publications/ps/evgeniou-reviewall.pdf" rel="noopener noreferrer" target="_blank">Regularization Networks and Support Vector Machines</a> – <a class="tc-tiddlylink-external" href="http://www.jmlr.org/papers/volume3/bartlett02a/bartlett02a.pdf" rel="noopener noreferrer" target="_blank">Rademacher and gaussian complexities: Risk bounds andstructural results (2002)</a> – (<a class="tc-tiddlylink-external" href="http://www.stats.ox.ac.uk/~rebeschi/teaching/AFoL/18/material/lecture3.pdf#page=4" rel="noopener noreferrer" target="_blank">pdf from oxford course</a>, <a class="tc-tiddlylink tc-tiddlylink-missing" href="Rademacher%2520complexity%2520of%2520neural%2520networks.html">Rademacher complexity of neural networks</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1503.00036" rel="noopener noreferrer" target="_blank">Norm-based capacity control in neural networks</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1412.6614" rel="noopener noreferrer" target="_blank">In search of the real inductive bias:  On the roleof implicit regularization in deep learning.</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1706.08498" rel="noopener noreferrer" target="_blank">Spectrally-normalized margin bounds for neural networks</a>, allow for use of a variety of norms. Further work explores which choices of norm are better! (comments on need for upper and lower bounds) They use <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Covering%2520number.html">Covering number</a> bounds based on those in Neural Network Learning: Theoretical Foundations by Anthony and Barlett,1999, itself based on fat-shattering analsyis in <a class="tc-tiddlylink-external" href="https://papers.nips.cc/paper/1204-for-valid-generalization-the-size-of-the-weights-is-more-important-than-the-size-of-the-network.pdf" rel="noopener noreferrer" target="_blank">Barlett 1996</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1707.09564" rel="noopener noreferrer" target="_blank">  PAC-bayesian  approach  to spectrally-normalized margin bounds for neural networks.</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1709.01953.pdf" rel="noopener noreferrer" target="_blank">Implicit regularization in deep learning</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1810.05369" rel="noopener noreferrer" target="_blank">On the Margin Theory of Feedforward Neural Networks</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1706.08947.pdf" rel="noopener noreferrer" target="_blank">Exploring Generalization in Deep Learning</a>
They are very related to sharpness/robustness analysis. They look at how many of the ReLU activations are changed by small changes to the weights, to bound what they call the <em>sharpness</em>, which bounds the generalization error via a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="PAC-Bayes.html">PAC-Bayes</a> bound (on parameter space, of course..). Similar for some of the compression-based bounds, although in that case they use a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Covering%2520number.html">Covering number</a> argument rather than a PAC-Bayes one</p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1805.12076" rel="noopener noreferrer" target="_blank">Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks</a> (new margin bounds which work better)</p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1810.00113" rel="noopener noreferrer" target="_blank">Predicting the Generalization Gap in Deep Networks with Margin Distributions</a> – <a class="tc-tiddlylink-external" href="https://openreview.net/forum?id=HJlQfnCqKX" rel="noopener noreferrer" target="_blank">openreview</a> (<strong>new</strong>)</p><p><strong>Lower bounds?</strong> some <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1805.12076" rel="noopener noreferrer" target="_blank">here</a>, but they are just lower bounds on RadComp I think, so lower bounds on worst-case over algos..</p><h2 class="fancy-title"><u><a class="tc-tiddlylink tc-tiddlylink-missing" href="Compression-based%2520generalization%2520bound.html">Compression-based generalization bound</a></u></h2><p>See Understanding Machine Learning by Shai and Shai, Chap 30</p><p><a class="tc-tiddlylink-external" href="https://www.offconvex.org/2018/02/17/generalization2/" rel="noopener noreferrer" target="_blank">https://www.offconvex.org/2018/02/17/generalization2/</a> – <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1802.05296" rel="noopener noreferrer" target="_blank">Stronger generalization bounds for deep nets via a compression approach</a>. See stability analysis from &quot;Threshold logic and its applications book&quot;</p><p><a class="tc-tiddlylink-external" href="https://openreview.net/forum?id=BJgqqsAct7" rel="noopener noreferrer" target="_blank">Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach</a> – <a class="tc-tiddlylink-external" href="https://severelytheoretical.wordpress.com/2018/06/25/why-do-neural-networks-generalize-well/" rel="noopener noreferrer" target="_blank">blog</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1804.05862" rel="noopener noreferrer" target="_blank">Compressibility and Generalization in Large-Scale Deep Learning</a></p><h2 class="fancy-title"><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Algorithmic%2520robustness.html">Robustness</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Algorithmic%2520stability.html">stability</a>, sensitivity, <small><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Continuous%2520function.html">continuity</a>/<a class="tc-tiddlylink tc-tiddlylink-resolves" href="Smooth%2520function.html">smoothness</a></small></u></h2><p>Robustness relative to changes in inputs (and to training set), for example by measured by <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Lipschitz.html">Lipschitz</a> continuitiy. Bounds are not very good usually, as they suffer from exponential dependence on dimension (<a class="tc-tiddlylink tc-tiddlylink-resolves" href="Curse%2520of%2520dimensionality.html">Curse of dimensionality</a>). I think results are probably proved using <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Covering%2520number.html">Covering number</a>s which bound <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Rademacher%2520complexity.html">Rademacher complexity</a> (via <a class="tc-tiddlylink tc-tiddlylink-missing" href="Massart's%2520lemma.html">Massart's lemma</a>..)</p><p><a class="tc-tiddlylink-external" href="http://www.jmlr.org/papers/volume5/luxburg04b/luxburg04b.pdf" rel="noopener noreferrer" target="_blank"> Distance-based classification with lipschitz functions</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1005.2243.pdf" rel="noopener noreferrer" target="_blank">Robustness and generalization</a> However, the covering number of the input domain and thus the capacity can be exponential in the input dimension</p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1610.04574" rel="noopener noreferrer" target="_blank">Generalization Error of Invariant Classifiers</a></p><p><small>Related to arguments for generaliztion used in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Information%2520bottleneck.html">Information bottleneck</a> approaches..</small></p><p><a class="tc-tiddlylink-external" href="https://openreview.net/forum?id=HJC2SzZCW" rel="noopener noreferrer" target="_blank">Sensitivity and Generalization in Neural Networks: an Empirical Study</a></p><h2 class="fancy-title"><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="VC%2520dimension.html">VC dimension</a> bounds</u></h2><p>Also <a class="tc-tiddlylink tc-tiddlylink-missing" href="Fat-shattering%2520dimension.html">Fat-shattering dimension</a> (and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Natarajan%2520dimension.html">Natarajan dimension</a>?)</p><p><a class="tc-tiddlylink-external" href="https://papers.nips.cc/paper/154-what-size-net-gives-valid-generalization" rel="noopener noreferrer" target="_blank">What Size Net Gives Valid Generalization?</a></p><p>M. Anthony and P. L. Bartlett.Neural network learning: Theoretical foundations. cambridgeuniversity press, 2009. – Discrete mathematics of neural networks</p><p><a class="tc-tiddlylink-external" href="https://dl.acm.org/citation.cfm?doid=168304.168322" rel="noopener noreferrer" target="_blank">Lower bounds on the Vapnik-Chervonenkis dimension of multi-layer threshold networks</a></p><p><a class="tc-tiddlylink-external" href="https://pdfs.semanticscholar.org/9f1e/b4445219fbc994eb3e47e76cf1428d99815c.pdf" rel="noopener noreferrer" target="_blank">The sample complexity of pattern classification with neural networks: the size ofthe weights is more important than the size of the network.</a></p><p><a class="tc-tiddlylink-external" href="https://papers.nips.cc/paper/1051-neural-networks-with-quadratic-vc-dimension.pdf" rel="noopener noreferrer" target="_blank">neural networks with quadratic vc dimension</a> – <a class="tc-tiddlylink-external" href="https://papers.nips.cc/paper/1515-almost-linear-vc-dimension-bounds-for-piecewise-polynomial-networks.pdf" rel="noopener noreferrer" target="_blank">almost linear vc-dimension bounds for piecewise polynomial networks</a> – <a class="tc-tiddlylink-external" href="https://link.springer.com/chapter/10.1007/978-1-4471-2097-1_136" rel="noopener noreferrer" target="_blank">Neural Nets with Superlinear VC-Dimension</a></p><p><a class="tc-tiddlylink-external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.49.8669" rel="noopener noreferrer" target="_blank">VC Dimension of Neural Networks</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1703.02930" rel="noopener noreferrer" target="_blank">Nearly-tight VC-dimension and pseudodimension bounds for piecewise linear neural networks</a></p><p><a class="tc-tiddlylink-external" href="https://epubs.siam.org/doi/pdf/10.1137/S0097539793256041" rel="noopener noreferrer" target="_blank">bounds for the computational power and learning complexity of analog neural nets</a></p><hr><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Generalization%2520dynamics.html">Generalization dynamics</a> (also <strong>interaction between generalization and noise</strong> <small><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=c_ez2O2QnCM#t=42m50s" rel="noopener noreferrer" target="_blank">counterintuitive behavior</a>, see discussion <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Statistical%2520mechanics%2520of%2520neural%2520networks.html">here</a></small>)</h3><p><a class="tc-tiddlylink-external" href="http://www.people.fas.harvard.edu/~asaxe/papers/Advani,%20Saxe%20-%202017%20-%20High-dimensional%20dynamics%20of%20generalization%20error%20in%20neural%20networks.pdf" rel="noopener noreferrer" target="_blank">High-dimensional dynamics of generalization error in neural networks</a> (see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Statistical%2520mechanics%2520of%2520neural%2520networks.html">Statistical mechanics of neural networks</a> also) Using random matrix theory and exact solutions in deep linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem.  </p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1805.11917" rel="noopener noreferrer" target="_blank">The Dynamics of Learning: A Random Matrix Approach</a></p><p><a class="tc-tiddlylink-external" href="https://openreview.net/forum?id=ryfMLoCqtQ" rel="noopener noreferrer" target="_blank">An analytic theory of generalization dynamics and transfer learning in deep linear networks </a></p><hr><h2 class="fancy-title"><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Optimization%2520algorithms%2520and%2520regularization.html">Optimization algorithms and regularization</a></u></h2><h3 class=""><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1709.01953.pdf" rel="noopener noreferrer" target="_blank">Implicit regularization in deep learning</a></h3><p>Regularization caused by optimization algorithm.. &quot;we investigate the transformations under which the function computed by a network remains the same and therefore argue for complexity measures and optimization algorithms that have similar invariances. We find complexity measures that have similar invariances to neural networks and optimization algorithms that implicitly regularize them&quot;. <a class="tc-tiddlylink tc-tiddlylink-missing" href="Path-norm.html">Path-norm</a> a metric in parameter space, that is closer to model distance? <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Sloppy%2520systems.html">Sloppy systems</a>?</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Stochastic%2520gradient%2520descent.html">Stochastic gradient descent</a></p><p><a class="tc-tiddlylink-external" href="https://openreview.net/forum?id=BJij4yg0Z" rel="noopener noreferrer" target="_blank">A Bayesian Perspective on Generalization and Stochastic Gradient Descent</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1710.10174" rel="noopener noreferrer" target="_blank">SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1808.01204" rel="noopener noreferrer" target="_blank">Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data</a></p><p><a class="tc-tiddlylink-external" href="http://proceedings.mlr.press/v80/bartlett18a.html" rel="noopener noreferrer" target="_blank">Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks</a> – <a class="tc-tiddlylink-external" href="http://proceedings.mlr.press/v75/li18a.html" rel="noopener noreferrer" target="_blank">Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations</a> – <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1808.04685" rel="noopener noreferrer" target="_blank">Learning ReLU Networks on Linearly Separable Data: Algorithm, Optimality, and Generalization</a> – <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1803.00195" rel="noopener noreferrer" target="_blank">The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Minima and Regularization Effects</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1802.04420" rel="noopener noreferrer" target="_blank">Towards Understanding the Generalization Bias of Two Layer Convolutional Linear Classifiers with Gradient Descent</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1711.04623" rel="noopener noreferrer" target="_blank">Three Factors Influencing Minima in SGD</a> – Characterizing the relation between learning rate, batch size and the properties of the final minima, such as width or generalization</p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1806.00468" rel="noopener noreferrer" target="_blank">Implicit Bias of Gradient Descent on Linear Convolutional Networks</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1809.00846" rel="noopener noreferrer" target="_blank">Towards Understanding Regularization in Batch Normalization</a></p><p><small><a class="tc-tiddlylink-external" href="https://link.springer.com/chapter/10.1007/978-3-030-01424-7_39" rel="noopener noreferrer" target="_blank">Width of Minima Reached by Stochastic Gradient Descent is Influenced by Learning Rate to Batch Size Ratio</a></small></p><h2 class="fancy-title"><u>Generalization of <a class="tc-tiddlylink tc-tiddlylink-missing" href="Linear%2520neural%2520network.html">Linear neural network</a>s</u></h2><p><a class="tc-tiddlylink-external" href="https://youtu.be/ACdjYP0-cMw?t=17m26s" rel="noopener noreferrer" target="_blank">Matrix factorization as two-layer linear network</a>, <a class="tc-tiddlylink-external" href="https://youtu.be/ACdjYP0-cMw?t=27m53s" rel="noopener noreferrer" target="_blank">increasing the number of hidden units in the linear network we get better generalization</a></p><p><a class="tc-tiddlylink-external" href="http://www.people.fas.harvard.edu/~asaxe/papers/Advani,%20Saxe%20-%202017%20-%20High-dimensional%20dynamics%20of%20generalization%20error%20in%20neural%20networks.pdf" rel="noopener noreferrer" target="_blank">High-dimensional dynamics of generalization error in neural networks</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=c_ez2O2QnCM" rel="noopener noreferrer" target="_blank">Sampolinsky lecture</a></p><p><a class="tc-tiddlylink-external" href="https://papers.nips.cc/paper/7195-implicit-regularization-in-matrix-factorization" rel="noopener noreferrer" target="_blank">Implicit Regularization in Matrix Factorization</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1806.00468" rel="noopener noreferrer" target="_blank">Implicit Bias of Gradient Descent on Linear Convolutional Networks</a></p><p>&quot;For fully connected networks with single output, Theorem 1 shows that there is no effect of depth on the implicit bias of gradient descent. Regardless of the depth of the network, the asymptotic classifier is always the hard margin support vector machine classifier, which is also the limit direction of gradient descent for linear logistic regression in the direct parameterization of β = w. In contrast, next we show that for convolutional networks we get very different biases. Let us first look at a 2–layer linear convolutional network, i.e., a network with single convolutional layer followed by a fully connected final layer.&quot;</p><p>Hmm but for matrix facorization, they did get differences? is it only about <a class="tc-tiddlylink tc-tiddlylink-missing" href="Matrix%2520rank.html">rank</a>?
What about for non linearly separable data?
In any case changing the way we parametrize can have an effect in general, clearly</p><p>&quot;Results for matrix sensing in <a class="tc-tiddlylink-external" href="https://papers.nips.cc/paper/7195-implicit-regularization-in-matrix-factorization" rel="noopener noreferrer" target="_blank">Gunasekar et al. [8]</a> imply that for two layer
fully connected networks with multiple outputs, the implicit bias is to a maximum margin solution
with respect to the nuclear norm kβk?. This is already different from the implicit bias of a one-layer
“network” (i.e. optimizing β directly), which would be in terms of the Frobenius norm kβkF . We
suspect that with multiple outputs, as more layers are added, even fully connected networks exhibit a
shrinking sparsity penalty on the singular values of the effective linear matrix predictor β ∈ R
C×D.&quot;</p><p><small> it might be beneficial to continue optimizing even
after the loss value L(β
(t)
) itself becomes negligible.</small></p><p><small>&quot;We can decompose the characterization of implicit bias of gradient descent on a parameterization
P(w) into two parts: (a) what is the implicit bias of gradient descent in the space of parameters w?,
and (b) what does this imply in term of the linear predictor β = P(w), i.e., how does the bias in
parameter space translate to the linear predictor learned from the model class?&quot;</small></p><p><a class="tc-tiddlylink-external" href="https://pdfs.semanticscholar.org/91eb/501638213fed2081c45c55df7ab856d4b737.pdf" rel="noopener noreferrer" target="_blank">Generalization Error of Linear Neural Networks in Unidentifiable Cases</a>. Only looks at one-hidden layer linear networks I think, and finds some cases in which overparametrization is bad?</p><p><small><a class="tc-tiddlylink-external" href="https://pdfs.semanticscholar.org/fd2d/712b36b7f63051b98143dc3fdaa19e956e73.pdf" rel="noopener noreferrer" target="_blank">Generalization Error of Linear Neural Networks in an Empirical Bayes Approach</a></small></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1710.05468" rel="noopener noreferrer" target="_blank">Generalization in Deep Learning</a> <small>we provide a theorem (Theorem 1) stating that the hypothesis space of over-parameterized linear models can memorize any training data and decrease the training and test errors arbitrarily close to zero (including zero) with the norm of parameters being arbitrarily large, even when the parameters are arbitrarily far from the ground-truth parameters</small></p><h2 class="fancy-title"><u>Generalization in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Deep%2520reinforcement%2520learning.html">Deep reinforcement learning</a></u></h2><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1804.06893" rel="noopener noreferrer" target="_blank">A Study on Overfitting in Deep Reinforcement Learning</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1810.12282" rel="noopener noreferrer" target="_blank">Assessing Generalization in Deep Reinforcement Learning</a></p><p><small><a class="tc-tiddlylink-external" href="https://openreview.net/forum?id=rJgvf3RcFQ&amp;noteId=rJgvf3RcFQ" rel="noopener noreferrer" target="_blank">On Inductive Biases in Deep Reinforcement Learning</a></small></p><hr><p><a class="tc-tiddlylink-external" href="https://openreview.net/forum?id=BygfghAcYX" rel="noopener noreferrer" target="_blank">The role of over-parametrization in generalization of neural networks</a></p><hr><p><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1710.05468.pdf" rel="noopener noreferrer" target="_blank">Generalization in deep learning</a> (Bengio et al) – en general, no me parecio tan interesante, excepto un bound de <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Rademacher%2520complexity.html">Rademacher complexity</a>, q tampoco es q le permita decir mucho per bueno. val just tells us that we generalize. It doesn't tells us why. Just like PAC bounds don't tells us why the target function lies within our test set, for e.g.. </p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1810.08591" rel="noopener noreferrer" target="_blank">A Modern Take on the Bias-Variance Tradeoff in Neural Networks</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1704.01312" rel="noopener noreferrer" target="_blank">On Generalization and Regularization in Deep Learning</a></p><p><a class="tc-tiddlylink-external" href="https://pdfs.semanticscholar.org/a13e/ab6052cc9f85054d70d3ba395b0d77652172.pdf" rel="noopener noreferrer" target="_blank">Unreasonable Effectiveness of Learning Neural Nets: Accessible States and Robust Ensembles</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1703.01678" rel="noopener noreferrer" target="_blank">Data-Dependent Stability of Stochastic Gradient Descent</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1509.01240" rel="noopener noreferrer" target="_blank">Train faster, generalize better: Stability of stochastic gradient descent</a> &lt;&gt; <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1705.08741" rel="noopener noreferrer" target="_blank">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</a> <small>(lol)</small></p><p><a class="tc-tiddlylink-external" href="http://www.sciencedirect.com/science/article/pii/0893608094900264" rel="noopener noreferrer" target="_blank">What size network is good for generalization of a specific task of interest</a> – We show that for some tasks increasing network size leads to worse generalization. This is not surprising. The striking feature is that there exist other tasks for which increasing network size improves generalization. We give an explanation of this phenomenon in terms of the information entropy. I think what this paper is missing is the concept of universal complexity measures. You can see that tasks of “medium complexity” are the hardest to learn, because their measure of complexity isn’t very good. Even just entropy, would be better (as highest entropy corresponds to medium complexity in their case)</p><p><a class="tc-tiddlylink-external" href="http://www.inderscienceonline.com/doi/abs/10.1504/IJAACS.2014.065198" rel="noopener noreferrer" target="_blank">related paper</a>, <a class="tc-tiddlylink-external" href="http://sci-hub.cc/10.1504/ijaacs.2014.065198" rel="noopener noreferrer" target="_blank">pdf</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1312.6184" rel="noopener noreferrer" target="_blank">Do Deep Nets Really Need to be Deep?</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1506.05232" rel="noopener noreferrer" target="_blank">On the Depth of Deep Neural Networks: A Theoretical View</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1810.03241" rel="noopener noreferrer" target="_blank">Diagnosing Convolutional Neural Networks using their Spectral Response</a> observe that the best models are also the most sensitive to perturbations of their input.. ??</p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1805.07883" rel="noopener noreferrer" target="_blank">How Many Samples are Needed to Learn a Convolutional Neural Network?</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1810.03389" rel="noopener noreferrer" target="_blank">On Breiman's Dilemma in Neural Networks: Phase Transitions of Margin Dynamics</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1806.04225" rel="noopener noreferrer" target="_blank">PAC-Bayes Control: Synthesizing Controllers that Provably Generalize to Novel Environments</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1810.11914" rel="noopener noreferrer" target="_blank">Rademacher Complexity for Adversarially Robust Generalization</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1808.05563" rel="noopener noreferrer" target="_blank">Learning Invariances using the Marginal Likelihood</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1802.01396" rel="noopener noreferrer" target="_blank">To understand deep learning we need to understand kernel learning</a></p><hr><h3 class=""><u>Franco's <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Generalization%2520complexity.html">Generalization complexity</a></u></h3><p><a class="tc-tiddlylink-external" href="http://www.lcc.uma.es/~lfranco/Franco-complex06.pdf" rel="noopener noreferrer" target="_blank">Generalization ability of Boolean functions implemented in feedforward neural networks</a></p><hr><p>From <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1611.03530" rel="noopener noreferrer" target="_blank">Understanding deep learning requires rethinking generalization</a></p><p>&quot;Moreover, we did not observe any overfitting: the generalization error does not degrade by reaching zero training error, or by using larger networks.&quot;</p><p>&quot;what we really want is to minimize the variance of the net functions induced by weights near the actual weight vector.&quot;</p><hr><p>Flat minima good to help fight catastrophic forgetting.</p><hr><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Simplicity%2520bias.html">Simplicity bias</a> in neural networks</u></h3><p>See <a class="tc-tiddlylink-external" href="https://gingkoapp.com/app#7abe722f5a31aa3e1000001b" rel="noopener noreferrer" target="_blank">this gingko tree</a> and <a class="tc-tiddlylink-external" href="https://www.overleaf.com/9939721prrtpqvjmdxd#/36478572/" rel="noopener noreferrer" target="_blank">this overleaf document</a> (from my short project in summer 2017 with <a class="tc-tiddlylink tc-tiddlylink-missing" href="Ard%2520Louis.html">Ard Louis</a>. See emails with Ard</p></div>


</div>

</p>

</section>
</body>
</html>
