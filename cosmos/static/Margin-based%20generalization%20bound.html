<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.21" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Margin-based generalization bound: Cosmos — All that is, or was, or ever will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">
<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists  tc-tagged-Rademacher%20complexity tc-tagged-Generalization tc-tagged-Structural%20risk%20minimization" data-tags="[[Rademacher complexity]] Generalization [[Structural risk minimization]]" data-tiddler-title="Margin-based generalization bound"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Margin-based generalization bound
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 19th December 2019 at 7:05pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">
<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Generalization
</span>
<span class="tc-drop-down tc-reveal" hidden="true"></span></span><span class="tc-tag-list-item">
<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Rademacher complexity
</span>
<span class="tc-drop-down tc-reveal" hidden="true"></span></span><span class="tc-tag-list-item">
<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Structural risk minimization
</span>
<span class="tc-drop-down tc-reveal" hidden="true"></span></span></div>
</div>

<div class="tc-tiddler-body tc-reveal"><p><em>aka Margin theory, margin bound</em></p><p>See Understanding Machine Learning by Shai and Shai for <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Support%2520vector%2520machine.html">SVM</a> bounds; they are mostly based on <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Structural%2520risk%2520minimization.html">Structural risk minimization</a> and bounding <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Rademacher%2520complexity.html">Rademacher complexity</a> by bounding norms of the parameters/inputs. But some developed by Langford and others are based on <a class="tc-tiddlylink tc-tiddlylink-resolves" href="PAC-Bayesian%2520learning.html">PAC-Bayesian theory</a>, and may be tighter (see section below)</p><p><a class="tc-tiddlylink-external" href="https://www.esaim-ps.org/articles/ps/pdf/2005/01/ps0420.pdf" rel="noopener noreferrer" target="_blank">Theory of classification: A survey of some recent advances.</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="norm_based_generalization_error_bounds_UML.pdf.html">section from UML</a></p><p><a class="tc-tiddlylink-external" href="https://youtu.be/8wFMkMI7J1U?t=1331" rel="noopener noreferrer" target="_blank">Video about margin theory</a></p><p><a class="tc-tiddlylink-external" href="https://www.springer.com/gb/book/9780387308654" rel="noopener noreferrer" target="_blank">Estimation of Dependences Based on Empirical Data</a> – <a class="tc-tiddlylink-external" href="http://www.svms.org/training/BOGV92.pdf" rel="noopener noreferrer" target="_blank">a training algorithm for optimal margin classifiers</a> – <a class="tc-tiddlylink-external" href="https://link.springer.com/article/10.1007/BF00994018" rel="noopener noreferrer" target="_blank">Support-vector networks.</a> –  <a class="tc-tiddlylink-external" href="https://www.springer.com/gb/book/9780387987804" rel="noopener noreferrer" target="_blank">The Nature of Statistical Learning Theory</a></p><p>The algorithmic idea of large margin classifiers was introduced in the linear case by <a class="tc-tiddlylink-external" href="https://www.springer.com/gb/book/9780387308654" rel="noopener noreferrer" target="_blank">Vapnik (1982)</a> (see also (<a class="tc-tiddlylink-external" href="http://www.svms.org/training/BOGV92.pdf" rel="noopener noreferrer" target="_blank">Boser et al., 1992</a>; <a class="tc-tiddlylink-external" href="https://link.springer.com/article/10.1007/BF00994018" rel="noopener noreferrer" target="_blank">Cortes and Vapnik, 1995</a>)). <a class="tc-tiddlylink-external" href="https://www.springer.com/gb/book/9780387987804" rel="noopener noreferrer" target="_blank">Vapnik (1995)</a> gave an intuitive explanation ofthe performance of these methods based on a sample-dependent VC-dimension calculation, but withoutgeneralization bounds. The first rigorous generalization bounds for large margin linear classifiers (<a class="tc-tiddlylink-external" href="https://pdfs.semanticscholar.org/db5f/533d9f06d8d86e4e003478b3dc4bba15b848.pdf" rel="noopener noreferrer" target="_blank">Shawe-Taylor et al., 1998</a>) required a scale-sensitive complexity analysis of real-valued function classes. At thesame time, a large margin analysis was developed for two-layer networks (<a class="tc-tiddlylink-external" href="https://papers.nips.cc/paper/1204-for-valid-generalization-the-size-of-the-weights-is-more-important-than-the-size-of-the-network.pdf" rel="noopener noreferrer" target="_blank">Bartlett, 1996</a>), indeed with a proof technique that inspired the layer-wise induction used to prove Theorem 1.1 in the <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1706.08498" rel="noopener noreferrer" target="_blank">present work</a> .Margin theory was quickly extended to many other settings (see for instance the survey by <a class="tc-tiddlylink-external" href="https://www.esaim-ps.org/articles/ps/pdf/2005/01/ps0420.pdf" rel="noopener noreferrer" target="_blank">Boucheron et al.(2005)</a>), one major success being an explanation of the generalization ability of boosting methods, whichexhibit an explicit growth in the size of the function class over time, but a stable excess risk (Schapireet al., 1997)</p><p><a class="tc-tiddlylink-external" href="https://pdfs.semanticscholar.org/db5f/533d9f06d8d86e4e003478b3dc4bba15b848.pdf" rel="noopener noreferrer" target="_blank">Structural risk minimization over data-dependent hierarchies</a> – <a class="tc-tiddlylink-external" href="https://papers.nips.cc/paper/1204-for-valid-generalization-the-size-of-the-weights-is-more-important-than-the-size-of-the-network.pdf" rel="noopener noreferrer" target="_blank">For valid generalization the size of the weights is more important than the size of thenetwork</a> – <a class="tc-tiddlylink-external" href="https://pdfs.semanticscholar.org/9f1e/b4445219fbc994eb3e47e76cf1428d99815c.pdf" rel="noopener noreferrer" target="_blank">The sample complexity of pattern classification with neural networks: the size ofthe weights is more important than the size of the network.</a></p><p><a class="tc-tiddlylink-external" href="https://projecteuclid.org/euclid.aos/1024691352" rel="noopener noreferrer" target="_blank">Boosting the margin: a new explanation for the effectiveness of voting methods</a></p><p>Paper with applications of Rademacher complexity to classes of margin classifiers: <a class="tc-tiddlylink-external" href="http://www.jmlr.org/papers/volume5/luxburg04b/luxburg04b.pdf" rel="noopener noreferrer" target="_blank">http://www.jmlr.org/papers/volume5/luxburg04b/luxburg04b.pdf</a></p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="PAC-Bayes.html">PAC-Bayes</a>ian approach to margin bounds</u></h2><p><a class="tc-tiddlylink-external" href="http://hunch.net/~jl/projects/prediction_bounds/averaging/averaging_tech.pdf" rel="noopener noreferrer" target="_blank">Bounds for Averaging Classifiers</a></p><p><a class="tc-tiddlylink-external" href="https://papers.nips.cc/paper/2317-pac-bayes-margins.pdf" rel="noopener noreferrer" target="_blank">PAC-Bayes &amp; Margins</a> (Langford and Shawe-Taylor). In this paper they show that if there exists a weight vector <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span></span> with a margin risk <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>l</mi><mi>γ</mi></msub><mo>(</mo><mi>w</mi><mo separator="true">,</mo><mi>S</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">l_\gamma(w,S)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.01968em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05556em;">γ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.05764em;">S</span><span class="mclose">)</span></span></span></span></span> for margin <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05556em;">γ</span></span></span></span></span> and training set <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05764em;">S</span></span></span></span></span> (i.e. probability of sample having a margin larger than <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05556em;">γ</span></span></span></span></span>), then we can choose a certain posterior distribution <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">Q</span></span></span></span></span> for the general <a class="tc-tiddlylink tc-tiddlylink-resolves" href="PAC-Bayes.html">PAC-Bayes</a>ian theorem which depends on <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span></span>. For this posterior we can compute the relative entropy, and bound the empirical risk by the margin irsk of <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span></span> plus a term which decreases with increasing margin. <small> The idea of this second term is that <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">Q</span></span></span></span></span> is a distribution supported on <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>w</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup></mrow><annotation encoding="application/x-tex">w&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:0.751892em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span> which lie on a halfplane parallel to <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span></span>. The larger the margin, the more of these vectors that classify well, so the smaller the bound on the empirical risk.</small></p><p><a class="tc-tiddlylink-external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.129.6048" rel="noopener noreferrer" target="_blank">Simplified PAC-Bayesian Margin Bounds</a></p><p>.The fundamental idea behind the PAC-Bayesian approach to margin bounds is that a small error rate relative to a large safety margin ensures the existence of a posterior distribution (a Gibbs classifier) with a small training error and a small KL-divergence from the prior.</p></div>



</div>

</p>
</section>
</body>
</html>
