<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.17" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Partially-observable MDP: Cosmos — Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Markov%20decision%20process " data-tags="[[Markov decision process]]" data-tiddler-title="Partially-observable MDP"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Partially-observable MDP
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 4th November 2016 at 1:43pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Markov decision process
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p>A partially-observabe <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Markov%2520decision%2520process.html">Markov decision process</a> is a Markov decision process where the state is only partially observable by the actor, so that the policy can only depend on a function of the state, which looses some of the state's <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Information.html">Information</a></p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Kalman%2520filter.html">Kalman filter</a>s and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="LQG%2520control.html">LQG control</a></u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=46m50" rel="noopener noreferrer" target="_blank">video</a></p><p>A type of reinforcement learning, where we don't observe the state explicitly!</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=53m40s" rel="noopener noreferrer" target="_blank">Want to estimate actual state, given the noisy and incomplete measurements of the state</a>. Can use the method of marginalization, as used in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Factor%2520analysis%2520model.html">Factor analysis model</a>s. However, it is very computationally expensive. Instead we use a <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=55m52s" rel="noopener noreferrer" target="_blank">Kalman filter</a> model, which turns out to be a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Hidden%2520Markov%2520model.html">Hidden Markov model</a> with continuous states.</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=57m35s" rel="noopener noreferrer" target="_blank">Outline of Kalman filter</a></p><ul><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=1h22s" rel="noopener noreferrer" target="_blank">Predict step</a></li><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=1h03m30s" rel="noopener noreferrer" target="_blank">Update step</a></li></ul><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=1h7m40s" rel="noopener noreferrer" target="_blank">Intuition</a>. I think this can be seen through the lens of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Sufficient%2520statistic.html">Sufficient statistic</a>s</p><p>Kalman filter + LQR = <a class="tc-tiddlylink tc-tiddlylink-resolves" href="LQG%2520control.html">LQG control</a> &lt;- <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=1h8m55s" rel="noopener noreferrer" target="_blank">video</a> &lt;– <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;list=PLA89DCFA6ADACE599&amp;index#t=1h9m50s" rel="noopener noreferrer" target="_blank">how to solve</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;list=PLA89DCFA6ADACE599&amp;index#t=1h14m32s" rel="noopener noreferrer" target="_blank">Separation principle</a> of LQG control</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=0m50s" rel="noopener noreferrer" target="_blank">recap</a></p><h3 class=""><u>Other POMDP</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=3m55s" rel="noopener noreferrer" target="_blank">In general finding optimal policies of POMDPs is NP hard</a></p></div>


</div>

</p>

</section>
</body>
</html>
