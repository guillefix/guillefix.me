<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.17" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Simplicity and learning: Cosmos â€” Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Simplicity tc-tagged-Learning " data-tags="Simplicity Learning" data-tiddler-title="Simplicity and learning"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Simplicity and learning
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 27th April 2018 at 5:16pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Learning
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
<span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Simplicity
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p>See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Learning%2520theory.html">Learning theory</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Order.html">Order</a> and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Simplicity%2520bias.html">Simplicity bias</a>. The simplicity and structure in signals in the real-world is often seized to make the learning problem easier to solve. Can be formalized via <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Learning%2520theory.html">Learning theory</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="PAC-Bayesian%2520learning.html">PAC-Bayesian learning</a></p><p>See my paper on <a class="tc-tiddlylink tc-tiddlylink-missing" href="Simplicity%2520bias%2520in%2520the%2520parameter-function%2520map.html">Simplicity bias in the parameter-function map</a></p><p>Applications in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Inverse%2520problem.html">Inverse problem</a>s. For instance, see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Convex%2520optimization%2520heuristics%2520for%2520linear%2520inverse%2520problems.html">Convex optimization heuristics for linear inverse problems</a> and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Linear%2520inverse%2520problem.html">Linear inverse problem</a></p><p>Applications in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Compressed%2520sensing.html">Compressed sensing</a></p><h3 class=""><u>Simplicity and neural networks</u></h3><p>See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Neural%2520network%2520theory.html">Neural network theory</a>, <a class="tc-tiddlylink-external" href="http://arxiv.org/abs/1608.08225" rel="noopener noreferrer" target="_blank">Why does deep and cheap learning work so well?</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Deep%2520learning%2520theory.html">Deep learning theory</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Generalization%2520in%2520deep%2520learning.html">Generalization in deep learning</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1111.3846" rel="noopener noreferrer" target="_blank">No Free Lunch versus Occam's Razor in Supervised Learning</a></p><p><b>Nature often results in functions that are polynomials with several simplifying features</b>:</p><p><em>1. Low polynomial order </em></p><p>For reasons that are still not fully understood, our uni-verse can be accurately described by polynomial <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Hamiltonian.html">Hamiltonian</a>s of low order <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">d</span></span></span></span></span>. </p><p>The <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Central%2520limit%2520theorem.html">Central limit theorem</a> gives rise to <a class="tc-tiddlylink tc-tiddlylink-missing" href="Probability%2520distributions.html">Probability distributions</a> corresponding to quadratic Hamiltonians (see def in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Neural%2520network%2520theory.html">Neural network theory</a>). Similar results regarding maximum entropy distributions are also mentioned in the paper. Several common operations on image and sound are linear and thus order 1 polynomials on the input.</p><p><em>2. Locality</em></p><p>locality in a lattice manifests itself by allowing only nearest-neighbor interaction. In other words, almost all coeficients in the polynomial are forced to vanish, and the total number of non-zero coeficients grows only linearly with <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span></span></span></span></span>.</p><p>This can be stated more generally and precisely using the Markov network formalism</p><p><em>3. Symmetry</em></p><p>Whenever the Hamiltonian obeys some symmetry (is in-variant under some transformation), the number of independent parameters required to describe it is further reduced.</p><hr><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=oOB4evKlEmQ#t=35m" rel="noopener noreferrer" target="_blank">deep learning</a></p><p>&quot;If f is a truly random function then
it is highly unlikely that anyone will ever conceive of its existence and
would want to learn it.&quot; ~ Li&amp;Vitanyi's book</p><p>See also comments in <a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1611.00740.pdf" rel="noopener noreferrer" target="_blank">this paper</a></p></div>


</div>

</p>

</section>
</body>
</html>
