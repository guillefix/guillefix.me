<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.15" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Model-free reinforcement learning: Cosmos — All that is, or was, or ever will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Reinforcement%20learning"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Model-free reinforcement learning
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 30th May 2018 at 12:10am
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Reinforcement learning
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p>See more at <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Reinforcement%2520learning.html">Reinforcement learning</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=1h34m10s" rel="noopener noreferrer" target="_blank">summary</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1803.07055.pdf" rel="noopener noreferrer" target="_blank">Simple random search provides a competitive approach to reinforcement learning</a> – Our findings contradict the common belief that policy gradient techniques,
which rely on exploration in the action space, are more sample efficient than methods based on
finite-differences [25, 26]. In more detail, our contributions are as follows:</p><h1 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Policy%2520evaluation.html">Prediction</a></u></h1><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h7m" rel="noopener noreferrer" target="_blank">comparing approaches</a></p><p>Evaluation the value function given a policy</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT" rel="noopener noreferrer" target="_blank">Introduction, monte carlo model-free prediction</a>, just <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Monte%2520Carlo.html">sample</a> over runs of the MDP+policy, and average empirical returns (discounted sum of rewards).</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Incremental%2520average.html">Incremental</a> <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=29m5s" rel="noopener noreferrer" target="_blank">Monte Carlo update</a></p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Temporal%2520difference%2520learning.html">Temporal difference learning</a></u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=59m" rel="noopener noreferrer" target="_blank">Simple example comparing monte carlo vs TD0</a> </p><h1 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Model-free%2520control.html">Model-free control</a> (tabular solutions)</u></h1><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;index=5&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT" rel="noopener noreferrer" target="_blank">intro video</a>!</p><p><b><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=9m55s" rel="noopener noreferrer" target="_blank">actually need to use the action-value function to be model-free</a></b></p><p>We are basically going to use <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Policy%2520iteration.html">Policy iteration</a> with the <a class="tc-tiddlylink tc-tiddlylink-missing" href="Action-value%2520function.html">Action-value function</a>, with different ways to do the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Policy%2520evaluation.html">Policy evaluation</a> (by sampling) and policy update step (in a way that explores enough, given that the sampling means we don't see everything). This is an instance <a class="tc-tiddlylink tc-tiddlylink-missing" href="Generalized%2520policy%2520iteration.html">Generalized policy iteration</a> with <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Q%2520function.html">Q function</a> <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Policy%2520evaluation.html">evaluated</a> by sampling (model-free)</p><h2 class=""><u>Policy improvement <small>in the model free setting</small></u></h2><h3 class=""><u><span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">ϵ</span></span></span></span></span>-greedy exploration</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=14m30s" rel="noopener noreferrer" target="_blank">motivation</a> – <a class="tc-tiddlylink tc-tiddlylink-missing" href="Exploration%2520versus%2520exploitation.html">Exploration versus exploitation</a>. We need to carry on exploring everything to make sure we understand the value of all options!</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=15m55s" rel="noopener noreferrer" target="_blank">epsilon-greedy exploration</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=17m50s" rel="noopener noreferrer" target="_blank">theorem of policy improvement by epsilon-greedy policy iteration</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=23m40s" rel="noopener noreferrer" target="_blank">Making the policy iteration more efficient by only partial policy evaluation</a></p><h3 class=""><u>Greedy in the limit with infinite exploration (GLIE)</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=25m37s" rel="noopener noreferrer" target="_blank">GLIE is a method that is guaranteed to converge to the optimal policy in a model-free manner</a> – </p><p>An example is <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">ϵ</span></span></span></span></span>-greedy policy iteration with gradual decay of <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">ϵ</span></span></span></span></span></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=28m35s" rel="noopener noreferrer" target="_blank">GLIE Monte Carlo control</a></p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="On-policy%2520learning.html">On-policy learning</a> methods</u></h2><h3 class=""><u>Monte Carlo</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=7m" rel="noopener noreferrer" target="_blank">first attempt</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Policy%2520iteration.html">Policy iteration</a> with Monte-Carlo policy evaluation – but this isn't very efficient, so we use TD learning methods.</p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Temporal%2520difference%2520learning.html">Temporal difference learning</a> methods</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=38m45s" rel="noopener noreferrer" target="_blank">introduction to TD learning for control</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Sarsa.html">Sarsa</a></p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-missing" href="Action-critic%2520method.html">Action-critic method</a>s</u></h3><p><a class="tc-tiddlylink-external" href="https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node66.html" rel="noopener noreferrer" target="_blank">https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node66.html</a></p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Off-policy%2520learning.html">Off-policy learning</a> methods</u></h2><hr><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Curiosity.html">Curiosity</a> – <a class="tc-tiddlylink-external" href="https://pathak22.github.io/noreward-rl/" rel="noopener noreferrer" target="_blank">Curiosity-driven Exploration by Self-supervised Prediction</a>, see work by Schmidhuber
</p></div>


</div>

</p>

</section>
</body>
</html>
