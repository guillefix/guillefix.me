<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.15" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Neuroevolution: Cosmos — Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Evolutionary%20computing tc-tagged-Neuroscience tc-tagged-Evolution tc-tagged-Reinforcement%20learning"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Neuroevolution
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 9th June 2018 at 5:02pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Evolution
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
<span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Evolutionary computing
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
<span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Neuroscience
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
<span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Reinforcement learning
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="HyperNEAT.html">HyperNEAT</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Tx1G4BNd4dw" rel="noopener noreferrer" target="_blank">Evolving Regular, Modular Neural Networks</a> <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Kd6XmR_UePY" rel="noopener noreferrer" target="_blank">another</a></p><p><a class="tc-tiddlylink-external" href="https://eng.uber.com/accelerated-neuroevolution/" rel="noopener noreferrer" target="_blank">https://eng.uber.com/accelerated-neuroevolution/</a> – 
&quot;Accelerating Deep Neuroevolution: Train Atari in Hours on a Single Personal Computer! What took ~1 hour on 720 CPUs now takes only ~4 hours on a *single* modern desktop. Code is open source. Awesome work by @felipesuch with @kenneth0stanley &quot;</p><hr><p>Hi,</p><p>Do you want to meet sometime soon to talk about my project?</p><p>By the way, I have found a few cool papers:</p><p>First about genotype-phenotype maps in general, I have found that the literature on evolutionary computation/genetic algorithms has quite a lot of good research onto the effects of GP maps in evolution.Here is an example: <a class="tc-tiddlylink-external" href="https://link.springer.com/article/10.1007/s10710-012-9159-4" rel="noopener noreferrer" target="_blank">https://link.springer.com/article/10.1007/s10710-012-9159-4</a> , they call &quot;phenotypic robustness&quot; to what we call the phenotype's frequency, on the arrival of the frequent. </p><p>This other one (<a class="tc-tiddlylink-external" href="https://link.springer.com/chapter/10.1007/978-3-319-10762-2_42" rel="noopener noreferrer" target="_blank">https://link.springer.com/chapter/10.1007/978-3-319-10762-2_42</a> ), whose conclusion is like a prelude to our current findings:
&quot;We conjecture that genotype networks could be shaped very differently in other GP systems, however our current observations capture
many general properties of GP, and might even be applicable to other EC
systems. Specifically, the distribution of neutrality is very heterogenous among
various phenotypes. Some genotype networks, i.e. phenotypes, could be orders
of magnitude larger than others. Moreover, the mutational connections among
phenotypes are biased, where a phenotype has more potential to mutate to particular
phenotypes and is less likely to mutate to or is even disconnected from
some phenotypes. The success of an innovative evolutionary search crucially depends
on locating the target phenotype, i.e. whether it is accessible from many
other phenotypes, and on finding an efficient mutational path towards it.
In future studies, we expect to use our methodology in other GP- or ECsystems
and test if our observations and conjectures hold for a wider range of
applications. It would be helpful to look into how a particular EC representation
correlates with genotype network properties, such that we can gain a better
understanding of how a representation influences evolutionary search and how
we could improve the performance of an evolutionary algorithm by designing
more appropriate representations.&quot;</p><p>This thesis ( <a class="tc-tiddlylink-external" href="http://etheses.whiterose.ac.uk/12035/1/thesis.pdf" rel="noopener noreferrer" target="_blank">http://etheses.whiterose.ac.uk/12035/1/thesis.pdf</a> ), which mentions a particular bias found in Cartesian Genetic Programming, which is reminiscent of &quot;bias towards simplicity&quot;:
&quot;However, for classification tasks, smaller solutions are often favoured over
larger as they typically perform better on unseen data; mirroring the concept of Occams
razor [30]. Additionally, smaller solutions are often favoured generally because (a) they
are quicker to execute and (b) they are easier to understand and reason about. Finally,
a bias towards certain topologies does not limit the topologies which can be found given
sufficient evolutionary pressure. In this regard if a task requires a number of nodes larger or
smaller than the number to which there is a bias, this is still possible. Therefore, although
results were presented which showed removing length bias produced better results on
problems specifically designed to require a very large percentage of the possible nodes to
be active [82, 84], on many real world applications, length bias may actually be of benefit.&quot;</p><p>More significantly, a few of papers by Per Kristian Lehre, which show not only certain GP maps with bias, but explores their bias towards simplicity. He measures &quot;phenotypic complexity&quot; with LZW, and finds a negative correlation with &quot;neutrality degree&quot; (size of neutral networks):
<a class="tc-tiddlylink-external" href="http://www.sciencedirect.com/science/article/pii/S0303264706001705" rel="noopener noreferrer" target="_blank">http://www.sciencedirect.com/science/article/pii/S0303264706001705</a> 
<a class="tc-tiddlylink-external" href="https://pdfs.semanticscholar.org/13ec/e15e53b3f6729d5f8cd79380d5dd4209d6d2.pdf" rel="noopener noreferrer" target="_blank">https://pdfs.semanticscholar.org/13ec/e15e53b3f6729d5f8cd79380d5dd4209d6d2.pdf</a>
<a class="tc-tiddlylink-external" href="http://sci-hub.cc/10.1109/eh.2005.26" rel="noopener noreferrer" target="_blank">http://sci-hub.cc/10.1109/eh.2005.26</a>
I should read the third paper more carefuly, because it has plots that are similar to those showing &quot;randomness deficit&quot;. However, he is actually looking at &quot;genotypic complexity&quot;, and so the normal simplicity bias seems not to be there.</p><p>Now, regarding neural networks:</p><p>First, here is a paper by Jurgen Schmidhuber (the inventor of LSTMs) on why using the ideas of Solomonoff et al to discover good neural nets: <a class="tc-tiddlylink-external" href="http://www.sciencedirect.com/science/article/pii/S089360809600127X" rel="noopener noreferrer" target="_blank">http://www.sciencedirect.com/science/article/pii/S089360809600127X</a> </p><p>People have used evolutionary algorithms to evolve neural nets in quite a few different ways. Most relevant to GP maps are those using &quot;indirect encodings&quot; or &quot;developmental encodings&quot; (an area called artificial embryogeny, etc, etc). A popular method is HyperNEAT. This is known to make neural networks be biased towards simplicity/regularity (<a class="tc-tiddlylink-external" href="http://www.evolvingai.org/huizinga-mouret-clune-2014-evolving-neural-networks-are" rel="noopener noreferrer" target="_blank">http://www.evolvingai.org/huizinga-mouret-clune-2014-evolving-neural-networks-are</a> for instance). However, I am not aware of any research showing a distinction similar to ARD1 vs ARD2 for these GP maps, which would be interesting to find.</p><p>Three recent papers from DeepMind and OpenAI (the two biggest groups on AI right now), which show some potential uses of evolution for deep learning: 
<a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1606.02580" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/1606.02580</a>
<a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1701.08734" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/1701.08734</a>
<a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1703.03864" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/1703.03864</a>
Another one: <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1703.00548" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/1703.00548</a> 
At least one of them uses HyperNEAT, to evolve the topology only. I think all of these train the weights of the synaptic connections, by gradient descent. I would be interested to learn/think more about the relation between evolution vs gradient descent...</p><p>People have also thought to use neural networks, as GP maps, or as &quot;genetic operators&quot;: 
<a class="tc-tiddlylink-external" href="https://link.springer.com/chapter/10.1007/978-3-319-10762-2_11" rel="noopener noreferrer" target="_blank">https://link.springer.com/chapter/10.1007/978-3-319-10762-2_11</a>
<a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1604.04153" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/1604.04153</a>
Also using neural networks as a GP map to evolve neural networks!: <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1609.09106" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/1609.09106</a> they use gradient descent, showing that the idea of GP maps may have applications in the more popular gradient descent learning.
</p></div>


</div>

</p>

</section>
</body>
</html>
