<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.15" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>New advances in deep learning: Cosmos — Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Artificial%20intelligence%20innovation"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
New advances in deep learning
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 3rd August 2018 at 1:25am
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Artificial intelligence innovation
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p>review/position paper on giving neural nets the right biases – <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1806.01261" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/1806.01261</a></p><p>neural theorem proving: <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1807.08204" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/1807.08204</a></p><p>2017 updates: <a class="tc-tiddlylink-external" href="http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/" rel="noopener noreferrer" target="_blank">http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/</a></p><p>DeepMind publications: <a class="tc-tiddlylink-external" href="https://deepmind.com/research/publications/" rel="noopener noreferrer" target="_blank">https://deepmind.com/research/publications/</a></p><p>Conferences: <a class="tc-tiddlylink-external" href="http://iclr.cc/doku.php?id=iclr2017:schedule" rel="noopener noreferrer" target="_blank">ICLR</a>, NIPS</p><p>Transformer networks. <a class="tc-tiddlylink-external" href="https://research.googleblog.com/2017/08/transformer-novel-neural-network.html" rel="noopener noreferrer" target="_blank">https://research.googleblog.com/2017/08/transformer-novel-neural-network.html</a> – New: Universal Transformers networks</p><p><a class="tc-tiddlylink tc-tiddlylink-missing" href="Attention%2520is%2520all%2520you%2520need.html">Attention is all you need</a>..</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Generative%2520adversarial%2520network.html">Generative adversarial network</a>s, etc.</p><p>Generating video!</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Variational%2520autoencoder.html">Variational autoencoder</a></p><p>WaveNet, PixelRNN, SampleRNN</p><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Augmented%2520RNN.html">Augmented RNN</a>s (<a class="tc-tiddlylink tc-tiddlylink-resolves" href="Memory.html">Memory</a>-augmented)</h3><h3 class="">Neural architecture search</h3><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1611.01578" rel="noopener noreferrer" target="_blank">Neural Architecture Search with Reinforcement Learning</a></p><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Meta-learning.html">Meta-learning</a></h3><p>Avoiding catastrophic forgetting..</p><p>Capsules. <a class="tc-tiddlylink-external" href="https://nips.cc/Conferences/2017/Schedule?showEvent=9167" rel="noopener noreferrer" target="_blank">Dynamic Routing Between Capsules</a></p><p>.. <a class="tc-tiddlylink tc-tiddlylink-resolves" href="PoseNet.html">PoseNet</a></p><hr><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Multisensory%2520integration.html">Multisensory integration</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Deep%2520learning%2520theory.html">Deep learning theory</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1412.6651" rel="noopener noreferrer" target="_blank">Deep learning with Elastic Averaging SGD</a></p><p>We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master).</p><hr><p>Solve vanishing gradients problem, allow be computations to depend on all/any of the existing computations, thus creating a DAG structure instead of a layered one: <a class="tc-tiddlylink-external" href="http://www.jonolick.com/home/dagnn-a-deeper-fully-connected-network" rel="noopener noreferrer" target="_blank">http://www.jonolick.com/home/dagnn-a-deeper-fully-connected-network</a></p><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Batch%2520normalization.html">Batch normalization</a></h3><p><a class="tc-tiddlylink-external" href="http://arxiv.org/abs/1603.09382v1" rel="noopener noreferrer" target="_blank"> Deep Networks with Stochastic Depth</a> <a class="tc-tiddlylink-external" href="http://deliprao.com/archives/134" rel="noopener noreferrer" target="_blank">Stochastic Depth Networks will Become the New Normal</a></p><p><a class="tc-tiddlylink-external" href="http://arxiv.org/abs/1505.00387" rel="noopener noreferrer" target="_blank"> Highway Networks</a></p><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Dropout.html">Dropout</a></h3><p><a class="tc-tiddlylink-external" href="https://en.m.wikipedia.org/wiki/Modular_neural_network" rel="noopener noreferrer" target="_blank">https://en.m.wikipedia.org/wiki/Modular_neural_network</a> STN?</p><p>DCGAN <a class="tc-tiddlylink-external" href="http://arxiv.org/abs/1511.06434" rel="noopener noreferrer" target="_blank">http://arxiv.org/abs/1511.06434</a></p><p>DRAW <a class="tc-tiddlylink-external" href="http://arxiv.org/abs/1502.04623" rel="noopener noreferrer" target="_blank">http://arxiv.org/abs/1502.04623</a></p><p>Soft/hard attention <a class="tc-tiddlylink-external" href="https://www.google.es/url?sa=t&amp;source=web&amp;rct=j&amp;url=http://arxiv.org/pdf/1502.03044&amp;ved=0ahUKEwi4yof-jPjLAhVC5xoKHcTjDM4QFgggMAA&amp;usg=AFQjCNEs1Yw8fZF9oaqo73cwbHJqKwQHTw" rel="noopener noreferrer" target="_blank">https://www.google.es/url?sa=t&amp;source=web&amp;rct=j&amp;url=http://arxiv.org/pdf/1502.03044&amp;ved=0ahUKEwi4yof-jPjLAhVC5xoKHcTjDM4QFgggMAA&amp;usg=AFQjCNEs1Yw8fZF9oaqo73cwbHJqKwQHTw</a></p><p>CharCNN <a class="tc-tiddlylink-external" href="https://www.google.es/url?sa=t&amp;source=web&amp;rct=j&amp;url=http://arxiv.org/pdf/1508.06615&amp;ved=0ahUKEwjZqaXnk_jLAhWCsxQKHZsuApUQFgglMAM&amp;usg=AFQjCNHk8JQpI98eUtyiluv7d2G9aWRtyA" rel="noopener noreferrer" target="_blank">https://www.google.es/url?sa=t&amp;source=web&amp;rct=j&amp;url=http://arxiv.org/pdf/1508.06615&amp;ved=0ahUKEwjZqaXnk_jLAhWCsxQKHZsuApUQFgglMAM&amp;usg=AFQjCNHk8JQpI98eUtyiluv7d2G9aWRtyA</a></p><p>NeuralStyle <a class="tc-tiddlylink-external" href="https://github.com/jcjohnson/neural-style" rel="noopener noreferrer" target="_blank">https://github.com/jcjohnson/neural-style</a></p><p>&quot;Take a look at @karpathy's Tweet: <a class="tc-tiddlylink-external" href="https://twitter.com/karpathy/status/709465955223543808?s=09" rel="noopener noreferrer" target="_blank">https://twitter.com/karpathy/status/709465955223543808?s=09</a>&quot;</p><p><a class="tc-tiddlylink-external" href="http://arxiv.org/abs/1604.00790" rel="noopener noreferrer" target="_blank">http://arxiv.org/abs/1604.00790</a> bidirectional LSTM</p><p><a class="tc-tiddlylink-external" href="http://www.computervisionblog.com/2016/06/deep-learning-trends-iclr-2016.html" rel="noopener noreferrer" target="_blank">http://www.computervisionblog.com/2016/06/deep-learning-trends-iclr-2016.html</a></p><p>Adversarial networks</p><p>Neural Turing machines, neural programmers-interpreters</p><p><a class="tc-tiddlylink-external" href="http://www.kdnuggets.com/2016/09/9-key-deep-learning-papers-explained.html" rel="noopener noreferrer" target="_blank">http://www.kdnuggets.com/2016/09/9-key-deep-learning-papers-explained.html</a></p><hr><p>A couple of cool recent neural network developments!
* Lip reading
* Neural enhance! Super-resolution of images (like in movies but real ;) )</p><p>LipNet. Lip reading NN: <a class="tc-tiddlylink-external" href="http://prostheticknowledge.tumblr.com/post/152735696866/lipnet-deep-learning-research-from-the-university" rel="noopener noreferrer" target="_blank">http://prostheticknowledge.tumblr.com/post/152735696866/lipnet-deep-learning-research-from-the-university</a></p><p>image super-resolution using deep convolutional networks! try here: <a class="tc-tiddlylink-external" href="http://waifu2x.udp.jp/" rel="noopener noreferrer" target="_blank">http://waifu2x.udp.jp/</a>
NeuralEnhance lets you apply 4x super-resolution to your photos CSI-style in only 340 lines of code!
<a class="tc-tiddlylink-external" href="https://github.com/alexjc/neural-enhance" rel="noopener noreferrer" target="_blank">https://github.com/alexjc/neural-enhance</a></p><p><a class="tc-tiddlylink-external" href="https://twitter.com/karpathy" rel="noopener noreferrer" target="_blank">https://twitter.com/karpathy</a></p><p><a class="tc-tiddlylink-external" href="https://twitter.com/alexjc" rel="noopener noreferrer" target="_blank">https://twitter.com/alexjc</a></p><p><a class="tc-tiddlylink-external" href="https://twitter.com/NandoDF" rel="noopener noreferrer" target="_blank">https://twitter.com/NandoDF</a>
</p></div>


</div>

</p>

</section>
</body>
</html>
