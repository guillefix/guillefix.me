<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.15" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Variational inference: Cosmos — Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Approximate%20Bayesian%20inference tc-tagged-Statistical%20inference"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Variational inference
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 1st December 2018 at 1:58am
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Approximate Bayesian inference
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
<span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Statistical inference
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p><em>aka VI, variational Bayes, <a class="tc-tiddlylink tc-tiddlylink-missing" href="Ensemble%2520learning.html">Ensemble learning</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Minimum%2520description%2520length.html">Minimum description length</a></em></p><p>Variational inference <span style="color: DarkMagenta">is based on reducing inference to an <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Optimization.html">Optimization</a> problem</span>. In <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Statistical%2520inference.html">Statistical inference</a>, we are often interested in finding a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Posterior.html">Posterior</a> distribution, defined via a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Probabilistic%2520model.html">Probabilistic model</a>, like a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Graphical%2520model.html">Graphical model</a>, and some observed data. However, posterior distributions are often hard to compute explicitly, and an <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Approximate%2520Bayesian%2520inference.html">approximate method</a> is needed.</p><blockquote><p><strong>Variational inference</strong> consists on finding an approximate representation of the posterior distribution (the <strong>variational distribution</strong>), minimizing the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Relative%2520entropy.html">KL divergence</a> with the real <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Posterior.html">Posterior</a> distribution. This is often done by choosing a tractable family of distributions <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">P</mi></mrow></mrow><annotation encoding="application/x-tex">\mathcal{P}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.08222em;">P</span></span></span></span></span></span> and maximizing a <strong>variational objective</strong> function like the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Evidence%2520lower%2520bound.html">Evidence lower bound</a> (ELBO) over this family.</p></blockquote><p>See <a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1601.00670.pdf" rel="noopener noreferrer" target="_blank">here</a>.</p><p>The interesting thing is that the KL divergence/ELBO is defined via a in integral over the latent/hidden variables, which is precisely the thing that we assumed was hard, and the reason to use variational inference! However, the variational distribution <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>q</mi><mo>(</mo><mi>w</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">q(w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span></span> may be much easier to integrate than the posterior!
<small>Otherwise, we can use samples instead of the integation and use <a class="tc-tiddlylink tc-tiddlylink-missing" href="Stochastic%2520optimization.html">Stochastic optimization</a> (like <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Stochastic%2520gradient%2520descent.html">Stochastic gradient descent</a>). But we could also have used sampling for the original integral! Or we could have used <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Monte%2520Carlo.html">Monte Carlo</a> methods (<a class="tc-tiddlylink tc-tiddlylink-missing" href="Markov%2520Chain%2520Monte%2520Carlo.html">MCMC</a>! In that case, the nontrivial thing is that using stochastic optimization can give good answer with much fewer samples than when approximating the integral in naive Bayesian inference (the denominator..). It is also typically more scalable (efficient for large datasets) than MCMC methods. </small></p><p>To understand this would require analysis of MCMC, Monte Carlo integration, and stochastic optimization.</p><h3 class=""><u>Zero-avoidance property</u></h3><p><span style="color: blue">Zero-avoidance property</span> of variational inference.  In <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mi mathvariant="double-struck">D</mi></mrow><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo>[</mo><mi>q</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>p</mi><mo>]</mo></mrow><annotation encoding="application/x-tex">\mathbb{D}_{KL}[q||p]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbb">D</span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span><span class="mord mathit">L</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">[</span><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="mord mathrm">∣</span><span class="mord mathrm">∣</span><span class="mord mathit">p</span><span class="mclose">]</span></span></span></span></span>  there  is  a  large  positive contribution  from  regions  in  which <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span></span></span></span></span> is near zero unless <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">q</span></span></span></span></span> is also close to zero.  On the other hand, <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mi mathvariant="double-struck">D</mi></mrow><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo>[</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo>]</mo></mrow><annotation encoding="application/x-tex">\mathbb{D}_{KL}[p||q]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbb">D</span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span><span class="mord mathit">L</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">[</span><span class="mord mathit">p</span><span class="mord mathrm">∣</span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="mclose">]</span></span></span></span></span> is  minimized  by <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">q</span></span></span></span></span> that  covers  the mass of <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span></span></span></span></span>. You want the first distribution to be like a &quot;subset&quot; of the second; because the first is &quot;testing&quot; the second one's code. So you don't want it to test it in a place which will utterly surprise the second one, as it'll have an infinite code length there, and KL divergence, would diverge<sup>badum tss</sup></p><p>The above implies that variational inference via the ELBO overconcentrates the posterior. Minimizing <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mi mathvariant="double-struck">D</mi></mrow><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo>[</mo><mi>p</mi><mo>(</mo><mi>w</mi><mi mathvariant="normal">∣</mi><mi>D</mi><mo>)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo>(</mo><mi>w</mi><mo>)</mo><mo>]</mo></mrow><annotation encoding="application/x-tex">\mathbb{D}_{KL}[p(w|D)||q(w)]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbb">D</span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span><span class="mord mathit">L</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">[</span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mord mathrm">∣</span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></span> leads to a different type of method ,e.g.,  <a class="tc-tiddlylink tc-tiddlylink-missing" href="Expectation-propagation.html">Expectation-propagation</a>.  This is then expected to underconcentrate the posterior.The two  methods  obtain  very  different approximations. See [Bishop, 2006] Figure 10.3</p><p><small>See <a class="tc-tiddlylink-external" href="https://emtiyaz.github.io/teaching/ds3_2018/approxBayesInference.pdf" rel="noopener noreferrer" target="_blank">here</a></small></p><p>I guess variational inference can be used to estimate the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Marginal%2520likelihood.html">Marginal likelihood</a>, by inverting Bayes' theorem to get <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mi>D</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mi>p</mi><mo>(</mo><mi>D</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mo>)</mo><mi>p</mi><mo>(</mo><mi>w</mi><mo>)</mo></mrow><mrow><mi>p</mi><mo>(</mo><mi>w</mi><mi mathvariant="normal">∣</mi><mi>D</mi><mo>)</mo></mrow></mfrac><mo>≈</mo><mfrac><mrow><mi>p</mi><mo>(</mo><mi>D</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mo>)</mo><mi>p</mi><mo>(</mo><mi>w</mi><mo>)</mo></mrow><mrow><mi>q</mi><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">p(D) = \frac{p(D|w)p(w)}{p(w|D)}\approx \frac{p(D|w)p(w)}{q(w)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.01em;"></span><span class="strut bottom" style="height:1.53em;vertical-align:-0.52em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.34500000000000003em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mclose">)</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.485em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mrel">≈</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.34500000000000003em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.485em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span>, which can be evaluated at any <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span></span>, but preferentially at <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span></span> sampled from variational distribution, as in those <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span></span> the variational approximation is likely better. The numerator is tractable to compute in cases where VI is applied. Using this formula we can see that a method (such as ELBO) which tends to overconcentrate the posterior (resulting in larger <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>q</mi><mo>(</mo><mi>w</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">q(w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span></span>), will give a smaller likelihood (the intuition being that if this data is less likely is because there are fewer <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span></span> which produce it, so we give more weight to this one); on the other hand methods (such as EP) which tend to underconcentrate posterior, will give a larger likelihood. This probably explains why we obtain lower values for the PAC-Bayes bounds when using EP, when compared to VI.</p><h2 class=""><u>Mean field VI</u></h2><p>A choice for the space of distributions over which to optimize, is distributions where the individual parameters are independent. This is known as a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Mean%2520field%2520theory.html">mean field approximation</a> (as we ignore statistical dependences).</p><h2 class=""><u>Applications</u></h2><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Variational%2520autoencoder.html">Variational autoencoder</a></p><hr><p>It is typically applied for <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Bayesian%2520inference.html">Bayesian inference</a>, but to be more specific it may be specified as Bayesian variational inference.</p></div>


</div>

</p>

</section>
</body>
</html>
