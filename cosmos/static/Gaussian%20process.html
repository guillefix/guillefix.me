<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.21" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Gaussian process: Cosmos — All that is, or was, or ever will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">
<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists  tc-tagged-Generative%20supervised%20learning tc-tagged-Probabilistic%20model" data-tags="[[Generative supervised learning]] [[Probabilistic model]]" data-tiddler-title="Gaussian process"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Gaussian process
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 3rd April 2019 at 4:57pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">
<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Generative supervised learning
</span>
<span class="tc-drop-down tc-reveal" hidden="true"></span></span><span class="tc-tag-list-item">
<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Probabilistic model
</span>
<span class="tc-drop-down tc-reveal" hidden="true"></span></span></div>
</div>

<div class="tc-tiddler-body tc-reveal"><p><strong>Gaussian processes</strong> are <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Probabilistic%2520model.html">Probabilistic model</a>s, defined as a probability distribution over a set of random variables (i.e. a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Stochastic%2520process.html">Stochastic process</a>) where any finite set of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Random%2520variable.html">Random variable</a>s in the process is <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Joint%2520probability%2520distribution.html">jointly</a> <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Gaussian.html">Gaussian</a> distributed. This set of random variables is usually interpreted as the output values of a function on an input space, so that we say that Gaussian processes define a <em>distribution over functions</em>, as we repeat below.</p><p><a class="tc-tiddlylink-external" href="https://www.robots.ox.ac.uk/~mebden/reports/GPtutorial.pdf" rel="noopener noreferrer" target="_blank">Good quick intro</a>.  <a class="tc-tiddlylink-external" href="https://distill.pub/2019/visual-exploration-gaussian-processes/[[. See [[Gaussian Processes for Machine Learning|http://www.gaussianprocess.org/gpml/chapters/" rel="noopener noreferrer" target="_blank">Visual introudction (distill)</a> for a thorough introduction. Also intro <a class="tc-tiddlylink-external" href="http://publications.aston.ac.uk/1305/1/NCRG_2001_014.pdf#page=1" rel="noopener noreferrer" target="_blank">here</a></p><p>A Gaussian process is thus a <strong>distribution over functions</strong> such as the values of the functions at a finite set of points are jointly distributed by a <a class="tc-tiddlylink tc-tiddlylink-missing" href="Multivariate%2520Gaussian%2520distribution.html">Multivariate Gaussian distribution</a> with a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Covariance%2520matrix.html">Covariance matrix</a> that is given by a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Kernel.html">Kernel</a> function (ensuring consistency via what's called the marginaliation property), which is a function of two <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Input.html">Input</a>s.  This is also called a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Gaussian%2520random%2520field.html">Gaussian random field</a>.</p><p>In terms of equations, the values of the function at any finite set of  <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span></span></span></span></span> inputs <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">(x_1,...,x_n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">n</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span></span>, are jointly distributed with a Gaussian distribution:,</p><p><span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mrow><mrow><mi>θ</mi></mrow><mo>∼</mo><mi>Q</mi></mrow></msub><mrow><mo fence="true">(</mo><msub><mi>f</mi><mrow><mi>θ</mi></mrow></msub><mo>(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>)</mo><mo>=</mo><msub><mover accent="true"><mrow><mi>y</mi></mrow><mo>~</mo></mover><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>f</mi><mrow><mi>θ</mi></mrow></msub><mo>(</mo><msub><mi>x</mi><mi>n</mi></msub><mo>)</mo><mo>=</mo><msub><mover accent="true"><mrow><mi>y</mi></mrow><mo>~</mo></mover><mi>n</mi></msub><mo fence="true">)</mo></mrow><mo>∝</mo><mi>exp</mi><mrow><mrow><mo fence="true">(</mo><mo>−</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mn>2</mn></mrow></mfrac><msup><mrow><mover accent="true"><mrow><mi mathvariant="bold">y</mi></mrow><mo>~</mo></mover></mrow><mi>T</mi></msup><msup><mrow><mi mathvariant="bold">K</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mover accent="true"><mrow><mi mathvariant="bold">y</mi></mrow><mo>~</mo></mover></mrow><mo fence="true">)</mo></mrow></mrow><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">
   P_{\mathbf{\theta}\sim Q} \left(f_\mathbf{\theta}(x_1)=\tilde{y}_1,...,f_\mathbf{\theta}(x_n)=\tilde{y}_n\right) \propto \exp{\left(-\frac{1}{2}\mathbf{\tilde{y}}^T \mathbf{K}^{-1}\mathbf{\tilde{y}}\right)},
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.45em;"></span><span class="strut bottom" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span><span class="mrel">∼</span><span class="mord mathit">Q</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.10764em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mrel">=</span><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span><span style="top:-0.35em;margin-left:0.11112em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>~</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.10764em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">n</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mrel">=</span><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span><span style="top:-0.35em;margin-left:0.11112em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>~</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">n</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;">)</span></span><span class="mrel">∝</span><span class="mop">exp</span><span class="mord displaystyle textstyle uncramped"><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord">−</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.686em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm">2</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord mathbf" style="margin-right:0.01597em;">y</span></span></span><span style="top:-0.36344em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>~</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">K</span></span><span class="vlist"><span style="top:-0.41300000000000003em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord">−</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord displaystyle textstyle uncramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord mathbf" style="margin-right:0.01597em;">y</span></span></span><span style="top:-0.36344em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>~</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;"><span class="delimsizing size3">)</span></span></span></span><span class="mpunct">,</span></span></span></span></span></span></p><p>where <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mover accent="true"><mrow><mi mathvariant="bold">y</mi></mrow><mo>~</mo></mover></mrow><mo>=</mo><mo>(</mo><msub><mover accent="true"><mrow><mi>y</mi></mrow><mo>~</mo></mover><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mover accent="true"><mrow><mi>y</mi></mrow><mo>~</mo></mover><mi>n</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathbf{\tilde{y}}=(\tilde{y}_1,...,\tilde{y}_n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathbf" style="margin-right:0.01597em;">y</span></span></span><span style="top:-0.36344em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>~</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span><span class="mrel">=</span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span><span style="top:-0.35em;margin-left:0.11112em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>~</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mpunct">,</span><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span><span style="top:-0.35em;margin-left:0.11112em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>~</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">n</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span></span>. The entries of the covariance matrix $\mathbf{K}$ are given by the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Kernel.html">Kernel</a> function <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span></span> as <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>K</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>k</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>x</mi><mi>j</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">K_{ij}=k(x_i,x_j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.07153em;">K</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07153em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span></span>.</p><p>Kernels encode how &quot;similar&quot; two points <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span> and <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">x_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span>  in the input <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Domain.html">Domain</a> of the distribution over functions are. What this means precisely is that the kernel at these two points, <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>x</mi><mi>j</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">k(x_i,x_j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span></span> is high, the the function is more likely to have similar values at these two points. This allows to encode a wide variety of prior knowledge/assumptions about the functions one is trying to learn, like <a class="tc-tiddlylink tc-tiddlylink-missing" href="Invariance.html">Invariance</a>s/<a class="tc-tiddlylink tc-tiddlylink-resolves" href="Symmetry.html">symmetries</a>. Often, one chooses kernels that prefers smoothness, so that that <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span></span>s which are close <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span></span>s under some <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Metric.html">Metric</a> (often <a class="tc-tiddlylink tc-tiddlylink-missing" href="Euclidean%2520metric.html">Euclidean metric</a>) are more likely to be similar... To see more on choice of kernels, see discussion in the page of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Reproducing%2520kernel%2520Hilbert%2520space.html">Reproducing kernel Hilbert space</a>s.</p><h3 class=""><u>Application in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Generative%2520supervised%2520learning.html">Generative supervised learning</a></u></h3><p>Gaussian processes are usually used in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Generative%2520supervised%2520learning.html">Generative supervised learning</a>. In brief, generative supervised learning works as follows: assume a certain model <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mrow><mi mathvariant="bold">y</mi></mrow><mi mathvariant="normal">∣</mi><mrow><mi mathvariant="bold">x</mi></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">p(\mathbf{y}|\mathbf{x})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord textstyle uncramped"><span class="mord mathbf" style="margin-right:0.01597em;">y</span></span><span class="mord mathrm">∣</span><span class="mord textstyle uncramped"><span class="mord mathbf">x</span></span><span class="mclose">)</span></span></span></span></span> where the <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span></span>s correspond to the <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span></span>s in these vectors. To learn a predictor from a set of data, we do the following: given output <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span></span>s for some inputs <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span></span>s as data, we can compute a <a class="tc-tiddlylink tc-tiddlylink-missing" href="Predictive%2520distribution.html">Predictive distribution</a> for the outputs <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span></span>s corresponding to unobserved inputs <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span></span>s (see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Bayesian%2520inference.html">Bayesian inference</a>). </p><p>A <strong>Gaussian process model</strong> models <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mrow><mi mathvariant="bold">y</mi></mrow><mi mathvariant="normal">∣</mi><mrow><mi mathvariant="bold">x</mi></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">p(\mathbf{y}|\mathbf{x})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord textstyle uncramped"><span class="mord mathbf" style="margin-right:0.01597em;">y</span></span><span class="mord mathrm">∣</span><span class="mord textstyle uncramped"><span class="mord mathbf">x</span></span><span class="mclose">)</span></span></span></span></span> as <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mrow><mi mathvariant="bold">y</mi></mrow><mi mathvariant="normal">∣</mi><mrow><mi mathvariant="bold">f</mi></mrow><mo>)</mo><mi>p</mi><mo>(</mo><mrow><mi mathvariant="bold">f</mi></mrow><mi mathvariant="normal">∣</mi><mrow><mi mathvariant="bold">x</mi></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">p(\mathbf{y}|\mathbf{f})p(\mathbf{f}|\mathbf{x})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord textstyle uncramped"><span class="mord mathbf" style="margin-right:0.01597em;">y</span></span><span class="mord mathrm">∣</span><span class="mord textstyle uncramped"><span class="mord mathbf" style="margin-right:0.10903em;">f</span></span><span class="mclose">)</span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord textstyle uncramped"><span class="mord mathbf" style="margin-right:0.10903em;">f</span></span><span class="mord mathrm">∣</span><span class="mord textstyle uncramped"><span class="mord mathbf">x</span></span><span class="mclose">)</span></span></span></span></span>, where <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mrow><mi mathvariant="bold">y</mi></mrow><mi mathvariant="normal">∣</mi><mrow><mi mathvariant="bold">f</mi></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">p(\mathbf{y}|\mathbf{f})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord textstyle uncramped"><span class="mord mathbf" style="margin-right:0.01597em;">y</span></span><span class="mord mathrm">∣</span><span class="mord textstyle uncramped"><span class="mord mathbf" style="margin-right:0.10903em;">f</span></span><span class="mclose">)</span></span></span></span></span> is a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Likelihood%2520function.html">Likelihood function</a> connecting outputs to the values of a &quot;latent function&quot; <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span></span>. This latent function is distributed according to a Gaussian process, as described above, which can now be interpreted as a <em>prior over functions</em>.</p><p>They are <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Bayesian%2520inference.html">Bayesian</a> <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Kernel%2520method.html">Kernel method</a>s</p><p>Using Gaussian processes for can be efficiently done up to datasets of about 100,000 data points, with current techniques and computers.</p><hr><p>They are equivalent to Bayesian <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Kernel%2520ridge%2520regression.html">Kernel ridge regression</a>! (what they call the &quot;weight-space view&quot; in <a class="tc-tiddlylink-external" href="http://www.gaussianprocess.org/gpml/chapters/RW2.pdf" rel="noopener noreferrer" target="_blank">here</a>)</p><p>See section 4.3 in Murphy's book (Machine learning - a probabilistic perspective) to see the derivation of the fact that the marginal distribution of a subset of variables from a larger set of random variables which have a <a class="tc-tiddlylink tc-tiddlylink-missing" href="Multivariate%2520Gaussian.html">Gaussian joint distribution</a>. This is why the Gaussian process property (that the values at any set of points have joint Gaussian distribution) corresponds to a Gaussian prior over functions (<a class="tc-tiddlylink tc-tiddlylink-resolves" href="Gaussian%2520random%2520field.html">Gaussian random field</a>; <a class="tc-tiddlylink tc-tiddlylink-missing" href="Physical%2520field.html">field</a> with quadratic energy functional..; see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Path%2520integral.html">Path integral</a> ).</p><hr><p><a class="tc-tiddlylink-external" href="https://pdfs.semanticscholar.org/3e38/092b962bcb430fdcebf1407d1299adb1a10b.pdf" rel="noopener noreferrer" target="_blank">Relationships between Gaussian processes, Support Vector machines and Smoothing Splines</a> – <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Support%2520vector%2520machine.html">Support vector machine</a> –<a class="tc-tiddlylink tc-tiddlylink-missing" href="Spline.html">Spline</a>s</p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1711.00165" rel="noopener noreferrer" target="_blank">Deep Neural Networks as Gaussian Processes</a> – Extensions for <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Convolutional%2520neural%2520network.html">CNN</a>s</p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1804.11271" rel="noopener noreferrer" target="_blank">Gaussian Process Behaviour in Wide Deep Neural Networks</a></p><p><a class="tc-tiddlylink-external" href="https://en.wikipedia.org/wiki/Gaussian_process" rel="noopener noreferrer" target="_blank">https://en.wikipedia.org/wiki/Gaussian_process</a></p><hr><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Approximate%2520Bayesian%2520inference.html">Approximate inference</a> for Gaussian processes</u></h2><h3 class=""><u>Gaussian processes with non-Gaussian likelihood</u></h3><p>two major obstacles: non-Gaussianity
of the posterior process and the size of the kernel matrix K0(xi,xj). A first obvious problem stems from the fact
that the posterior process is usually non-Gaussian (except when the likelihood itself is Gaussian in the fx). Hence,
in many important cases its analytical form precludes an exact evaluation of the multidimensional integrals that
occur in posterior averages. Nevertheless, various methods have been introduced to approximate these averages. A
variety of such methods may be understood as approximations of the non-Gaussian posterior process by a Gaussian
one (Jaakkola and Haussler 1999; Seeger 2000), for instance in (Williams and Barber 1998) the posterior mean
is replaced by the posterior maximum (MAP) and information about the fluctuations are derived by a quadratic
expansion around this maximum</p><p>Usually the observed labels / <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span></span> are assumed to be either equal to the function modelled by the GP, or have a Gaussian distribution around it (what's called a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Gaussian.html">Gaussian</a> <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Likelihood%2520function.html">likelihood</a> – note that here the function <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span></span> works like the parameters in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Bayesian%2520inference.html">Bayesian inference</a>).</p><p>If one assumes a non-Gaussian likelihood, then the problem is not <a class="tc-tiddlylink tc-tiddlylink-missing" href="Analytically%2520tractable.html">Analytically tractable</a> any more..</p><p>There are several approximations which are used then </p><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Laplace%2520method.html">Laplacian approximation</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Expectation%2520propagation.html">Expectation propagation</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Variational%2520inference.html">Variational inference</a></li></ul><p>The most common case is in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Gaussian%2520process%2520classification.html">Gaussian process classification</a>. See <a class="tc-tiddlylink-external" href="http://www.gaussianprocess.org/gpml/chapters/RW3.pdf" rel="noopener noreferrer" target="_blank">here</a></p><h3 class=""><u>Further techniques</u></h3><p>One approach is to partition the data set into separate groups [e.g. Snelsonand  Ghahramani,  2007,  Urtasun  and  Darrell,  2008]. An  alternative  is  to  build  a  low  rank  approximationto the covariance matrix based around 'inducing vari-ables' [see e.g. <a class="tc-tiddlylink-external" href="http://publications.aston.ac.uk/1305/1/NCRG_2001_014.pdf" rel="noopener noreferrer" target="_blank">Csato and Opper, 2002</a>, Seeger et al.,2003, <a class="tc-tiddlylink-external" href="http://www.jmlr.org/papers/volume6/quinonero-candela05a/quinonero-candela05a.pdf" rel="noopener noreferrer" target="_blank">Quinonero Candela and Rasmussen, 2005</a>, <a class="tc-tiddlylink-external" href="http://proceedings.mlr.press/v5/titsias09a.html" rel="noopener noreferrer" target="_blank">Titsias, 2009</a>].  These approaches lead to a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Computational%2520complexity.html">Computational complexity</a> of <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><msup><mi>m</mi><mn>2</mn></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(nm^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathit">n</span><span class="mord"><span class="mord mathit">m</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span></span> and storage demands of <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mi>m</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(nm)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathit">n</span><span class="mord mathit">m</span><span class="mclose">)</span></span></span></span></span> where <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span></span></span></span></span> is number of data points and <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">m</span></span></span></span></span> is  a  user  selected  parameter  governing  the number  of  inducing  variables. </p><p>In <a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1309.6835.pdf" rel="noopener noreferrer" target="_blank">this paper</a>, they then introduced <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Variational%2520inference%2520for%2520Gaussian%2520processes.html">Variational inference for Gaussian processes</a>.</p><p><a class="tc-tiddlylink-external" href="http://publications.aston.ac.uk/1305/1/NCRG_2001_014.pdf" rel="noopener noreferrer" target="_blank">Sparse On-Line Gaussian Processes</a></p><hr><h2 class=""><u>Theory of Gaussian processess</u></h2><p><u>Gaussian processes where training data cover the whole input space</u> (non trivial because the y values are still random samples according to likelihood, and our task is to esimate the latent <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span></span> (which we can use to estimate future <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span></span>))</p><p><a class="tc-tiddlylink-external" href="https://link.springer.com/content/pdf/10.1007%2F978-3-642-38886-6_17.pdf" rel="noopener noreferrer" target="_blank">paper</a></p><h2 class=""><u>Gaussian process <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Kernel.html">Kernel</a>s</u></h2><p>See <a class="tc-tiddlylink-external" href="https://www.cs.toronto.edu/~duvenaud/cookbook/" rel="noopener noreferrer" target="_blank">https://www.cs.toronto.edu/~duvenaud/cookbook/</a></p><p><u>Combination of kernels</u></p><p>See <a class="tc-tiddlylink-external" href="https://distill.pub/2019/visual-exploration-gaussian-processes/" rel="noopener noreferrer" target="_blank">here</a> and <a class="tc-tiddlylink-external" href="https://www.cs.toronto.edu/~duvenaud/cookbook/" rel="noopener noreferrer" target="_blank">here</a></p><p>Remember that kernel functions with one of its arguments evaluated are members of the reproducing kernel Hilbert space to which all the functions supported by a particular Gaussian process belong.</p><p>Therefore adding kernels, amounts to adding the functions on these two spaces. That is why the resulting functions work like this when combining kernels!</p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-missing" href="Automated%2520statistician.html">Automated statistician</a></u></h3><p>Automatically chooses kernels, and does many other things: <a class="tc-tiddlylink-external" href="https://www.automaticstatistician.com/index/" rel="noopener noreferrer" target="_blank">https://www.automaticstatistician.com/index/</a></p></div>



</div>

</p>
</section>
</body>
</html>
