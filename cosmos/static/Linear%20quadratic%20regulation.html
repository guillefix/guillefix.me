<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.17" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Linear quadratic regulation: Cosmos — Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Reinforcement%20learning%20in%20continuous%20state%20space " data-tags="[[Reinforcement learning in continuous state space]]" data-tiddler-title="Linear quadratic regulation"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Linear quadratic regulation
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 4th November 2016 at 1:43pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Reinforcement learning in continuous state space
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p>An type of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Reinforcement%2520learning%2520in%2520continuous%2520state%2520space.html">Reinforcement learning in continuous state space</a></p><p>–&gt;using <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Dynamic%2520programming.html">Dynamic programming</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=-ff6l5D8-j8&amp;index=18&amp;list=PLA89DCFA6ADACE599#t=27m50s" rel="noopener noreferrer" target="_blank">intro</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=-ff6l5D8-j8&amp;index=18&amp;list=PLA89DCFA6ADACE599#t=30m" rel="noopener noreferrer" target="_blank">State transition probabilites</a>. These matrices can be obtained by <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Linear%2520regression.html">Linear regression</a> from samples of the real or simulated dynamics of the system; or they can be a linearization of a non-linear transition function, derived from physics, or other assumptions. This constitutes the linear model neede for LQR</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=-ff6l5D8-j8&amp;index=18&amp;list=PLA89DCFA6ADACE599#t=33m10s" rel="noopener noreferrer" target="_blank">Reward function</a></p><p>Goal: <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=-ff6l5D8-j8&amp;index=18&amp;list=PLA89DCFA6ADACE599#t=53m20s" rel="noopener noreferrer" target="_blank">Finding optimal policy</a>, modelling world as a finite-horizon <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Markov%2520decision%2520process.html">MDP</a>, which can be solved recursively, using <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Dynamic%2520programming.html">Dynamic programming</a>. It turns out that the optimal action is a linear function of the current state, in this case.</p><p>The recursive equation for calculating the optimal value function at time <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.61508em;"></span><span class="strut bottom" style="height:0.61508em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span></span></span></span></span>, given its value at time <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span></span> is known as the discrete-time <strong>Riccati equation</strong>.</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=-ff6l5D8-j8&amp;index=18&amp;list=PLA89DCFA6ADACE599#t=1h12m05s" rel="noopener noreferrer" target="_blank">algorithm</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=-ff6l5D8-j8&amp;index=18&amp;list=PLA89DCFA6ADACE599#t=1h15m20s" rel="noopener noreferrer" target="_blank">Advantage over discretization methods</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=18m50" rel="noopener noreferrer" target="_blank">recap</a> –&gt; <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=31m30s" rel="noopener noreferrer" target="_blank">some comments</a>, don't need covariance.</p><p><u>Differential dynamic programming (DDP)</u></p><p>Turns out to be a form of local search algorithm</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=35m" rel="noopener noreferrer" target="_blank">video</a></p></div>


</div>

</p>

</section>
</body>
</html>
