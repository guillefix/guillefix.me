<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.17" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Parallel computing: Cosmos — Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-High-performance%20computing " data-tags="[[High-performance computing]]" data-tiddler-title="Parallel computing"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Parallel computing
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 4th April 2017 at 10:26am
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 High-performance computing
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=8_ywDfr1FGU" rel="noopener noreferrer" target="_blank">Nice video about parallel computing</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=jMfVx4hFHVk" rel="noopener noreferrer" target="_blank">Why we cannot keep increasing CPU speed?</a> Power has emerged as one of the primary factors in processor design.</p><p>Often used in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Computer%2520cluster.html">Computer cluster</a> and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="GPU%2520computing.html">GPU computing</a>. Main application is for <a class="tc-tiddlylink tc-tiddlylink-resolves" href="High-performance%2520computing.html">High-performance computing</a> (see more there)</p><p>Fundamental concept: <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?time_continue=81&amp;v=cQ--7XZs1ew" rel="noopener noreferrer" target="_blank">total time vs total work</a></p><p>We say that a parallel algorithm is <strong>work efficient</strong> if its <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?time_continue=88&amp;v=V8TTrUdfpIY" rel="noopener noreferrer" target="_blank">work complexity</a> is asymptotically the same as the equivalent serial algorithm</p><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Analysis%2520of%2520parallel%2520algorithms.html">Analysis of parallel algorithms</a></h3><hr><h1 class="">Parallel programming</h1><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="CUDA.html">CUDA</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="OpenMP.html">OpenMP</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="MPI.html">MPI</a></li></ul><h3 class=""><u><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=LjWlZHqUG8A" rel="noopener noreferrer" target="_blank">Parallel communication patterns</a></u></h3><p>Tasks &lt;&gt; Memory</p><ul><li>Map. 1-to-1.. 1 thread on 1 part of memory, independently.</li><li>Scatter. 1-to-many. 1 thread, write to a potentially different and potentially more than 1 part of memory, independently.</li><li>Gather. many-to-1. Like scatter but for reading instead of writting.<ul><li>Stencil. Read from a fixed set of neighbours, and write to 1 part of  memory</li></ul></li><li>Transpose.1-to-1.  Any read and any write locations?</li><li>Reduce. all-to-1.</li><li>scan/sort. all-to-all.</li><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?time_continue=10&amp;v=Jo6RnEi6eHE" rel="noopener noreferrer" target="_blank">More methods</a><ul><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?time_continue=9&amp;v=N1eQowSCdlw" rel="noopener noreferrer" target="_blank">Reduce</a> –&gt; <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?time_continue=24&amp;v=prLb1MbAm8M" rel="noopener noreferrer" target="_blank">parallelizing reduce</a> for binary/associative operators. See more at <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Analysis%2520of%2520parallel%2520algorithms.html">Analysis of parallel algorithms</a></li><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=We9j876CjtA" rel="noopener noreferrer" target="_blank">Scan</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?time_continue=11&amp;v=hS_uAPgXpzE" rel="noopener noreferrer" target="_blank">math</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?time_continue=142&amp;v=HfXkXUDlBqI" rel="noopener noreferrer" target="_blank">why do we care about parallel scan</a></li></ul></li></ul><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?time_continue=49&amp;v=8NiigEw_UIE" rel="noopener noreferrer" target="_blank">Thread diveregence</a></p><hr><p>Introduction to parallel programming by nvidia in Udacity: <a class="tc-tiddlylink-external" href="https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/671181630923" rel="noopener noreferrer" target="_blank">https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/671181630923</a></p><hr><h2 class=""><u>Latency vs throughput tradeoff</u></h2><p>Latency: time for a single unit operation to take place</p><p>Throughput: number of operations per second.</p><p>Latency has advanced more slowly than throughput in technologies: <a class="tc-tiddlylink-external" href="http://dl.acm.org.sci-hub.cc/citation.cfm?id=1022596" rel="noopener noreferrer" target="_blank">Latency lags throughput</a></p><h2 class="">Types of parallel computing</h2><ul><li>High-throughput computing, aka embarassingly parallel computing: lots of *independent* tasks.</li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="High-performance%2520computing.html">High-performance computing</a> often refers to a big task divided into many parallel computing nodes, but they are not totally independent, and so issues of communication ened to be addressed.</li></ul><h2 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Memory.html">Memory</a> models</h2><p>distributed and shared memory parallel computing models </p><ul><li><strong>Share memory</strong>: all the cores can see the same memory. <a class="tc-tiddlylink tc-tiddlylink-resolves" href="OpenMP.html">OpenMP</a>. Limited to one node in a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Computer%2520cluster.html">Computer cluster</a></li><li><strong>Distributed memory</strong>: each core has a separate memory they can access. <a class="tc-tiddlylink tc-tiddlylink-resolves" href="MPI.html">MPI</a>. Scales to many many thousdands of cores accross several nodes..</li></ul><p>Often use a combination of both, like <a class="tc-tiddlylink tc-tiddlylink-resolves" href="CUDA.html">CUDA</a></p><hr><p>– Clusters	and	job	managers.
– Jobs	vs Tasks.	
• Creating	and	submitting	them.
• Getting	the	results
– Code	portability.
– Callback	functions
• Advanced	parallelism.
– spmd mode,	message	passing.
– GPU	computing.</p><p><a class="tc-tiddlylink-external" href="https://uk.mathworks.com/help/distcomp/how-parallel-computing-products-run-a-job.html" rel="noopener noreferrer" target="_blank">https://uk.mathworks.com/help/distcomp/how-parallel-computing-products-run-a-job.html</a></p></div>


</div>

</p>

</section>
</body>
</html>
