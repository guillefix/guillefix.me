<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.15" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Language model: Cosmos — All that is, or was, or ever will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Natural%20language%20understanding tc-tagged-Natural%20language%20processing"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Language model
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 16th February 2019 at 1:20pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Natural language processing
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
<span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Natural language understanding
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p>A <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Probabilistic%2520model.html">Probabilistic model</a> for <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Language.html">Language</a>: A <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Probability%2520distribution.html">Probability distribution</a> over strings (sequence of tokens) in a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Language.html">Language</a></p><h2 class=""><u>Uses of conditional/joint probability models</u></h2><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=DVSvz2eaZns&amp;list=PL613dYIGMXoZBtZhbyiBqb0QtgK6oJbpm&amp;index=9" rel="noopener noreferrer" target="_blank">Conditional language models</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Machine%2520translation.html">Machine translation</a></p><p><a class="tc-tiddlylink tc-tiddlylink-missing" href="Question%2520answering.html">Question answering</a></p><p><a class="tc-tiddlylink tc-tiddlylink-missing" href="Dialogue.html">Dialogue</a></p><hr><p>We use the <a class="tc-tiddlylink tc-tiddlylink-missing" href="Chain%2520rule%2520for%2520joint%2520probabilities.html">Chain rule for joint probabilities</a> (exact) to expand the joint prob dist of tokens.</p><p>Our objective function can be the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Cross%2520entropy.html">Cross entropy</a> (a way of measuring how close two prob dists are) relative to the empirically observed frequency.</p><p><strong>Perplexity</strong>: <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mn>2</mn><mrow><mtext><mi mathvariant="normal">c</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">s</mi><mtext> </mtext><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">y</mi></mtext></mrow></msup></mrow><annotation encoding="application/x-tex">2^{\text{cross entropy}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathrm">2</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="text mord scriptstyle uncramped"><span class="mord mathrm">c</span><span class="mord mathrm">r</span><span class="mord mathrm">o</span><span class="mord mathrm">s</span><span class="mord mathrm">s</span><span class="mord mspace"> </span><span class="mord mathrm">e</span><span class="mord mathrm">n</span><span class="mord mathrm">t</span><span class="mord mathrm">r</span><span class="mord mathrm">o</span><span class="mord mathrm">p</span><span class="mord mathrm" style="margin-right:0.01389em;">y</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span></p><hr><p><u>Data</u></p><p>WikiText dataset.</p><hr><h1 class=""><u>Language models</u></h1><h2 class=""><u>n-gram models</u></h2><p>A nth-order <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Markov%2520chain.html">Markov chain</a>, where each word's prob dist depends on the previous n words. </p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Maximum%2520likelihood.html">Maximum likelihood</a> correspond to empirical counts of the form <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><msub><mi>w</mi><mn>3</mn></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo>)</mo><mo>=</mo><mfrac><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo>(</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>3</mn></msub><mo>)</mo></mrow><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo>(</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo>)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">p(w_3|w_1,w_2)=\frac{count(w_1,w_2,w_3)}{count(w_1,w_2)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.01em;"></span><span class="strut bottom" style="height:1.53em;vertical-align:-0.52em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">3</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.34500000000000003em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">u</span><span class="mord mathit">n</span><span class="mord mathit">t</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.485em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">u</span><span class="mord mathit">n</span><span class="mord mathit">t</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathrm">3</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span> for 3-grams.</p><p>Maximum likelihood is a bad objective for language, apparently, need instead to choose a good prior and do <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Maximum%2520a%2520posteriori.html">MAP</a></p><p><u>Smoothing techniques</u></p><p>There are many cases which come out with prob 0. To improve on this, we can use bi-grams to resolve these cases. This is the idea of <strong>back-off</strong>, which is a kind of smoothing technique (c.f. <small><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Laplace%2520smoothing.html">Laplace smoothing</a></small>)</p><p>One very simple way is linear interpolation: the probability of the next word is a convex combination of the probabilities assigned by the one-gram, bi-gram, three-gram etc, probs.</p><p><strong>Kneser-Ney</strong></p><p><em>An empirical study of smoothing techniques for language modeling</em></p><p>We want our posterior distribution to agree with the real distribution in the real world. For insance, we want to recover <a class="tc-tiddlylink tc-tiddlylink-missing" href="Zipf's%2520law.html">Zipf's law</a> (<a class="tc-tiddlylink tc-tiddlylink-resolves" href="Power%2520laws.html">Power laws</a>, long tails..) These long tails is what make good-old-AI rule systems to fail, as there's too much stuff to account for.</p><p>~ constant time algo.</p><p>Long n-grams: data is too sparse –&gt; can't really capture long-term dependencies.</p><p>– N-grams can't capture correlations and other patterns which it hasn't seen, so it's bad at generalizing</p><h2 class=""><u>Neural n-gram language models</u></h2><p>These will be able to better capture correlations, and <u>generalize</u>.., by better capturing semantics, stored in the hidden layers..</p><p>Use <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Artificial%2520neural%2520network.html">Artificial neural network</a> to model the n-gram prob dist.: input being n previous words, output prob dist. of next word.</p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Recurrent%2520neural%2520network.html">Recurrent neural network</a>s</u></h2><p>And also <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Augmented%2520RNN.html">Augmented RNN</a>.</p><p>Want to have memory going all the way back..</p><p>The models are harder to paralelize than the neural n-gram models, because there's depenednce between the network acting at dfifferent points in the sequence.</p><p><u>Truncated <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Backpropagation.html">Backpropagation</a> through time</u></p><p>Add breaks in the backpropagation, maybe between sentences.</p><p>However, we do forward propagate!</p><p><u>Skip-thought vectors</u></p><h2 class=""><u>Attention-based models</u></h2><p>See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="New%2520advances%2520in%2520deep%2520learning.html">New advances in deep learning</a> (BERT, Transformer, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Attention.html">Attention</a></p><p><a class="tc-tiddlylink-external" href="https://blog.openai.com/better-language-models/#task6" rel="noopener noreferrer" target="_blank">https://blog.openai.com/better-language-models/#task6</a></p></div>


</div>

</p>

</section>
</body>
</html>
