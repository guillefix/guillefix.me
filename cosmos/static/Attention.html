<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.17" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Attention: Cosmos â€” Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Intelligence " data-tags="Intelligence" data-tiddler-title="Attention"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="tiddlymap" class="tc-btn-invisible tc-btn-%24%3A%2Fplugins%2Ffelixhayashi%2Ftiddlymap%2Fmisc%2FquickConnectButton " title="Toggle TiddlyMap actions">


</button></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Attention
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 12th February 2017 at 10:49am
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Intelligence
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Attention%2520in%2520machine%2520learning.html">Attention in machine learning</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=iyaktBk8AjU" rel="noopener noreferrer" target="_blank">The frontal and parietal cortex: Eye movements and attention</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Predictive%2520coding.html">Predictive coding</a> is related to attention</p><p><a class="tc-tiddlylink-external" href="https://www.ncbi.nlm.nih.gov/pubmed/19186161" rel="noopener noreferrer" target="_blank">The normalization model of attention</a>. Model proposes attention is mostly accomplished by multiplying input by an <strong>attention field</strong>. Furthermore, the propose  a model of attention that incorporates <strong>divisive normalization</strong> (code on paper)</p><dl><dd>Some results are consistent with the appealingly simple proposal that attention increases neuronal responses multiplicatively by applying a fixed response gain factor (McAdams and Maunsell, 1999; Treue and Martinez-Trujillo, 1999), while others are more in keeping with a change in contrast gain (Li and Basso, 2008, Martinez-Trujillo and Treue, 2002; Reynolds et at., 2000) or with effects that are intermediate between response gain and contrast gain changes (Williford and Maunsell, 2006)</dd></dl><p><small>We propose that this computational principle endows the brain with the capacity to increase sensitivity to faint stimuli presented alone and to reduce the impact of task irrelevant distracters when multiple stimuli are presented. </small></p><p>The three basic components of the model are: the stimulation field, the suppressive field, and the attention field</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=nA5LVjAqkt8" rel="noopener noreferrer" target="_blank">https://www.youtube.com/watch?v=nA5LVjAqkt8</a></p></div>


</div>

</p>

</section>
</body>
</html>
