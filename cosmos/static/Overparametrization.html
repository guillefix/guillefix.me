<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.21" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Overparametrization: Cosmos â€” All that is, or was, or ever will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">
<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists " data-tags="" data-tiddler-title="Overparametrization"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Overparametrization
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 5th July 2019 at 12:52am
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"></div>
</div>

<div class="tc-tiddler-body tc-reveal"><hr><p>See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Recent%2520developments%2520in%2520overparametrized%2520neural%2520networks.html">Recent developments in overparametrized neural networks</a></p><p>See here: <a class="tc-tiddlylink-external" href="https://youtu.be/uC2IGoTE2u4?t=1435" rel="noopener noreferrer" target="_blank">Recent Developments in Over-parametrized Neural Networks, Part I</a></p><p><a class="tc-tiddlylink-external" href="https://youtu.be/uC2IGoTE2u4?t=2518" rel="noopener noreferrer" target="_blank">https://youtu.be/uC2IGoTE2u4?t=2518</a> - should say Hessian negative?
YouTube
Recent Developments in Over-parametrized Neural Networks, Part I
Jason Lee (University of Southern California) <a class="tc-tiddlylink-external" href="https://simons.berkeley.edu/talks/optimizations-i" rel="noopener noreferrer" target="_blank">https://simons.berkeley.edu/talks/optimizations-i</a> Deep Learning Boot Camp</p><p><a class="tc-tiddlylink-external" href="https://youtu.be/uC2IGoTE2u4?t=3231" rel="noopener noreferrer" target="_blank">https://youtu.be/uC2IGoTE2u4?t=3231</a> - pero no siempre no? Si haces como yo por ejemplo que ejecuto SGD hasta que el classification error es 0, no estas necesariamente en un critical point del diffenertiable loss que uses; y el set de parametros con training classification error 0 no es measure 0 .
Also tampoco dicen que SGD returnee critical points sino epsilon critical, aunq tampoco es suficiente que el Jacobian sea full rank, sino que su minimo eigenvalue sea suficientemente grande i guess.</p><p>hmm, pero actually los puntos que puede encontrar SGD estan en un set de measure 0 xq utiliza un numero finito de random bits... La inicialization no, pero como demonstrar que no  converge a un numero finito de puntos posibles aun asi...</p><p><a class="tc-tiddlylink-external" href="https://youtu.be/uC2IGoTE2u4?t=2280" rel="noopener noreferrer" target="_blank">SGD can scape second order saddle point</a>. But not higher order ones apparently. <mark>Why?</mark> <em>why doesn't SGD find higher order descent directions, by the same argument it finds second order ones?</em></p><p><a class="tc-tiddlylink-external" href="https://youtu.be/uC2IGoTE2u4?t=2624" rel="noopener noreferrer" target="_blank">One hidden layer with quadratic activation functions has no local minima of degree higher than two, so SGD can't get stuck</a></p><p><a class="tc-tiddlylink-external" href="https://youtu.be/uC2IGoTE2u4?t=2938" rel="noopener noreferrer" target="_blank">functional gradient descent and optimality conditions of nonlinear least squares</a>. If the Jacobian from parameters to predictions (on training set) is non-degenerate at a local optimum, then it is a global optimum! Sugoi!</p><p>Almost all points (and apparently almost all critical points too?) have a non-degenerate Jacobian!</p><p>no spurious local minima (can scape any critical point with a second order descent direction, which SGD can apparently find), but only for analytic activation functions, like quadratic, monomial etc.</p><p><a class="tc-tiddlylink-external" href="https://youtu.be/uC2IGoTE2u4?t=3945" rel="noopener noreferrer" target="_blank">connections to Frank-Wolfe</a></p><p>arxiv.org/abs/1605.08361 arxiv.org/abs/1710.10928 arxiv.org/abs/1803.02968</p></div>



</div>

</p>
</section>
</body>
</html>
