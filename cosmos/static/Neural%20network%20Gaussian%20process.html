<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.15" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Neural network Gaussian process: Cosmos — Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Gaussian%20process"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Neural network Gaussian process
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 30th November 2018 at 1:30am
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Gaussian process
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Gaussian%2520process.html">Gaussian process</a> that approximates the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Prior.html">Prior</a> over functions in a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Bayesian%2520neural%2520network.html">Bayesian neural network</a> (<a class="tc-tiddlylink tc-tiddlylink-missing" href="Bayesian%2520deep%2520learning.html">Bayesian deep learning</a>). The approximation is exact in the limit of infinitely wide layers (infinitely many neurons per hidden layer), and under the assumption that the distribution over weights in the Bayesian neural network is i.i.d. (and often <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Gaussian.html">Gaussian</a>)</p><p>It may also approximate the prior over functions when training with <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Stochastic%2520gradient%2520descent.html">SGD</a>.</p><p>The kernel function of the Gaussian process depends on the choice of architecture, and properties of the parameter distribution, in particular the weight variance <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>σ</mi><mi>w</mi><mn>2</mn></msubsup><mi mathvariant="normal">/</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">\sigma_w^2/n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="vlist"><span style="top:0.247em;margin-left:-0.03588em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">/</span><span class="mord mathit">n</span></span></span></span></span> (where <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span></span></span></span></span> is the size of the input to the layer) and the bias variance <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\sigma_b^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.097216em;vertical-align:-0.2831079999999999em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="vlist"><span style="top:0.2831079999999999em;margin-left:-0.03588em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">b</span></span></span><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span>. The kernel for fully connected ReLU networks has a well known analytical form known as the arccosine kernel <a class="tc-tiddlylink-external" href="http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning" rel="noopener noreferrer" target="_blank">[ref</a>], while for convolutional and residual networks it can be efficiently computed (see e.g. <a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1808.05587.pdf" rel="noopener noreferrer" target="_blank">here</a>) </p><p>–&gt; For a derivation and more detailed introduction of the main results (for <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Fully%2520connected%2520network.html">Fully connected network</a>s) </p><p>For an even more rigorous.careful treatment, and further results see: <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1711.00165" rel="noopener noreferrer" target="_blank">Deep Neural Networks as Gaussian Processes</a>, <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1804.11271" rel="noopener noreferrer" target="_blank">Gaussian Process Behaviour in Wide Deep Neural Networks</a></p><h3 class=""><u>Convolutional neural network Gaussian processes</u></h3><p>It turns out that <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Convolutional%2520neural%2520network.html">Convolutional neural network</a>s (CNNs) with infinitely many filters per layer are also Gaussian processes, with a different, and more complex, kernel function (so that in effect CNNs &quot;see&quot; different inputs as being &quot;similar&quot; to each other than fully connected nets).</p><p><a class="tc-tiddlylink-external" href="https://openreview.net/forum?id=Bklfsi0cKm" rel="noopener noreferrer" target="_blank">Deep Convolutional Networks as shallow Gaussian Processes</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1810.05148.pdf" rel="noopener noreferrer" target="_blank">BAYESIAN CONVOLUTIONAL NEURAL NETWORKS WITH MANY CHANNELS ARE GAUSSIAN PROCESSES</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1810.10798" rel="noopener noreferrer" target="_blank">A Gaussian Process perspective on Convolutional Neural Networks</a></p><h3 class=""><u>Applications to <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Generalization.html">Generalization</a></u></h3><p><a class="tc-tiddlylink-external" href="https://openreview.net/forum?id=rye4g3AqFm" rel="noopener noreferrer" target="_blank">Deep learning generalizes because the parameter-function map is biased towards simple functions</a></p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Mean%2520field%2520theory%2520of%2520neural%2520networks.html">Mean field theory of neural networks</a></u></h2><p>These papers study in more detail the properties of the kernel of neural networks, and even explore some ideas related to robustness of the outputs of the neural network to changes of the weight (see SI of this paper: <a class="tc-tiddlylink-external" href="http://papers.nips.cc/paper/6322-exponential-expressivity-in-deep-neural-networks-through-transient-chaos" rel="noopener noreferrer" target="_blank">Exponential expressivity in deep neural networks through transient chaos</a> in particular). &lt;– Here is an <span style="color:#13BAAD">idea</span>: Given their analysis of the effect of a change on weights <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span></span>, <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Δ</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">\Delta w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">Δ</span><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span></span>, on the corresponding change on the outputs of the network <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Δ</mi><mi>f</mi><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mi>w</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\Delta f(x,w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">Δ</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span></span>, for some given input point <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span></span>, one can perhaps find a formula for the Hessian of the loss (which is a sum of functions of <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo separator="true">;</mo><mi>w</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(x;w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mpunct">;</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span></span> over a set of <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span></span> corresponding to the training set). If we are lucky, it may be possible to relate that formula with the formula for <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>f</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(f)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mclose">)</span></span></span></span></span> given by the Gaussian process analysis (which is rather similar in nature, as the calculation of the kernel is what they explore in that paper, in terms of how the correlation between two points changes as you propagate through layers, see comments in <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1711.00165" rel="noopener noreferrer" target="_blank">Deep Neural Networks as Gaussian Processes</a> and <a class="tc-tiddlylink-external" href="http://papers.nips.cc/paper/6322-exponential-expressivity-in-deep-neural-networks-through-transient-chaos" rel="noopener noreferrer" target="_blank">the paper itself</a> for this to make more sense!)</p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1710.06570.pdf" rel="noopener noreferrer" target="_blank">A Correspondence Between Random Neural Networks and Statistical Field Theory</a></p><h3 class=""><u><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1611.01232" rel="noopener noreferrer" target="_blank">Deep Information Propagation</a></u></h3><p><a class="tc-tiddlylink-external" href="http://papers.nips.cc/paper/6322-exponential-expressivity-in-deep-neural-networks-through-transient-chaos" rel="noopener noreferrer" target="_blank">Exponential expressivity in deep neural networks through transient chaos</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1711.04735" rel="noopener noreferrer" target="_blank">Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice</a></p><p>See more at <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Statistical%2520mechanics%2520of%2520neural%2520networks.html">Statistical mechanics of neural networks</a></p><hr><p>&quot;The results indicate that on this dataset (Delf yatch hydrodynamics dataset) the Bayesian deep network and theGaussian process do not make similar predictions.  Of the two, the Bayesian neural networkachieves  signi cantly  better  log  likelihoods  on  average,  indicating  that  a   nite  networkperforms better than its in nite analogue in this case.&quot;
(<a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1804.11271.pdf" rel="noopener noreferrer" target="_blank">https://arxiv.org/pdf/1804.11271.pdf</a>)</p><p><img src="" width="500"></p></div>


</div>

</p>

</section>
</body>
</html>
