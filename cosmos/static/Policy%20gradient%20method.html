<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.21" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Policy gradient method: Cosmos — All that is, or was, or ever will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">
<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists  tc-tagged-Reinforcement%20learning" data-tags="[[Reinforcement learning]]" data-tiddler-title="Policy gradient method"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Policy gradient method
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 9th November 2019 at 3:41pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">
<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Reinforcement learning
</span>
<span class="tc-drop-down tc-reveal" hidden="true"></span></span></div>
</div>

<div class="tc-tiddler-body tc-reveal"><p>A class of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Reinforcement%2520learning.html">Reinforcement learning</a> algorithms. <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=9m" rel="noopener noreferrer" target="_blank">These are also known as direct search algorithms</a> or Policy search, in contrast with algorithms where our aim is to find the optimal value function.</p><p>They directly optimize the <a class="tc-tiddlylink tc-tiddlylink-missing" href="Policy%2520function.html">Policy function</a> to maximize expected reward. If the expected reward can be computed exactly, this is typically an instance of <a class="tc-tiddlylink tc-tiddlylink-missing" href="Model-based%2520control.html">Model-based control</a>. If the environment is unknown, or can't be integrated over, then we may approximate the expected reward with a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Monte%2520Carlo.html">Monte Carlo</a> estimate (sum over samples). But this alone doesn't let us calculate the gradients! W need a Monte Carlo estimate of the gradients themselves. This isn't as easy as in supervised learning (where the cost is a sum over i.i.d. examples) because the distribution of states depends on the policy itself. The solution to this problem is the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Policy%2520gradient%2520theorem.html">Policy gradient theorem</a></p><p>With Monte Carlo estimates of the gradient of the expected reward, we can then use an <a class="tc-tiddlylink tc-tiddlylink-missing" href="Stochastic%2520optimization.html">Stochastic optimization</a> algorithm like <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Stochastic%2520gradient%2520descent.html">Stochastic gradient descent</a> (when we parametrize the policy in a way such that the gradients with respect to the parameters exist).</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=6m25s" rel="noopener noreferrer" target="_blank">intro vid</a> – 
<a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=7m35s" rel="noopener noreferrer" target="_blank">General aim</a> – 
<strong>Stochastic policy</strong> <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=11m23s" rel="noopener noreferrer" target="_blank">Definition</a></p><p><u>Classes of policy gradient algorithms</u></p><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="REINFORCE%2520algorithm.html">REINFORCE algorithm</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Actor-critic%2520method.html">Actor-critic method</a><ul><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="AC3.html">AC3</a></li></ul></li><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="Proximal%2520policy%2520optimization.html">Proximal policy optimization</a></li><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="Trust%2520region%2520policy%2520optimization.html">Trust region policy optimization</a></li></ul><h3 class=""><u>REINFORCE</u></h3><p>Sometimes called the <strong>reinforce algorithm</strong>, and is a form of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Stochastic%2520gradient%2520descent.html">Stochastic gradient descent</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=23m" rel="noopener noreferrer" target="_blank">Goal</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=27m" rel="noopener noreferrer" target="_blank">Algorithm</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=28m45s" rel="noopener noreferrer" target="_blank">explanation</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=33m" rel="noopener noreferrer" target="_blank">Derivation</a>, using the <a class="tc-tiddlylink tc-tiddlylink-missing" href="Product%2520rule.html">Product rule</a></p><ol><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=36m05s" rel="noopener noreferrer" target="_blank">Differentiation</a></li><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=38m" rel="noopener noreferrer" target="_blank">Factor out joint probability from terms in sum</a> </li><li>Rewrite as expectation –&gt; <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=40m05s" rel="noopener noreferrer" target="_blank">On expectation, reinforce algorithm updates parameters in the direction of the gradient of the expected payout</a>. This shows the algorithm is an <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Stochastic%2520gradient%2520descent.html">Stochastic gradient descent</a> algorithm!</li></ol><p>With direct policy search, <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=kUiR0RLmGCo&amp;index=15&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw#t=40m50s" rel="noopener noreferrer" target="_blank">rewards may be combined in other ways other than by summing them</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=kUiR0RLmGCo&amp;index=15&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw#t=42m43s" rel="noopener noreferrer" target="_blank">Derivation by Nando</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=kUiR0RLmGCo&amp;index=15&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw#t=46m50" rel="noopener noreferrer" target="_blank">comment on reward function not being really needed</a> –&gt; <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=kUiR0RLmGCo&amp;index=15&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw#t=49m05s" rel="noopener noreferrer" target="_blank">result</a></p><p>What we use for the gradient descent is do a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Monte%2520Carlo.html">Monte Carlo</a> estimate, which makes it stochastic.</p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Actor-critic%2520method.html">Actor-critic method</a></u></h2><h2 class=""><u>Policy optimization</u></h2><p><a class="tc-tiddlylink tc-tiddlylink-missing" href="Proximal%2520policy%2520optimization.html">Proximal policy optimization</a></p><p><a class="tc-tiddlylink tc-tiddlylink-missing" href="Trust%2520region%2520policy%2520optimization.html">Trust region policy optimization</a></p><h2 class=""><u>Pegasus</u></h2><p>-—&gt;<a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=48m" rel="noopener noreferrer" target="_blank">vid</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=kUiR0RLmGCo&amp;index=15&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw#t=52m" rel="noopener noreferrer" target="_blank">Nando's vid</a></p><h2 class=""><u>Direct policy gradient methods</u></h2><h3 class=""><u>Deterministic Policy Gradient Algorithms</u></h3><p><a class="tc-tiddlylink-external" href="http://jmlr.org/proceedings/papers/v32/silver14.pdf" rel="noopener noreferrer" target="_blank">paper</a></p><h3 class=""><u>Natural policy gradient</u></h3><h2 class=""><u>Other variations</u></h2><p>Can approach it as an <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Inference.html">Inference</a> problem, or in other ways. See <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=dV80NAlEins&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;index=16#t=1m47" rel="noopener noreferrer" target="_blank">comment</a></p><h3 class=""><u>pair-wise policy comparisons</u></h3><h3 class=""><u>probabilistic policy search approaches</u></h3><p><u>based on EM </u></p><p><u>based on probabilistic modeling </u></p><h3 class=""><u> Relative Entropy Policy Search </u></h3><p><a class="tc-tiddlylink-external" href="https://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/2012/AISTATS-2012-Daniel.pdf" rel="noopener noreferrer" target="_blank">Hierarchical Relative Entropy Policy Search</a> – <a class="tc-tiddlylink-external" href="http://jmlr.org/papers/volume17/15-188/15-188.pdf" rel="noopener noreferrer" target="_blank">extended version</a></p></div>



</div>

</p>
</section>
</body>
</html>
