<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.21" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Model-based reinforcement learning: Cosmos — All that is, or was, or ever will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">
<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists  tc-tagged-Reinforcement%20learning" data-tags="[[Reinforcement learning]]" data-tiddler-title="Model-based reinforcement learning"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Model-based reinforcement learning
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 15th May 2019 at 2:22am
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">
<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Reinforcement learning
</span>
<span class="tc-drop-down tc-reveal" hidden="true"></span></span></div>
</div>

<div class="tc-tiddler-body tc-reveal"><p><em>aka <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Planning.html">Planning</a></em></p><p>Solving the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Bellman%2520equation.html">Bellman equation</a>s</p><p><a class="tc-tiddlylink-external" href="https://worldmodels.github.io" rel="noopener noreferrer" target="_blank">https://worldmodels.github.io</a></p><p><a class="tc-tiddlylink-external" href="https://twitter.com/wgussml/status/1126984030090596354" rel="noopener noreferrer" target="_blank">https://twitter.com/wgussml/status/1126984030090596354</a> </p><p>World models for atari: <a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1903.00374.pdf" rel="noopener noreferrer" target="_blank">https://arxiv.org/pdf/1903.00374.pdf</a> . they train the world model on trajectories that the agent actually explores, rather than random ones. And they iterate several times</p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Linear%2520programming.html">Linear programming</a></u></h2><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Dynamic%2520programming.html">Dynamic programming</a></u></h2><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=dV80NAlEins&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;index=16#t=13m05" rel="noopener noreferrer" target="_blank">idea</a>
 – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=RtxI449ZjSc&amp;list=PLA89DCFA6ADACE599&amp;index=16#t=1h01m45s" rel="noopener noreferrer" target="_blank">Tradeoffs</a>. The idea is to solve consistency equations (derived by a <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=3#t=1h4m25s" rel="noopener noreferrer" target="_blank">look ahead tree</a> and <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=3#t=1h1m48s" rel="noopener noreferrer" target="_blank">principle of optimality</a>) iteratively (see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Fixed-point%2520iteration.html">Fixed-point iteration</a>). – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=3#t=1h23m55s" rel="noopener noreferrer" target="_blank">Summary of methods</a></p><h3 class=""><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=dV80NAlEins&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;index=16#t=3m" rel="noopener noreferrer" target="_blank">Neuro-dynamic programming</a></h3><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Policy%2520iteration.html">Policy iteration</a></h3><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Value%2520iteration.html">Value iteration</a></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=3#t=1h29m" rel="noopener noreferrer" target="_blank">Extensions to dynamic programming</a>:</p><ul><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=3#t=1h29m35s" rel="noopener noreferrer" target="_blank">Asynchronous dynamic programming (DP)</a><ul><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=3#t=1h30m52s" rel="noopener noreferrer" target="_blank">In-place DP</a></li><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=3#t=1h33m30s" rel="noopener noreferrer" target="_blank">Prioritized sweeping</a></li><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=3#t=1h35m38s" rel="noopener noreferrer" target="_blank">Real-time dynamic programming</a></li></ul></li></ul><p>There are other algorithms described in the <a class="tc-tiddlylink-external" href="https://www.wikiwand.com/en/Reinforcement_learning" rel="noopener noreferrer" target="_blank">Wiki page</a>:</p><ul><li>Trust Region Policy Optimization [1]</li><li>Proximal Policy Optimization (i.e., TRPO, but using a penalty instead of a constraint on KL divergence), where each subproblem is solved with either SGD or L-BFGS</li><li>Cross Entropy Method</li></ul><table><tbody><tr class="evenRow"><td><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=3#t=1h36m30s" rel="noopener noreferrer" target="_blank">final comment on DP methods</a>, DP uses full-width look ahead, plus it assumes we know dynamics. Instead we can <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Monte%2520Carlo.html">sample</a>) –&gt; leads to <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Model-free%2520reinforcement%2520learning.html">Model-free reinforcement learning</a></td></tr></tbody></table><h3 class=""><u>Asynchronous DP</u></h3><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Real-time%2520dynamic%2520programming.html">Real-time dynamic programming</a> (RTDP), which uses <a class="tc-tiddlylink tc-tiddlylink-resolves" href="On-policy%2520trajectory%2520sampling.html">On-policy trajectory sampling</a></p><hr><h2 class=""><u>Combining <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Model-free%2520reinforcement%2520learning.html">model-free</a> with model-based RL</u></h2><p><a class="tc-tiddlylink-external" href="https://deepmind.com/blog/agents-imagine-and-plan/" rel="noopener noreferrer" target="_blank">https://deepmind.com/blog/agents-imagine-and-plan/</a> – <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1707.06170" rel="noopener noreferrer" target="_blank">Learning model-based planning from scratch</a></p><p>Using <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Generative%2520model.html">Generative model</a>s, and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Environment%2520model.html">Environment model</a>s</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Empowerment.html">Empowerment</a></p></div>



</div>

</p>
</section>
</body>
</html>
