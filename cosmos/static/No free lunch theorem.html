<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.17" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>No free lunch theorem: Cosmos — Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Learning%20theory " data-tags="[[Learning theory]]" data-tiddler-title="No free lunch theorem"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
No free lunch theorem
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 4th October 2018 at 5:33pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Learning theory
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=KlKe44dIYqU&amp;index=6&amp;list=PLFze15KrfxbH8SE4FgOHpMSY1h5HiRLMm#t=52m" rel="noopener noreferrer" target="_blank">video</a></p><h3 class=""><a class="tc-tiddlylink-external" href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=1A58309E5BB673000B0A0AA9E69BD559?doi=10.1.1.49.1549&amp;rep=rep1&amp;type=pdf" rel="noopener noreferrer" target="_blank">Wolpert article on No Free Lunch and the different learning theory frameworks</a></h3><p>Much of machine learning is concerned with devising different models, and different algorithms
to fit them. We can use methods such as cross validation to empirically choose the best method
for our particular problem. However, there is no universally best model — this is sometimes
called the no free lunch theorem (Wolpert 1996). The reason for this is that a set of assumptions
that works well in one domain may work poorly in another.</p><p><a class="tc-tiddlylink-external" href="http://dml.cs.byu.edu/~cgc/docs/mldm_tools/Reading/LCG.pdf" rel="noopener noreferrer" target="_blank">A conservation law for generalization performance.</a></p><p><a class="tc-tiddlylink-external" href="https://link.springer.com/chapter/10.1007%2F978-1-4471-0123-9_3" rel="noopener noreferrer" target="_blank">The supervised learning no-free-lunch theorems.</a></p><p><a class="tc-tiddlylink-external" href="http://ieeexplore.ieee.org/document/585893/" rel="noopener noreferrer" target="_blank">No free lunch theorems for optimization.</a></p><p><a class="tc-tiddlylink-external" href="https://books.google.co.uk/books?id=6GdQDwAAQBAJ&amp;pg=PA139&amp;lpg=PA139&amp;dq=pac+and+the+statistical+physics+framework+bayesian+and+vc+frameworks&amp;source=bl&amp;ots=H2ZrlVhGko&amp;sig=MZxydUEmFpt7YJYVErXp9xteKPM&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjOifKFmpTaAhWGD8AKHV56CIkQ6AEIQzAF#v=onepage&amp;q&amp;f=false" rel="noopener noreferrer" target="_blank">The mathematics of generalization</a></p><p><a class="tc-tiddlylink-external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.49.1549" rel="noopener noreferrer" target="_blank">The Relationship between PAC, the Statistical Physics framework, the Bayesian framework, and the VC framework (1994)</a></p><hr><p>Given data, most probably test error is not indicative of off-given-data error.
Given real function, most probably test error is indicative.</p><p>Cross-validation doesnt work because on-training error doesn't tell you anything about off-training-set error</p><p>symmetry between hypothesis class and class of possible real functions.</p><p>if realizability assumption is not satisfied then pac theorem doesnt work</p><p>(1-e)^m/(H/F +(F-H)/F*(1-e)^m)</p><p>for algo that can only produce 1 function.</p><p>pac Bayes needs matching P(f)?</p><p>draw.io things</p><hr><p>–&gt; <a class="tc-tiddlylink-external" href="https://www.researchgate.net/profile/Marcus_Hutter/publication/51956303_No_Free_Lunch_versus_Occam%27s_Razor_in_Supervised_Learning/links/0fcfd5101263db7be3000000/No-Free-Lunch-versus-Occams-Razor-in-Supervised-Learning.pdf" rel="noopener noreferrer" target="_blank">No Free Lunch versus Occam’s Razor in Supervised Learning</a></p><p>Basically it's a proof (Theorems 14 and 15) giving a generalization bound similar to <a class="tc-tiddlylink tc-tiddlylink-missing" href="Occam's%2520theorem.html">Occam's theorem</a>s in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Probably%2520approximately%2520correct.html">Probably approximately correct</a> theory; but instead of being based on probability, it gives a bound for a particular training set, and complexity of a target function. Intuitively, this is possible. For if given the target function complexity (or a bound on it), for a particular Turing machine (to make K complexity concrete), then we can look at all functions which agree with this complexity bound and see the generalization of each (given the output of the algorithm, which only depends on the training data). We can then see if all the corresponding generalization errors are below some bound. I am just saying this to show that the information which the theorem assumes could be enough in principle to construct a generalization bound. Now the theorem uses several properties of Kolmogorov complexity and entropy to obtain a bound.</p><p>The idea is the following: given the target function and the function outputted by the algorithm, we immediately know all the elements in the domain <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span></span></span> on which the functions differ. We assume that the algorithm gives a function which agrees perfectly on the training set.</p><p>To give a description of the training set (which elements are in the training set), then we just need to add a description of {which of {the elements in the domain, for which the function agrees with the target} are in the domain and which are not}. Intuitively, this information (call it <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>I</mi></mrow><annotation encoding="application/x-tex">I</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">I</span></span></span></span></span>) will be less than describing which elements are in the domain and which are not, over the set of <em>all</em> elements. They prove this. However, because the functions plus this is a description of the training set then, {information needed to describe the functions} + <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>I</mi></mrow><annotation encoding="application/x-tex">I</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">I</span></span></span></span></span> <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>≥</mo></mrow><annotation encoding="application/x-tex">\geq</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.63597em;"></span><span class="strut bottom" style="height:0.7719400000000001em;vertical-align:-0.13597em;"></span><span class="base textstyle uncramped"><span class="mrel">≥</span></span></span></span></span> <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span></span>(training set). Furthermore <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>I</mi><mo>≤</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">I \leq K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.8193em;vertical-align:-0.13597em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mrel">≤</span><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span></span>, as we intuited above. Now, <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>I</mi></mrow><annotation encoding="application/x-tex">I</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">I</span></span></span></span></span> and <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span></span>(training set) both turn out to scale with <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span></span></span></span></span> (the number of elements in the domain), if we assume the training set is a fixed fraction <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span></span> of the elements of the domain. If {information needed to describe the functions} scales more slowly, then asymptotically, we have <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>I</mi><mo>≥</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">I \geq K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.8193em;vertical-align:-0.13597em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mrel">≥</span><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span></span>(training set) and <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>I</mi><mo>≤</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">I \leq K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.8193em;vertical-align:-0.13597em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mrel">≤</span><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span></span>(training set), so asymptotically, <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>I</mi><mo>=</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">I = K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span></span>(training set). From the intuitive argument above (made rigorous in the paper), this can only be if the set of {the elements in the domain, for which the function agrees with the target} becomes the whole domain, asymptotically. That is the function agrees with the target over the whole domain (and in particular over the whole test set, which is a constant fraction <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>−</mo><mi>θ</mi></mrow><annotation encoding="application/x-tex">1-\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span></span> of the whole domain).</p><p><a class="tc-tiddlylink-external" href="http://www2.math.su.se/gemensamt/grund/exjobb/matte/2013/rep3/report.pdf" rel="noopener noreferrer" target="_blank">Universal Indution and Optimisation: No Free Lunh</a></p><hr><blockquote><p>All models are wrong, but some models are useful. — George Box (Box and Draper 1987,</p></blockquote><p>p424).12</p><hr><p>Just like I blame the 2nd law for my room being messy, now I blame the No Free Lunch theorem whenever I'm not good at something: &quot;No Free Lunch! One can't be good at everything!&quot; &quot;I'm just good at other things&quot;, &quot;in average we are just as good&quot;, etc</p><ol><li>mathematicallyjustifiedexcuses</li></ol></div>


</div>

</p>

</section>
</body>
</html>
