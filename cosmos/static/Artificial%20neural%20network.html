<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.15" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Artificial neural network: Cosmos — Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Neuronal%20network tc-tagged-Artificial%20intelligence"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Artificial neural network
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 2nd November 2018 at 8:26pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Artificial intelligence
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
<span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Neuronal network
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p><small>Aka artificial neural network..</small></p><p>A particularly useful way of representing nonlinear functions, for problems in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Machine%2520learning.html">Machine learning</a>. It is a very good model for many problems, and learning algorithms produce very good results with them. In particular <u>deep learning</u> (which uses ANNs with many layers). It is a nonlinear <a class="tc-tiddlylink tc-tiddlylink-missing" href="Classifier.html">classifier</a>, and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Regression%2520analysis.html">Regression analysis</a> model.</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=aircAruvnKk" rel="noopener noreferrer" target="_blank">But what *is* a Neural Network? -- Deep learning, Part 1</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=apPiZd-qnZ8&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=4" rel="noopener noreferrer" target="_blank">Hugo Larochelle class videos</a> (<a class="tc-tiddlylink-external" href="http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html" rel="noopener noreferrer" target="_blank">website</a>). <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=qyyJKd-zXRE&amp;index=6&amp;list=PLA89DCFA6ADACE599#t=26m" rel="noopener noreferrer" target="_blank">Andrew Ng intro</a>. <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=qyyJKd-zXRE&amp;index=6&amp;list=PLA89DCFA6ADACE599#t=29m" rel="noopener noreferrer" target="_blank">NN</a>. Learning parameters in a NN is generally a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Convex%2520optimization.html">non-convex optimization problem</a>, which makes it very hard to reach global optima. – <a class="tc-tiddlylink-external" href="https://archive.org/details/NeuralNetworks_201810/page/n11" rel="noopener noreferrer" target="_blank">book</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=3JQ3hYko51Y" rel="noopener noreferrer" target="_blank">nice visualization</a></p><h2 class=""><u>Definition</u></h2><p>Neuron has:</p><p>1) <strong>inputs</strong></p><p>2) <strong>weight vectors</strong>, that multiplies the input vector or activation vector of hidden layers.</p><p>3) <strong>bias</strong>, that is added to result</p><p>4) <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Activation%2520function.html">Activation function</a> takes as argument the result of the above (called pre-activation or input activation)</p><p>5) The result (called <strong>activation</strong>) may be the input of other neurons in the next <strong>layer</strong>, in a <strong>multilayer feedforward neural network</strong>.</p><p>6) The activation of the last layer, is the <strong>output</strong></p><p>Overall... we are multiplying by matrices and applying simple nonlinear function</p><h3 class="">See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Neural%2520network%2520theory.html">Neural network theory</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Mathematical%2520modelling%2520of%2520neural%2520networks.html">Mathematical modelling of neural networks</a>, for more on the theory</h3><h2 class=""><u>Learning by <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Optimization.html">optimization</a></u></h2><p>Learning by minimizing cost function (see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Learning%2520theory.html">Learning theory</a>)</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Bver7Ttgb9M&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=17" rel="noopener noreferrer" target="_blank">Training neural networks - optimization</a>. There are several global optima, and plateaus. Uses <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Gradient%2520descent.html">Gradient descent</a>, in particular <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Stochastic%2520gradient%2520descent.html">SGD</a>.</p><p>An efficient algorithm to compute the gradients of the loss function for (SGD) w.r.t. the ANN's parameters is <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Backpropagation.html">Backpropagation</a>.</p><p>see more at <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Learning%2520theory.html">Learning theory</a></p><h3 class=""><u>Parameter initialization for NNs</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sLfogkzFNfc&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=15" rel="noopener noreferrer" target="_blank">Neural networks [2.9] : Training neural networks - parameter initialization</a></p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Model%2520selection.html">Model selection</a> for neural networks</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Fs-raHUnF2M&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=16" rel="noopener noreferrer" target="_blank">Neural networks [2.10] : Training neural networks - model selection</a>. How to set the hyperparameters. Can use <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Cross-validation.html">Cross-validation</a>.</p><h2 class=""><u>Types of neural networks</u></h2><p><a class="tc-tiddlylink-external" href="http://www.asimovinstitute.org/neural-network-zoo/" rel="noopener noreferrer" target="_blank">NN Zoo</a></p><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Feedforward%2520neural%2520network.html">Feedforward neural network</a> (basic)</li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Convolutional%2520neural%2520network.html">Convolutional neural network</a>. Good for image recognition for e.g.</li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Recurrent%2520neural%2520network.html">Recurrent</a>. Good for sequences in time</li><li>Long-Short term memory NN.</li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Spiking%2520neural%2520network.html">Spiking neural network</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Residual%2520neural%2520network.html">Residual neural network</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Boltzmann%2520machine.html">Boltzmann machine</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Autoencoder.html">Autoencoder</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Deep%2520belief%2520network.html">Deep belief network</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Generative%2520adversarial%2520network.html">Generative adversarial network</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Hopfield%2520network.html">Hopfield network</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Binarized%2520neural%2520network.html">Binarized neural network</a></li><li><a class="tc-tiddlylink-external" href="http://www.ntu.edu.sg/home/egbhuang/" rel="noopener noreferrer" target="_blank">Extreme learning machine</a></li></ul><p>Many models in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Machine%2520learning.html">Machine learning</a> can be seen as neural networks</p><p><img src="http://www.asimovinstitute.org/wp-content/uploads/2016/09/neuralnetworks.png"></p><hr><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=qyyJKd-zXRE&amp;list=PLA89DCFA6ADACE599&amp;index=6#t=41m23s" rel="noopener noreferrer" target="_blank">Early video that created about TTS</a> using ANNs (NetTalk), see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Speech%2520synthesis.html">Speech synthesis</a></p><p><a class="tc-tiddlylink-external" href="http://fossbytes.com/a-neural-network-in-11-lines-of-python/" rel="noopener noreferrer" target="_blank">A Neural Network in 11 Lines of Python</a></p><p><u>More models, and generalizations</u></p><p><em>Backpropagation</em>, temporal networks, etc..</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=ghEmQSxT6tw" rel="noopener noreferrer" target="_blank">Visualizing and Understanding Deep Neural Networks by Matt Zeiler</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=bLFISzfQCDQ" rel="noopener noreferrer" target="_blank">Two Minute Papers - Estimating Matrix Rank With Neural Networks</a></p><hr><p>Physical implementations:</p><p><a class="tc-tiddlylink-external" href="http://www.dna.caltech.edu/courses/cs191/paperscs191/PNAS(88)10983.pdf" rel="noopener noreferrer" target="_blank">Chemical implementations of neural networks and Turing machines</a></p><p><a class="tc-tiddlylink-external" href="http://knowmtech.com/" rel="noopener noreferrer" target="_blank">http://knowmtech.com/</a></p><hr><p><em>More</em></p><p>Layerless neural networks? See Chico Calmagro's work with Ard Louis.</p><p>On the complex backpropagation algorithm</p><p><a class="tc-tiddlylink-external" href="http://www.sciencedirect.com/science/article/pii/000510989290053I" rel="noopener noreferrer" target="_blank">Neural networks for control systems—A survey</a></p><p><a class="tc-tiddlylink-external" href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=7364099&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7364099" rel="noopener noreferrer" target="_blank">Genetic deep neural networks using different activation functions for financial data mining</a></p><p><a class="tc-tiddlylink-external" href="http://www.merl.com/publications/docs/TR2015-032.pdf" rel="noopener noreferrer" target="_blank">Structure Discovery of Deep Neural Network Based on Evolutionary Algorithms</a></p><p><a class="tc-tiddlylink-external" href="http://dl.acm.org/citation.cfm?id=2602287" rel="noopener noreferrer" target="_blank">Genetic algorithms for evolving deep neural networks</a></p><p><a class="tc-tiddlylink-external" href="http://polar.lsi.uned.es/revista/index.php/ia/article/viewFile/532/516" rel="noopener noreferrer" target="_blank">Busqueda de la estructura optima de redes neurales con Algoritmos Geneticos y Simulated Annealing. Verificacion con el benchmark PROBEN1</a></p><p><a class="tc-tiddlylink-external" href="http://ceur-ws.org/Vol-1315/paper15.pdf" rel="noopener noreferrer" target="_blank">Implementation of Evolutionary Algorithms for Deep Architectures</a></p><p>See ideas here: Idea for neural network for chemical synethesis and manufacturing etc. Facebook post: <a class="tc-tiddlylink-external" href="https://www.facebook.com/guillermovalleperez/posts/10153853693416223?" rel="noopener noreferrer" target="_blank">https://www.facebook.com/guillermovalleperez/posts/10153853693416223?</a></p><h3 class=""><u>Statistical mechanics of neural networks</u></h3><p><a class="tc-tiddlylink-external" href="http://www.pnas.org/content/79/8/2554.short" rel="noopener noreferrer" target="_blank">Neural networks and physical systems with emergent collective computational abilities</a></p><p><a class="tc-tiddlylink-external" href="http://journals.aps.org/pra/abstract/10.1103/PhysRevA.32.1007" rel="noopener noreferrer" target="_blank">Spin-glass models of neural networks</a></p><p><a class="tc-tiddlylink-external" href="http://link.springer.com/article/10.1007/BF01304440" rel="noopener noreferrer" target="_blank">Learning and pattern recognition in spin glass models</a></p><p><a class="tc-tiddlylink-external" href="http://ir.library.oregonstate.edu/xmlui/handle/1957/28802" rel="noopener noreferrer" target="_blank">Neural nets : classical results and current problems</a></p></div>


</div>

</p>

</section>
</body>
</html>
