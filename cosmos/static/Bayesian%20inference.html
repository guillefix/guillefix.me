<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.15" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Bayesian inference: Cosmos — Everything there was, there is, and there will be</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Statistical%20inference tc-tagged-Bayesian%20statistics"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Bayesian inference
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="cosmos.html">
cosmos
</a> 30th November 2018 at 6:12pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Bayesian statistics
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
<span class="tc-tag-list-item">


<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
 Statistical inference
</span>

<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p><a class="tc-tiddlylink-external" href="http://approximateinference.org/" rel="noopener noreferrer" target="_blank">http://approximateinference.org/</a></p><h2 class=""><u>Introduction</u></h2><h2 class=""><u>Method</u></h2><table><tbody><tr class="evenRow"><td><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Likelihood.html">Likelihood</a> + <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Prior.html">Prior</a> – <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Bayes'%2520theorem.html">Bayes' theorem</a> –&gt; <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Posterior.html">Posterior</a></td></tr></tbody></table><ol><li>Define variables</li><li>Define <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Probabilistic%2520model.html">Probabilistic model</a> that we are going to consider.</li><li>We first choose a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Prior%2520distribution.html">Prior distribution</a> over the set of hypotheses, for instance favouring simple ones (see regularization below), which defines the parametrized family of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Likelihood%2520function.html">Likelihood function</a>s</li><li>We then calculate the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Posterior.html">posterior distribution</a> using <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Bayes'%2520theorem.html">Bayes' theorem</a></li><li>And we can then <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=6m07s" rel="noopener noreferrer" target="_blank">make a new prediction</a> by weighting over all hypothesis to calculate the <strong>expected value</strong> of the output for a new input. I think one can show (see Elements of statistical learning book) that if we knew the real distribution of output given input, the expectation value is the prediction that minimizes the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Generalization%2520error.html">Generalization error</a></li></ol><p>The last two steps <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=8m20s" rel="noopener noreferrer" target="_blank">are often computationally very difficult</a>. So, <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=9m05s" rel="noopener noreferrer" target="_blank">what's commonly done</a> is maximizing the posterior distribution (MAP principle, above).</p><h3 class=""><u>Posteriors summaries</u></h3><ul><li>Point summaries.<ul><li>Posterior mean (gives less expected error).</li><li>Posterior <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Median.html">Median</a></li><li>Posterior mode (<a class="tc-tiddlylink tc-tiddlylink-resolves" href="Maximum%2520a%2520posteriori.html">Maximum a posteriori</a>)</li></ul></li><li>Interval summaries. Prefer estimates incorporating uncertainty over point estimates.<ul><li><em>Credible intervals</em></li><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="Central%2520posterior%2520interval.html">Central posterior interval</a> (CPI)</li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Highest%2520density%2520region.html">Highest density region</a>/interval (HDI). Useful if avoiding nonsensical (low density) regions is important</li></ul></li></ul><p>Depending on the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Loss%2520function.html">Loss function</a>, different choices may be optimal, as studied by <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Decision%2520theory.html">Decision theory</a>. However, generally prefer posterior mean or median over MAP.</p><p><u>Ways of dealing with the problem of integrating prior to find normalization</u></p><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Conjugate%2520prior.html">Conjugate prior</a>s are particular choices of prior distributioj which give posterior distributions which are analytically integrable.</li><li>Discretize Baye's rule.</li><li>Sampling</li></ul><h1 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Approximate%2520Bayesian%2520inference.html">Approximate Bayesian inference</a></u></h1><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Sampling.html">Sampling</a></u></h2><p><a class="tc-tiddlylink-external" href="https://benlambertdotcom.files.wordpress.com/2016/05/bayesian-course-4-v1-handout.pdf" rel="noopener noreferrer" target="_blank">slides</a>. Often, we can't calculate the posterior distritbution directly, and so we <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Sampling.html">sample</a>, using <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Monte%2520Carlo.html">Monte Carlo</a> methods (basically just sampling methods).</p><ul><li><strong>Rejection sampling</strong>, creates independent samples, but it becomes increasingly inefficient as dimension increases (one example of the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Curse%2520of%2520dimensionality.html">Curse of dimensionality</a>).</li><li><strong>Dependent sampling</strong>. A sampling algorithm where the next sample depends on the current value.&quot;<ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Markov%2520chain%2520Monte%2520Carlo.html">Markov chain Monte Carlo</a>. Where to step next is determined via a distribution conditional on the current parameter value (1st order <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Markov%2520chain.html">Markov chain</a>). We want to choose starting position, and conditional sampling distribution so that the distribution converges to the posterior.<ul><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="Metropolis%2520algorithm.html">Metropolis algorithm</a>. Random walk Metropolis. Under quite general conditions the Random Walk Metropolis sampler converges asymptotically to the posterior. <a class="tc-tiddlylink tc-tiddlylink-missing" href="Ergodic%2520theorem.html">Ergodic theorem</a>... We move based the ratio of the proposed un-normalised posterior to our current location =&gt; no need to calculate troublesome denominator. <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=pHsuIaPbNbY" rel="noopener noreferrer" target="_blank">Efficient Bayesian inference with Hamiltonian Monte Carlo -- Michael Betancourt (Part 1)</a>. To check for convergence, multiple walkers are used (Multiple chain convergence monitoring). Still the measure to use isn't clear. Gelman and Rubin (1992) had the idea of comparing within-chain to between-chain variability. Dependence <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>↑</mo></mrow><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mrel">↑</span></span></span></span></span> =&gt; Effective sample size <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>↓</mo></mrow><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mrel">↓</span></span></span></span></span><ul><li>Metropolis-Hastings. See <a class="tc-tiddlylink-external" href="https://benlambertdotcom.files.wordpress.com/2016/05/bayesian-course-5-vfinal-v2-handout.pdf" rel="noopener noreferrer" target="_blank">here</a>. Help with uniform convergence near boundaries. For unconstrained parameters we are free to use symmetric jumping kernels. However for constrained parameters we are forced to break this symmetry.</li></ul></li></ul></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Gibbs%2520sampling.html">Gibbs sampling</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Hamiltonian%2520Monte%2520Carlo.html">Hamiltonian Monte Carlo</a></li></ul></li></ul><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Variational%2520inference.html">Variational inference</a></u></h2><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Approximate%2520Bayesian%2520computation.html">Approximate Bayesian computation</a></u></h2><hr><h2 class=""><u>Hierarchical models</u></h2><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Differential%2520equations.html">Differential equations</a> models</u></h2><p>Estimating ODE/PDE parameters. Add random noise around DE solution</p><p>Can use random walk <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Metropolis-Hastings%2520algorithm.html">Metropolis-Hastings algorithm</a>..</p><h2 class=""><u>Posterior predictive distribution</u></h2><p>from <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi><mi mathvariant="normal">∣</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">\theta | X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span></span></span> to <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mrow><mi>X</mi></mrow><mo>~</mo></mover><mi mathvariant="normal">∣</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">\tilde{X}|X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.9201900000000001em;"></span><span class="strut bottom" style="height:1.17019em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span><span style="top:-0.60233em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>~</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span></span></span>. Find probability distribution over new observations by marginalizing over posterior <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mover accent="true"><mrow><mi>X</mi></mrow><mo>~</mo></mover><mi mathvariant="normal">∣</mi><mi>X</mi><mo>)</mo><mo>=</mo><msub><mo>∑</mo><mrow><mi>θ</mi></mrow></msub><mi>P</mi><mo>(</mo><mover accent="true"><mrow><mi>X</mi></mrow><mo>~</mo></mover><mi mathvariant="normal">∣</mi><mi>θ</mi><mo separator="true">,</mo><mi>X</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>θ</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(\tilde{X}|X) = \sum_{\theta} P(\tilde{X}|\theta, X)P(\theta|X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.9201900000000001em;"></span><span class="strut bottom" style="height:1.2202000000000002em;vertical-align:-0.30001em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span><span style="top:-0.60233em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>~</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mrel">=</span><span class="mop"><span class="op-symbol small-op mop" style="top:-0.0000050000000000050004em;">∑</span><span class="vlist"><span style="top:0.30001em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span><span style="top:-0.60233em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>~</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span></span>.</p><hr><p><a class="tc-tiddlylink-external" href="https://ben-lambert.com/bayesian-lecture-slides/" rel="noopener noreferrer" target="_blank">Lecture course</a> - <a class="tc-tiddlylink-external" href="https://benlambertdotcom.files.wordpress.com/2016/05/bayesian-course-1-vfinal-vfinal.pdf" rel="noopener noreferrer" target="_blank">notes pdf</a>. <a class="tc-tiddlylink-external" href="https://ben-lambert.com/bayesian-lecture-slides/" rel="noopener noreferrer" target="_blank">notes2</a></p><hr><p><a class="tc-tiddlylink-external" href="https://www.wikiwand.com/en/Bayesian_inference" rel="noopener noreferrer" target="_blank">https://www.wikiwand.com/en/Bayesian_inference</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Bayesian%2520inference%2520exercises.html">Bayesian inference exercises</a></p><hr><p>As exemplified by <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Gaussian%2520process.html">Gaussian process</a>es, one can also apply <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Bayes'%2520theorem.html">Bayes' theorem</a> by modeling the joint Data + parameter (or thing to be inferred) distribution, which appears in the numerator.</p><hr><p>Bayes: what's the optimal predictor for a given prior. What is the optimal prior?
<a class="tc-tiddlylink tc-tiddlylink-resolves" href="Learning%2520theory.html">Learning theory</a>: is your prior good enough for the data you have? What is a good enough predictor?</p></div>


</div>

</p>

</section>
</body>
</html>
