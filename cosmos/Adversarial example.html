<p><a class="tc-tiddlylink-external" href="https://blog.openai.com/adversarial-example-research/" rel="noopener noreferrer" target="_blank">Attacking Machine Learning with Adversarial Examples</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1610.04563" rel="noopener noreferrer" target="_blank">Are Accuracy and Robustness Correlated?</a> We find that adversarial examples are mostly transferable across similar network topologies, and we demonstrate that better machine learning models are less vulnerable to adversarial examples.</p><p><a class="tc-tiddlylink-external" href="file:///home/guillefix/downloads/Deep%20learning.pdf" rel="noopener noreferrer" target="_blank">EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=M2IebCN9Ht4" rel="noopener noreferrer" target="_blank">Deep Neural Networks are Easily Fooled</a></p><p>Numerical accuracy may help against adversarial attacks: <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1704.01547" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/1704.01547</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=hDlHpBBGaKs" rel="noopener noreferrer" target="_blank">https://www.youtube.com/watch?v=hDlHpBBGaKs</a></p>