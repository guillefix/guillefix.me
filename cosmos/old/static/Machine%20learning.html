<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.12" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Machine learning: Cosmos — a non-linear personal web notebook</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Artificial%20and%20machine%20intelligence"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible" title="More actions">


</button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible" title="Edit this tiddler">


</button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible" title="Close this tiddler">


</button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Machine learning
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="guillefix.html">
guillefix
</a> 12th July 2016 at 12:34am
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">

<button class="tc-btn-invisible tc-tag-label" style="background-color:;
fill:#333333;
color:#333333;">
 Artificial and machine intelligence
</button>
<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p>See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Artificial%2520and%2520machine%2520intelligence.html">Artificial and machine intelligence</a> and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Artificial%2520intelligence.html">Artificial intelligence</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Deep%2520learning.html">Deep learning</a></p><p>–<a class="tc-tiddlylink-external" href="https://www.quora.com/Machine-Learning/How-do-I-learn-machine-learning-1" rel="noopener noreferrer" target="_blank">Book recommendations</a></p><p><a class="tc-tiddlylink-external" href="https://www.packtpub.com/big-data-and-business-intelligence/building-machine-learning-systems-python" rel="noopener noreferrer" target="_blank">Building Machine Learning Systems with Python</a>
– <a class="tc-tiddlylink-external" href="http://uk.mathworks.com/videos/machine-learning-with-matlab-87051.html?s_eid=PSM_12825" rel="noopener noreferrer" target="_blank">Machine learning in Matlab</a>
 –<a class="tc-tiddlylink-external" href="https://www.quora.com/What-are-the-differences-between-the-Andrew-Ngs-Machine-Learning-courses-offered-on-Coursera-and-iTunes-U" rel="noopener noreferrer" target="_blank">Lecture list of Andrew's course:</a>
– <a class="tc-tiddlylink-external" href="http://cs229.stanford.edu/materials.html" rel="noopener noreferrer" target="_blank">lecture notes</a>
– <a class="tc-tiddlylink-external" href="https://www.coursera.org/learn/machine-learning" rel="noopener noreferrer" target="_blank">Andrew Ng machine learning course</a> <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UzxYlbK2c7E" rel="noopener noreferrer" target="_blank">https://www.youtube.com/watch?v=UzxYlbK2c7E</a> . On <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=5u4G23_OohI" rel="noopener noreferrer" target="_blank">lecture 2</a>
 – <a class="tc-tiddlylink-external" href="https://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA" rel="noopener noreferrer" target="_blank">Machine Learning - mathematicalmonk</a>
 – <a class="tc-tiddlylink-external" href="http://www.amazon.co.uk/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020" rel="noopener noreferrer" target="_blank">Machine Learning: A Probabilistic Perspective</a> <a class="tc-tiddlylink tc-tiddlylink-missing" href="https%253A%252F%252Fwww.cs.ubc.ca%252F~murphyk%252FMLbook%252F.html">and here</a>
 – <a class="tc-tiddlylink-external" href="http://www.amazon.co.uk/Machine-Learning-Discriminative-International-Engineering/dp/1402076479" rel="noopener noreferrer" target="_blank">Machine Learning: Discriminative and Generative (The Springer International Series in Engineering and Computer Science)</a> <a class="tc-tiddlylink-external" href="https://en.wikipedia.org/wiki/Generative_model" rel="noopener noreferrer" target="_blank">https://en.wikipedia.org/wiki/Generative_model</a></p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Supervised%2520learning.html">Supervised learning</a></u></h2><p>Training data consisting on inputs and outputs. Want to find function relating inputs to outputs, to then be able to predict new outputs from new inputs. Need <strong>a way to represent the function approximation</strong>, with some parameters (the <strong>model</strong>): </p><ul><li>Linear functions</li><li>Kernel (basis) functions, polynomials, Gaussians, etc.</li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Artificial%2520neural%2520network.html">Artificial neural network</a>s</li><li>...</li></ul><p>and a <strong>learning algorithm</strong> to find best parameters for the data. </p><p>Two main types:</p><ul><li><u>Regression</u>. Output value is continuous</li><li><u>Classification</u>. Output value is discrete</li></ul><p>New paradigm: <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Deep%2520learning.html">Deep learning</a></p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Unsupervised%2520learning.html">Unsupervised learning</a></u></h2><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UzxYlbK2c7E#t=50m40s" rel="noopener noreferrer" target="_blank">Intro by Andrew Ng</a></p><p><a class="tc-tiddlylink-external" href="https://en.wikipedia.org/wiki/Self-organizing_map" rel="noopener noreferrer" target="_blank">Self-organizing map</a></p><p>Cocktail party problem. Independent component analysis</p><p>K-means</p><p><em>Clustering</em></p><p>Community clustering in networks</p><h2 class=""><u>Variations on supervised and unsupervised</u></h2><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=pytUuJPOnVI&amp;list=PLD0F06AA0D2E8FFBA&amp;index=4" rel="noopener noreferrer" target="_blank">Variations on supervised and unsupervised</a></p><h3 class=""><u><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=pytUuJPOnVI&amp;list=PLD0F06AA0D2E8FFBA&amp;index=4#t=27s" rel="noopener noreferrer" target="_blank">Semi-supervised learning</a></u></h3><p>You are given a set of inputs <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span></span>, but you only have the corresponding outputs <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span></span> for some. You have to predict the <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span></span> for the rest (by learning the function <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">y(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span> for instance, like in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Supervised%2520learning.html">Supervised learning</a>.</p><h3 class=""><u><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=pytUuJPOnVI&amp;list=PLD0F06AA0D2E8FFBA&amp;index=4#t=4m07s" rel="noopener noreferrer" target="_blank">Active learning</a></u></h3><p>Like semi-supervised learning but the algorithm can <em>ask</em> for extra data, which it deems to be the most useful data to ask for.</p><h3 class=""><u>Decision-theoretic learning</u></h3><p>Basically loss-functions/costs used by the learning agent are based on <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Decision%2520theory.html">Decision theory</a>. See example <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=pytUuJPOnVI&amp;list=PLD0F06AA0D2E8FFBA&amp;index=4#t=5m32s" rel="noopener noreferrer" target="_blank">here</a>.</p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Reinforcement%2520learning.html">Reinforcement learning</a></u></h2><p>To me it seems like the difference with supervised learning, is that you <em>don't specify input, output pairs, but just outputs</em>. You specify desired outputs, and undesired outputs. There is no input, but still the problem is not just trivial (i.e. it only ever produces one output), because the model is probabilistic.</p><p>Sequence of decisions</p><p>Reward function</p><p>Used often in robotics.</p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Learning%2520theory.html">Learning theory</a> and <a class="tc-tiddlylink tc-tiddlylink-missing" href="Learning%2520algorithm.html">Learning algorithm</a>s</u></h2><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Deep%2520learning.html">Deep learning</a></u></h2><p><small>Go deep into the rabbit hole</small></p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Bayesian%2520inference.html">Bayesian inferential statistics</a></u></h2><p><u><a class="tc-tiddlylink-external" href="https://en.wikipedia.org/wiki/Graphical_model" rel="noopener noreferrer" target="_blank">Graphical models</a></u></p><p>Good framework: <a class="tc-tiddlylink-external" href="http://mc-stan.org/" rel="noopener noreferrer" target="_blank">Stan</a></p><hr><p><img src="https://lh3.googleusercontent.com/1vNipHvUofw7v2XurbRcoKWdrha4XwW2Wg6iv_CjnPJ7yaJeLuOGPHEXP5r0bHiXDa5jXmi3gXWzRs-rnOmoWT2qdrpDlhqoPaINOW1e8wCnkcMmsfjL5I7MAnuysZNkA0ZS-AduSU6My_vj8QjrLwgU7PtqeOxEmOHYOzJMm1COtI55peywxXwYc4Ot0XMg3WSk4ctE620Fg-kQuA8Zw86ejVU0wPx4C6f-yYJYDol4KmH_zV43EJREoK0ZJaU0v4j54Luq0_enrS9FA4oPcWX5v4h6hTCXJq3aubFRI-HBAP0Az3Js3cA9ZxPQV0U-1MZBCEdfI-0b87bEVSEAvZ7vsWTfyadsG43bfwc8ZGr4XRhXWYVlGj48WxrpQyTPFhPQMXNoiRURzx5bm4ZHukhomdEE98JJ4c5XqhybUHdIk6qJbUS7BXjcYaBlm3z8bGiBlPtDSdt61a59mbotPi7DS3N-LdHrHUd3PXtG59t_5fHfKi3WpqNS_dJOefgRukPJ0OAK4fE579XHNw_8l0Fi2mAqsP7Y8WNm1lg8yXQI2c6hrlGzWt2jO_4it_Zef_2r=w1269-h675-no"></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=qz9bKfOqd0Y&amp;index=5&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw#t=25m43s" rel="noopener noreferrer" target="_blank">Deep Learning Lecture 5: Regularization, model complexity and data complexity (part 2)</a></p><p>So the simplest model that works seems to work best most of the time. Seems like an example of Occam's razor, and thus related to Solomonoff's ideas on inference (see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Algorithmic%2520information%2520theory.html">Algorithmic information theory</a>). Epicurus principle also related to Bayesian inference, because we give a distribution over models, but we keep all of them.</p><p>Hmm, also your error can't be smaller than the fundamental noise in the data. Well it can, but your model will at best be wasteful then.</p><hr><p><mark>Try <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Torch%2520(Deep%2520learning%2520framework).html">Torch</a>:</mark></p><p>See <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=DHspIG64CVM#t=45m40s" rel="noopener noreferrer" target="_blank">https://www.youtube.com/watch?v=DHspIG64CVM#t=45m40s</a></p></div>



</div>

</p>

</section>
</body>
</html>
