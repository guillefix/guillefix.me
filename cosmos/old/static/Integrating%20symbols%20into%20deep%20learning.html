<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.12" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Integrating symbols into deep learning: Cosmos â€” a non-linear personal web notebook</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   "><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible" title="More actions">


</button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible" title="Edit this tiddler">


</button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible" title="Close this tiddler">


</button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Integrating symbols into deep learning
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="guillefix.html">
guillefix
</a> 22nd May 2016 at 4:15pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"></div>
</div>
<div class="tc-tiddler-body tc-reveal"><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="Deep%2520learning.html">Deep learning</a></p><p><a class="tc-tiddlylink-external" href="https://www.cs.ox.ac.uk/seminars/1627.html" rel="noopener noreferrer" target="_blank">Integrating Symbols into Deep Learning</a></p><p><strong>Abstract of talk</strong>: Computer Science is the symbolic science of programming, incorporating techniques for representing and reasoning about the semantics, correctness and synthesis of computer programs. Recent techniques involving the learning of deep neural networks has challenged the &quot;human programmer&quot; model of Computer Science by showing that bottom-up approaches to program synthesis from sensory data can achieve impressive results ranging from visual scene analysis, expert level play in Atari games and world-class play in complex board games such as Go. Alongside the successes of Deep Learning increasing concerns are being voiced in the public domain concerning the deployment of fully automated systems with unexpected and undesirable behaviours. In this presentation we will discuss the state-of-the-art and future challenges of Machine Learning technologies which promise the transparency of symbolic Computer Science with the power and reach of sub-symbolic Deep Learning. We will discuss both weak and strong integration models for symbolic and sub-symbolic Machine Learning alongside ongoing work on applications in this area. </p><p>Integratint symbols in deep learning talk notes:</p><ul><li>Motivation <ul><li><strong>Transparency</strong>. Easily interpretable..<ul><li>Comprehensibility test. Given a program, ask questions about it.. How well someone asnwers the questions..</li><li>Depends on how our own human minds work..</li><li>Can we even understand some of the problems..</li><li>Alternative: Machines that teach us how they work would be wonderful, because at the moment we need to make the effor to interpret them.</li></ul></li><li>Computer science. Clear semantics, verification, etc.</li><li>Deep learning. Very different from rest of CS</li><li>Royal society + others... Public concern</li><li>Neet to integrate CS transparency and power of DL</li></ul></li><li>Deduction and programming<ul><li>Howard-Curry correspondance. </li><li>Proofs as programs!</li><li>Used in verification and synthesis of programs</li></ul></li><li>Machine learning, deduction and programming<ul><li>Logic programming (Kowalski 1975). Program &lt;&gt; set of clauses in logic...</li><li>Inductive logic programming (Shapiro 1982, Muggleton 1991). Start with prior knowledge, hypothesize from data addition to knowledge, if hypothesis is verified as sufficiently valid, you add to your knowledge.</li><li>Inverse resolution (Muggleton and Buntine 1988). Resolution?. </li><li>Inverse entailment (more efficient). (muggleton 1995)</li><li>Problems with recursion and predicate invention (muggleton et all 2011)</li><li>Meta-interpretative learning (muggleton et al 2015). Make it into higher order logic framework. </li><li>target theory....</li><li>Hmm, gotta learn more about logic</li></ul></li><li>Symbolic and non-symbolic machine learning.<ul><li>Neural Turing machines! (Graves et al 2014). NIPS 2015 workshop. Still doesnt make things necessarely transparent..</li><li>Bayesian-neural integration. Sum-Product Markov networks (Domingos 2015)</li><li>ILP-neural integration. Bottom-clause Neural Nets (Garcez 2014)</li></ul></li><li>Applications<ul><li>Sensory<ul><li>Staircase, Euclid project, microbe movies now</li></ul></li><li>Motor<ul><li>UCAI 2013. Build stable wall</li><li>UCAI 2015. Learning efficient strategies.</li></ul></li><li>Language applications.<ul><li>Learning formal grammars (MLJ 2014). </li><li>Dependent srting transformations (ECAI 2014). Transparent?..</li></ul></li><li>What next for meta-interpretive learning<ul><li>Neuro-logical Turing machines</li><li>Problem decomposition. One of the central issues in programming. Predicate invention is part of this</li><li>Object invention. Intrinsic to learning and perception. Introducing new entities into language. Hard problem to make the meaningful</li><li>Large-scale background knowledge: How can learners scope relevance of background concepts?</li><li>Probabilistic reasoning.Bayesian.. Single examples?</li><li></li></ul></li></ul></li></ul></div>



</div>

</p>

</section>
</body>
</html>
