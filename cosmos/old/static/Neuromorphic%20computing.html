<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.12" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Neuromorphic computing: Cosmos — a non-linear personal web notebook</title>
</head>
<body class="tc-body">

<section class="tc-story-river">

<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   "><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class="tc-btn-invisible" title="More actions">


</button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class="tc-btn-invisible" title="Edit this tiddler">


</button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class="tc-btn-invisible" title="Close this tiddler">


</button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Neuromorphic computing
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="guillefix.html">
guillefix
</a> 23rd June 2016 at 11:16pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"></div>
</div>
<div class="tc-tiddler-body tc-reveal"><p>Computing systems that imitate the working of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Neuronal%2520network.html">Neuronal network</a>s, at hardware and/or software level. A basic model is the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="Spiking%2520neural%2520network.html">Spiking neural network</a>. One advantage is that they tend to be more energy-efficient.</p><p>Numenta</p><p>IBM <a class="tc-tiddlylink tc-tiddlylink-missing" href="TrueNorth.html">TrueNorth</a>. </p><h3 class=""><u><a class="tc-tiddlylink-external" href="http://arxiv.org/abs/1603.08270" rel="noopener noreferrer" target="_blank">Convolutional Networks for Fast, Energy-Efficient Neuromorphic Computing</a></u></h3><p>This is direct evidence that an “integrate-and-spike” mechanism has the similar computational capability as the more proven ANNs.   The IBM paper however highlighted one major weakness of SNN.  That is, training of the <a class="tc-tiddlylink tc-tiddlylink-missing" href="TrueNorth.html">TrueNorth</a> system required simulation of back-propagation using another conventional GPU:</p><p>Training was performed offline on conventional GPUs, using a library of custom training layers built upon functions from the <a class="tc-tiddlylink tc-tiddlylink-missing" href="MatConvNet.html">MatConvNet</a> toolbox. Network specication and training complexity using these layers is on par with standard deep learning.</p><p>See more interesting stuff here: <a class="tc-tiddlylink-external" href="http://blog.alluviate.com/?p=123" rel="noopener noreferrer" target="_blank">Microglia: A Biologically Plausible Basis for Back-Propagation</a></p><p>There however has been no biological evidence of a structural mechanism of “back-propagation” in biological brains.  Yoshua Bengio published a paper in 2015 (see:  <a class="tc-tiddlylink-external" href="http://arxiv.org/abs/1502.04156" rel="noopener noreferrer" target="_blank">http://arxiv.org/abs/1502.04156</a> ) “Towards Biologically Plausible Deep Learning”.  The investigation attempts to explain a mechanism for back-propagation exists in Spike-Timing-Dependent Plasticity (STDP) of biological neurons. </p><p>It is however questionable whether neurons are  able to learn by themselves without the need of an external feedback pathway that spans multiple layers.</p><p>There is however an alternative mechanism that recently has been discovered that may be a more convincing argument that is based on a structure that is independent of the brain’s neurons.   There is a large class of cells in the Brain called Microglia ( see: <a class="tc-tiddlylink-external" href="https://www.technologyreview.com/s/601137/the-rogue-immune-cells-that-wreck-the-brain" rel="noopener noreferrer" target="_blank">https://www.technologyreview.com/s/601137/the-rogue-immune-cells-that-wreck-the-brain</a> ) that are responsible for regulating the neurons and their connectivity.</p><p>In summary, biological brains have a regulatory mechanism in the form of microglia that are highly dynamic in regulating synapse connectivity and pruning neural growth.   The activity is most pronounced during sleep. SNNs have been shown to have inference capabilities equivalent to Convolution Networks.  SNNs however have not shown to effectively learn on their own without a ‘back-propagation’ mechanism.   This mechanism is most plausibly provided by the microglia.</p></div>



</div>

</p>

</section>
</body>
</html>
